{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3678df3-9844-48f3-b60c-ab5cf792aced",
   "metadata": {},
   "source": [
    "# **Homework Assignment: Is a *queen* really just a *king*, minus a *man*, plus a *woman*?**\n",
    "\n",
    "--------------\n",
    "\n",
    "\n",
    "\n",
    "In class, we dealt with **embeddings** trained for **sentiment classification**. These embeddings are optimized to separate *positive* from *negative* expressions and **do not encode deeper semantic information**.\n",
    "\n",
    "However, in modern natural language processing, there exist other embeddings — such as those from **BERT**, **word2vec**, or **GloVe** — that **do capture semantic structure**. These models are trained on large corpora, and their embeddings often allow for meaningful **vector arithmetic**, like the famous:\n",
    "\n",
    "```\n",
    "embedding(\"king\") - embedding(\"man\") + embedding(\"woman\") ≈ embedding(\"queen\")\n",
    "```\n",
    "\n",
    "This homework explores **semantic vector relationships** using such pretrained embeddings.\n",
    "\n",
    "## **The Objective**\n",
    "\n",
    "Your task is to:\n",
    "\n",
    "1. Construct semantic classes of word pairs.\n",
    "2. Visualize them using PCA.\n",
    "3. Explore arithmetic operations in embedding space.\n",
    "\n",
    "## **Tasks & Deliverables**\n",
    "\n",
    "### 1. **Semantic Pair Classes**\n",
    "\n",
    "- You must gather **at least 10 classes** of semantically related word pairs.\n",
    "- Each class must contain **at least 5 pairs**.\n",
    "- That gives a **minimum total of 100 unique words** (10 classes x 5 pairs x 2 words per pair).\n",
    "\n",
    "Two example classes:\n",
    "\n",
    "**Class 1: Gender**\n",
    "\n",
    "- (king, queen)\n",
    "- (man, woman)\n",
    "- (doctor, nurse)\n",
    "- (prince, princess)\n",
    "- *(you must add one more)*\n",
    "\n",
    "**Class 2: Verb tense (past tense)**\n",
    "\n",
    "- (bring, brought)\n",
    "- (get, got)\n",
    "- (like, liked)\n",
    "- *(you must add two more)*\n",
    "\n",
    "**Your job:**\n",
    "\n",
    "- Invent or search for **at least 10 such classes**, including the examples above.\n",
    "- Each class must be conceptually coherent.\n",
    "- Other examples: singular/plural, country/capital, comparative/superlative, tool/user, job/object, etc.\n",
    "\n",
    "### 2. **Global PCA (Across All Words)**\n",
    "\n",
    "- Use PCA to reduce the **entire set of 100 word embeddings** to 2D, and plot it.\n",
    "- Plot the additional **10 separate charts**, one for each class.\n",
    "  - Each chart should display only the 10 words (5 pairs) of the given class.\n",
    "- Points should be labeled with the words themselves.\n",
    "\n",
    "### 3. **Local PCA (Per Class)**\n",
    "\n",
    "- For each class (10 total), perform PCA **only** on the 10 words of that class.\n",
    "- Plot these class-wise PCA visualizations as separate charts.\n",
    "- Again, points should be labeled with the words.\n",
    "\n",
    "**Total: 21 charts**\n",
    "(1 global plot with 100 words + 10 global-space class plots + 10 local PCA class plots)\n",
    "\n",
    "Charts should be presented in a self-explanatory manner with clear labels.\n",
    "\n",
    "### 4. **Embedding Arithmetic**\n",
    "\n",
    "For each class, choose **one example pair** (e.g., (king, queen)) and perform the operation:\n",
    "\n",
    "```\n",
    "embedding(B) - embedding(A) + embedding(C)\n",
    "```\n",
    "\n",
    "Where A and B form a known pair, and C is another base word.\n",
    "For example:\n",
    "\n",
    "```\n",
    "embedding(\"king\") - embedding(\"man\") + embedding(\"woman\")\n",
    "```\n",
    "\n",
    "* For each such result vector, find the **5 closest word embeddings** (using cosine similarity or Euclidean distance).\n",
    "* Print the top 5 neighbors **with their distances**.\n",
    "* Do this **once per class** (i.e., 10 times).\n",
    "\n",
    "This will make it possible to verify if\n",
    " ```\n",
    "embedding(\"queen\") ≈ embedding(\"king\") - embedding(\"man\") + embedding(\"woman\")\n",
    "```\n",
    "for the *gender*-related class.\n",
    "\n",
    "\n",
    "### 5. **Discussion**\n",
    "\n",
    "* Analyze and interpret your 21 plots.\n",
    "* Discuss whether the vector relationships are preserved.\n",
    "* Does PCA capture semantic differences?\n",
    "* Are the closest words from the arithmetic meaningful?\n",
    "* What kinds of relationships are captured, and what are not?\n",
    "* Are some classes better behaved than others?\n",
    "\n",
    "\n",
    "### 6. **Publish on GitHub**  \n",
    "   - Place the Colab notebook in your **GitHub repository** for this course.\n",
    "   - In your repository’s **README**, add a **link** to the notebook and also include an **“Open in Colab”** badge at the top of the notebook so it can be launched directly from GitHub.\n",
    "\n",
    "\n",
    "## Acknowledgments\n",
    "\n",
    "*This homework assignment was inspired by an idea from my master's student **Andrzej Małek**, to whom I would like to express my thanks.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91f5dbc8-31d4-40b7-b6dd-34fd5aed26cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f248556b-66b7-4c07-bd1a-d4275b8c8a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b063f357-63a3-4541-b059-8ffad0deb33c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
