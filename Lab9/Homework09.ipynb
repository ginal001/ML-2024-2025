{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fZB1Y2w9qYi",
   "metadata": {
    "id": "4fZB1Y2w9qYi"
   },
   "source": [
    "\n",
    "# MNIST dataset\n",
    "----------------------\n",
    "\n",
    "In this workshop, we will construct an MLP network designed to a specific task of classification of MNIST dataset: a set of handwritten digits from *zero* to *nine*. MNIST stands for Modified National Institute of Standards and Technology database.\n",
    "\n",
    "**You can read more about this dataset [here](https://colah.github.io/posts/2014-10-Visualizing-MNIST/#MNIST).**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wqJVU0gp9rws",
   "metadata": {
    "id": "wqJVU0gp9rws"
   },
   "source": [
    "\n",
    "\n",
    "# Multi Layer Perceptron Notation\n",
    "---------------------------------\n",
    "\n",
    "In this workshop we will be classifying 28 by 28 images into 10 classes. Thus, a four layer perceptron (our classificator) we will work further with can be defined as\n",
    "$\\hat f:\\mathbb{R}^{28\\cdot 28} \\rightarrow \\mathbb{R}^{10}$ defined as\n",
    "\n",
    "$$\\hat f \\left(x; W_1, W_2, W_3, W_4, b_1, b_2, b_3, b_4 \\right) =  W_4 \\left[ W_3 \\left[ W_2 \\left[ W_1 x  + b_1 \\right]_+  + b_2 \\right]_+  + b_3 \\right]_+ + b_4,$$\n",
    "\n",
    "where matrices $W_1, \\ldots, W_4$ are tensors of order two (matrices) with matching dimensions and bias terms $b_1, \\ldots, b_4$ are tensors of order one (vectors) of matching dimensions, and we are using ReLU activation.\n",
    "\n",
    "Note, that there is no nonlinear activation after the last layer in our neural network. **There is an implicit softmax applied while cross entropy loss is calculated by `torch.nn.functional`.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s6RwV0RN_Kc9",
   "metadata": {
    "id": "s6RwV0RN_Kc9"
   },
   "source": [
    "# Reading MNIST Dataset to Play with It\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "vjAlmExA_Twh",
   "metadata": {
    "id": "vjAlmExA_Twh",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from matplotlib import pyplot\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data',\n",
    "                                      train=True,\n",
    "                                      download=True,\n",
    "                                      transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "gcgep60g_Xs6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "gcgep60g_Xs6",
    "outputId": "b1451990-d01c-4752-de25-e49db3f8a68e"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_image, train_target = trainset[0]    #let us examine the 0-th sample\n",
    "pyplot.imshow(train_image)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5toa5f-l_eUS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5toa5f-l_eUS",
    "outputId": "77bb3c95-ea74-4ecc-c1ed-122ac1dd913d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,  18,\n",
       "          18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170, 253,\n",
       "         253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253, 253,\n",
       "         253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253, 253,\n",
       "         198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253, 205,\n",
       "          11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,  90,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253, 190,\n",
       "           2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190, 253,\n",
       "          70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35, 241,\n",
       "         225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  81,\n",
       "         240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39, 148,\n",
       "         229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221, 253,\n",
       "         253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253, 253,\n",
       "         253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253, 195,\n",
       "          80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,  11,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]],\n",
       "       dtype=torch.uint8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.data[0]     #it will be shown in two rows, so a human has hard time classificating it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eNgiEB1V_hbV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eNgiEB1V_hbV",
    "outputId": "652c4ac9-8b46-4d7a-9d3f-266b8cc37ba1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset[0][1]    #check if you classified it correctly in your mind"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5kvPHSvuAdDK",
   "metadata": {
    "id": "5kvPHSvuAdDK"
   },
   "source": [
    "# Rereading the MNIST Data (Serious Preparation for Training)\n",
    "------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "Ar2Rzq7UAcjC",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ar2Rzq7UAcjC",
    "outputId": "68f8f0f2-69c4-44d1-c770-2eac2b0e08f2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(0.1306604762738429), np.float64(0.30810780385646264))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(trainset.data.numpy().mean()/255.0, trainset.data.numpy().std()/255.0)   #MNIST datapoints are RGB integers 0-255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-waIu3OQXGx0",
   "metadata": {
    "id": "-waIu3OQXGx0"
   },
   "source": [
    "# Task\n",
    "---------------\n",
    "\n",
    "Why do we need to normalize the data, and not feed the NN with the 0-255 integers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01dayZiEAk3C",
   "metadata": {
    "id": "01dayZiEAk3C"
   },
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose(\n",
    "    [ torchvision.transforms.ToTensor(), #Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0]\n",
    "      torchvision.transforms.Normalize((0.1307), (0.3081))])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data',\n",
    "                                      train=True,\n",
    "                                      download=True,\n",
    "                                      transform=transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset,\n",
    "                                          batch_size=2048,\n",
    "                                          shuffle=True)   #we do shuffle it to give more randomizations to training epochs\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data',\n",
    "                                     train=False,\n",
    "                                     download=True,\n",
    "                                     transform=transform)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset,\n",
    "                                         batch_size=1,\n",
    "                                         shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfqt0kUECnZB",
   "metadata": {
    "id": "dfqt0kUECnZB"
   },
   "source": [
    "## Visualizing Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fxGIwBKGCtFa",
   "metadata": {
    "id": "fxGIwBKGCtFa"
   },
   "source": [
    "### Labels (Ground Truth Outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bzu3lJ2lCwFq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bzu3lJ2lCwFq",
    "outputId": "a7616a56-a698-4f01-bdd0-dae88e7f5b86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -th batch labels : tensor([4, 8, 4,  ..., 0, 0, 4])\n",
      "1 -th batch labels : tensor([2, 4, 0,  ..., 8, 4, 8])\n",
      "2 -th batch labels : tensor([4, 2, 3,  ..., 3, 7, 5])\n",
      "3 -th batch labels : tensor([7, 8, 1,  ..., 3, 7, 2])\n",
      "4 -th batch labels : tensor([0, 3, 7,  ..., 3, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(trainloader):\n",
    "        batch_inputs, batch_labels = data\n",
    "\n",
    "        if i<5:\n",
    "            print(i, \"-th batch labels :\", batch_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PebIPPfpC05T",
   "metadata": {
    "id": "PebIPPfpC05T"
   },
   "source": [
    "A single label is an entity of order zero (a constant), but batched labels are of order one. The first (and only) index is a sample index within a batch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NLmKJ1FbC2qJ",
   "metadata": {
    "id": "NLmKJ1FbC2qJ"
   },
   "source": [
    "### Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2jcRrNw_DQui",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2jcRrNw_DQui",
    "outputId": "2d9a5d12-72cd-4a38-a318-c36eb410d137"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -th batch inputs : tensor([[[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]]])\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(trainloader):\n",
    "        batch_inputs, batch_labels = data\n",
    "\n",
    "        if i==0:\n",
    "            print(i, \"-th batch inputs :\", batch_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YTcC3qH2DZOK",
   "metadata": {
    "id": "YTcC3qH2DZOK"
   },
   "source": [
    "OK, so each data image was initially a two dimensional image when we first saw it, but now the batches have order 4. The first index is a sample index within a batch, but a second index is always 0. This index represents a Channel number inserted here by `ToTensor()` transformation, always 0. As this order is one-dimensional, we can get rid of it, later, in training, in `Flatten` layer or by using `squeeze()` on a tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZX09I7rUDkei",
   "metadata": {
    "id": "ZX09I7rUDkei"
   },
   "source": [
    "# MLP Definition\n",
    "-----------------\n",
    "\n",
    "Your job now is to take the (fully functional) definition of the MLP structure and get rid off the Sequential layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "_jaDXM9oDvrx",
   "metadata": {
    "id": "_jaDXM9oDvrx"
   },
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mlp = torch.nn.Sequential(   #Sequential is a structure which allows stacking layers one on another in such a way,\n",
    "                                          #that output from a preceding layer serves as input to the next layer\n",
    "            torch.nn.Flatten(),   #change the last three orders in data (with dimensions 1, 28 and 28 respectively) into one order of dimensions (1*28*28)\n",
    "            torch.nn.Linear(1*28*28, 1024),  #which is used as INPUT to the first Linear layer\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(1024, 2048),   #IMPORTANT! Please observe, that the OUTPUT dimension of a preceding layer is always equal to the INPUT dimension of the next layer.\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(2048, 256),\n",
    "            torch.nn.ReLU(),            #ReLU (or a Sigmoid if you want) is a nonlinear function which is used in-between layers\n",
    "            torch.nn.Linear(256, 10),\n",
    "        )\n",
    "        self.dropout = torch.nn.Dropout(0.05)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mlp(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fE5rN8UTFFw7",
   "metadata": {
    "id": "fE5rN8UTFFw7"
   },
   "source": [
    "# Training Loop\n",
    "----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55Sda8Y5FIOh",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "55Sda8Y5FIOh",
    "outputId": "3a6bba26-fbc4-4d7c-9765-57c93cd3c6d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on cuda\n",
      "epoch: 0 batch: 0 current batch loss: 2.3029351234436035\n",
      "epoch: 0 batch: 1 current batch loss: 2.07623028755188\n",
      "epoch: 0 batch: 2 current batch loss: 1.602181077003479\n",
      "epoch: 0 batch: 3 current batch loss: 1.0692520141601562\n",
      "epoch: 0 batch: 4 current batch loss: 0.9160162806510925\n",
      "epoch: 0 batch: 5 current batch loss: 1.2607285976409912\n",
      "epoch: 0 batch: 6 current batch loss: 1.0266956090927124\n",
      "epoch: 0 batch: 7 current batch loss: 0.7301799058914185\n",
      "epoch: 0 batch: 8 current batch loss: 0.9679941534996033\n",
      "epoch: 0 batch: 9 current batch loss: 0.5944001078605652\n",
      "epoch: 0 batch: 10 current batch loss: 0.5503993034362793\n",
      "epoch: 0 batch: 11 current batch loss: 0.5401183366775513\n",
      "epoch: 0 batch: 12 current batch loss: 0.5690064430236816\n",
      "epoch: 0 batch: 13 current batch loss: 0.5267996191978455\n",
      "epoch: 0 batch: 14 current batch loss: 0.4406414330005646\n",
      "epoch: 0 batch: 15 current batch loss: 0.40220212936401367\n",
      "epoch: 0 batch: 16 current batch loss: 0.4184619188308716\n",
      "epoch: 0 batch: 17 current batch loss: 0.4695563316345215\n",
      "epoch: 0 batch: 18 current batch loss: 0.4256553053855896\n",
      "epoch: 0 batch: 19 current batch loss: 0.380233496427536\n",
      "epoch: 0 batch: 20 current batch loss: 0.41052892804145813\n",
      "epoch: 0 batch: 21 current batch loss: 0.36639001965522766\n",
      "epoch: 0 batch: 22 current batch loss: 0.36175537109375\n",
      "epoch: 0 batch: 23 current batch loss: 0.3094196915626526\n",
      "epoch: 0 batch: 24 current batch loss: 0.3175511658191681\n",
      "epoch: 0 batch: 25 current batch loss: 0.346913605928421\n",
      "epoch: 0 batch: 26 current batch loss: 0.3051895499229431\n",
      "epoch: 0 batch: 27 current batch loss: 0.2868872880935669\n",
      "epoch: 0 batch: 28 current batch loss: 0.26303356885910034\n",
      "epoch: 0 batch: 29 current batch loss: 0.3408665060997009\n",
      "epoch: 1 batch: 0 current batch loss: 0.2729967534542084\n",
      "epoch: 1 batch: 1 current batch loss: 0.2723272144794464\n",
      "epoch: 1 batch: 2 current batch loss: 0.2588370442390442\n",
      "epoch: 1 batch: 3 current batch loss: 0.27996838092803955\n",
      "epoch: 1 batch: 4 current batch loss: 0.23194441199302673\n",
      "epoch: 1 batch: 5 current batch loss: 0.2077285498380661\n",
      "epoch: 1 batch: 6 current batch loss: 0.22656388580799103\n",
      "epoch: 1 batch: 7 current batch loss: 0.23054353892803192\n",
      "epoch: 1 batch: 8 current batch loss: 0.2306596040725708\n",
      "epoch: 1 batch: 9 current batch loss: 0.1903761625289917\n",
      "epoch: 1 batch: 10 current batch loss: 0.2112348973751068\n",
      "epoch: 1 batch: 11 current batch loss: 0.1968984603881836\n",
      "epoch: 1 batch: 12 current batch loss: 0.20543107390403748\n",
      "epoch: 1 batch: 13 current batch loss: 0.21078699827194214\n",
      "epoch: 1 batch: 14 current batch loss: 0.19433477520942688\n",
      "epoch: 1 batch: 15 current batch loss: 0.1888597458600998\n",
      "epoch: 1 batch: 16 current batch loss: 0.2243678718805313\n",
      "epoch: 1 batch: 17 current batch loss: 0.17147476971149445\n",
      "epoch: 1 batch: 18 current batch loss: 0.21468093991279602\n",
      "epoch: 1 batch: 19 current batch loss: 0.2168801724910736\n",
      "epoch: 1 batch: 20 current batch loss: 0.17242473363876343\n",
      "epoch: 1 batch: 21 current batch loss: 0.18907472491264343\n",
      "epoch: 1 batch: 22 current batch loss: 0.17980073392391205\n",
      "epoch: 1 batch: 23 current batch loss: 0.12993913888931274\n",
      "epoch: 1 batch: 24 current batch loss: 0.15601830184459686\n",
      "epoch: 1 batch: 25 current batch loss: 0.1515335589647293\n",
      "epoch: 1 batch: 26 current batch loss: 0.15781435370445251\n",
      "epoch: 1 batch: 27 current batch loss: 0.1674073487520218\n",
      "epoch: 1 batch: 28 current batch loss: 0.16202375292778015\n",
      "epoch: 1 batch: 29 current batch loss: 0.15789592266082764\n",
      "epoch: 2 batch: 0 current batch loss: 0.14801082015037537\n",
      "epoch: 2 batch: 1 current batch loss: 0.16025473177433014\n",
      "epoch: 2 batch: 2 current batch loss: 0.11314639449119568\n",
      "epoch: 2 batch: 3 current batch loss: 0.15540042519569397\n",
      "epoch: 2 batch: 4 current batch loss: 0.12757614254951477\n",
      "epoch: 2 batch: 5 current batch loss: 0.1232510656118393\n",
      "epoch: 2 batch: 6 current batch loss: 0.14038307964801788\n",
      "epoch: 2 batch: 7 current batch loss: 0.12822791934013367\n",
      "epoch: 2 batch: 8 current batch loss: 0.12282214313745499\n",
      "epoch: 2 batch: 9 current batch loss: 0.13084566593170166\n",
      "epoch: 2 batch: 10 current batch loss: 0.13045305013656616\n",
      "epoch: 2 batch: 11 current batch loss: 0.16339737176895142\n",
      "epoch: 2 batch: 12 current batch loss: 0.12405270338058472\n",
      "epoch: 2 batch: 13 current batch loss: 0.11914937198162079\n",
      "epoch: 2 batch: 14 current batch loss: 0.12123870849609375\n",
      "epoch: 2 batch: 15 current batch loss: 0.12530316412448883\n",
      "epoch: 2 batch: 16 current batch loss: 0.11291425675153732\n",
      "epoch: 2 batch: 17 current batch loss: 0.1389414668083191\n",
      "epoch: 2 batch: 18 current batch loss: 0.12257234752178192\n",
      "epoch: 2 batch: 19 current batch loss: 0.11019539088010788\n",
      "epoch: 2 batch: 20 current batch loss: 0.1132350042462349\n",
      "epoch: 2 batch: 21 current batch loss: 0.1377497762441635\n",
      "epoch: 2 batch: 22 current batch loss: 0.12352343648672104\n",
      "epoch: 2 batch: 23 current batch loss: 0.10737261921167374\n",
      "epoch: 2 batch: 24 current batch loss: 0.15552476048469543\n",
      "epoch: 2 batch: 25 current batch loss: 0.11447682976722717\n",
      "epoch: 2 batch: 26 current batch loss: 0.14486640691757202\n",
      "epoch: 2 batch: 27 current batch loss: 0.12785398960113525\n",
      "epoch: 2 batch: 28 current batch loss: 0.11148061603307724\n",
      "epoch: 2 batch: 29 current batch loss: 0.11845824867486954\n",
      "epoch: 3 batch: 0 current batch loss: 0.10036428272724152\n",
      "epoch: 3 batch: 1 current batch loss: 0.0951172336935997\n",
      "epoch: 3 batch: 2 current batch loss: 0.10552572458982468\n",
      "epoch: 3 batch: 3 current batch loss: 0.10307738184928894\n",
      "epoch: 3 batch: 4 current batch loss: 0.11689776927232742\n",
      "epoch: 3 batch: 5 current batch loss: 0.11608891189098358\n",
      "epoch: 3 batch: 6 current batch loss: 0.0995517298579216\n",
      "epoch: 3 batch: 7 current batch loss: 0.08843472599983215\n",
      "epoch: 3 batch: 8 current batch loss: 0.08860979974269867\n",
      "epoch: 3 batch: 9 current batch loss: 0.11842312663793564\n",
      "epoch: 3 batch: 10 current batch loss: 0.10551591217517853\n",
      "epoch: 3 batch: 11 current batch loss: 0.09199746698141098\n",
      "epoch: 3 batch: 12 current batch loss: 0.10102368891239166\n",
      "epoch: 3 batch: 13 current batch loss: 0.09580972045660019\n",
      "epoch: 3 batch: 14 current batch loss: 0.09018027782440186\n",
      "epoch: 3 batch: 15 current batch loss: 0.08874382078647614\n",
      "epoch: 3 batch: 16 current batch loss: 0.08506050705909729\n",
      "epoch: 3 batch: 17 current batch loss: 0.11693480610847473\n",
      "epoch: 3 batch: 18 current batch loss: 0.07995226979255676\n",
      "epoch: 3 batch: 19 current batch loss: 0.09036713093519211\n",
      "epoch: 3 batch: 20 current batch loss: 0.08647063374519348\n",
      "epoch: 3 batch: 21 current batch loss: 0.08678361773490906\n",
      "epoch: 3 batch: 22 current batch loss: 0.10163053125143051\n",
      "epoch: 3 batch: 23 current batch loss: 0.0821375697851181\n",
      "epoch: 3 batch: 24 current batch loss: 0.0911249965429306\n",
      "epoch: 3 batch: 25 current batch loss: 0.10265081375837326\n",
      "epoch: 3 batch: 26 current batch loss: 0.10700762271881104\n",
      "epoch: 3 batch: 27 current batch loss: 0.08936110138893127\n",
      "epoch: 3 batch: 28 current batch loss: 0.10169829428195953\n",
      "epoch: 3 batch: 29 current batch loss: 0.04982588812708855\n",
      "epoch: 4 batch: 0 current batch loss: 0.0685427337884903\n",
      "epoch: 4 batch: 1 current batch loss: 0.07468263059854507\n",
      "epoch: 4 batch: 2 current batch loss: 0.06729559600353241\n",
      "epoch: 4 batch: 3 current batch loss: 0.06731376051902771\n",
      "epoch: 4 batch: 4 current batch loss: 0.07783738523721695\n",
      "epoch: 4 batch: 5 current batch loss: 0.07052572071552277\n",
      "epoch: 4 batch: 6 current batch loss: 0.08454591780900955\n",
      "epoch: 4 batch: 7 current batch loss: 0.0568552166223526\n",
      "epoch: 4 batch: 8 current batch loss: 0.060587238520383835\n",
      "epoch: 4 batch: 9 current batch loss: 0.07425298541784286\n",
      "epoch: 4 batch: 10 current batch loss: 0.06656008958816528\n",
      "epoch: 4 batch: 11 current batch loss: 0.06411486864089966\n",
      "epoch: 4 batch: 12 current batch loss: 0.07355687022209167\n",
      "epoch: 4 batch: 13 current batch loss: 0.062261082231998444\n",
      "epoch: 4 batch: 14 current batch loss: 0.059150248765945435\n",
      "epoch: 4 batch: 15 current batch loss: 0.07504627853631973\n",
      "epoch: 4 batch: 16 current batch loss: 0.08528931438922882\n",
      "epoch: 4 batch: 17 current batch loss: 0.06135298311710358\n",
      "epoch: 4 batch: 18 current batch loss: 0.06633738428354263\n",
      "epoch: 4 batch: 19 current batch loss: 0.054254885762929916\n",
      "epoch: 4 batch: 20 current batch loss: 0.07623445242643356\n",
      "epoch: 4 batch: 21 current batch loss: 0.08958208560943604\n",
      "epoch: 4 batch: 22 current batch loss: 0.08014421910047531\n",
      "epoch: 4 batch: 23 current batch loss: 0.07130375504493713\n",
      "epoch: 4 batch: 24 current batch loss: 0.06561702489852905\n",
      "epoch: 4 batch: 25 current batch loss: 0.07160407304763794\n",
      "epoch: 4 batch: 26 current batch loss: 0.07037573307752609\n",
      "epoch: 4 batch: 27 current batch loss: 0.051761988550424576\n",
      "epoch: 4 batch: 28 current batch loss: 0.07124291360378265\n",
      "epoch: 4 batch: 29 current batch loss: 0.06692811101675034\n",
      "epoch: 5 batch: 0 current batch loss: 0.06565802544355392\n",
      "epoch: 5 batch: 1 current batch loss: 0.058766867965459824\n",
      "epoch: 5 batch: 2 current batch loss: 0.06160479038953781\n",
      "epoch: 5 batch: 3 current batch loss: 0.05883548781275749\n",
      "epoch: 5 batch: 4 current batch loss: 0.04546825587749481\n",
      "epoch: 5 batch: 5 current batch loss: 0.06788289546966553\n",
      "epoch: 5 batch: 6 current batch loss: 0.056480854749679565\n",
      "epoch: 5 batch: 7 current batch loss: 0.07437773048877716\n",
      "epoch: 5 batch: 8 current batch loss: 0.06637437641620636\n",
      "epoch: 5 batch: 9 current batch loss: 0.06399528682231903\n",
      "epoch: 5 batch: 10 current batch loss: 0.05798572301864624\n",
      "epoch: 5 batch: 11 current batch loss: 0.05507221445441246\n",
      "epoch: 5 batch: 12 current batch loss: 0.05869215354323387\n",
      "epoch: 5 batch: 13 current batch loss: 0.05600515007972717\n",
      "epoch: 5 batch: 14 current batch loss: 0.04200273007154465\n",
      "epoch: 5 batch: 15 current batch loss: 0.061583153903484344\n",
      "epoch: 5 batch: 16 current batch loss: 0.07412347942590714\n",
      "epoch: 5 batch: 17 current batch loss: 0.04843075945973396\n",
      "epoch: 5 batch: 18 current batch loss: 0.06391996890306473\n",
      "epoch: 5 batch: 19 current batch loss: 0.04923505336046219\n",
      "epoch: 5 batch: 20 current batch loss: 0.04641897231340408\n",
      "epoch: 5 batch: 21 current batch loss: 0.05496957525610924\n",
      "epoch: 5 batch: 22 current batch loss: 0.05781847611069679\n",
      "epoch: 5 batch: 23 current batch loss: 0.05360953509807587\n",
      "epoch: 5 batch: 24 current batch loss: 0.06040562316775322\n",
      "epoch: 5 batch: 25 current batch loss: 0.06048289313912392\n",
      "epoch: 5 batch: 26 current batch loss: 0.05260004475712776\n",
      "epoch: 5 batch: 27 current batch loss: 0.060842759907245636\n",
      "epoch: 5 batch: 28 current batch loss: 0.04761296510696411\n",
      "epoch: 5 batch: 29 current batch loss: 0.056554969400167465\n",
      "epoch: 6 batch: 0 current batch loss: 0.05036816745996475\n",
      "epoch: 6 batch: 1 current batch loss: 0.047377005219459534\n",
      "epoch: 6 batch: 2 current batch loss: 0.05704495683312416\n",
      "epoch: 6 batch: 3 current batch loss: 0.04626324027776718\n",
      "epoch: 6 batch: 4 current batch loss: 0.04160803183913231\n",
      "epoch: 6 batch: 5 current batch loss: 0.0507347472012043\n",
      "epoch: 6 batch: 6 current batch loss: 0.045661505311727524\n",
      "epoch: 6 batch: 7 current batch loss: 0.049014266580343246\n",
      "epoch: 6 batch: 8 current batch loss: 0.04093144088983536\n",
      "epoch: 6 batch: 9 current batch loss: 0.034168168902397156\n",
      "epoch: 6 batch: 10 current batch loss: 0.052588336169719696\n",
      "epoch: 6 batch: 11 current batch loss: 0.05735234171152115\n",
      "epoch: 6 batch: 12 current batch loss: 0.05801331251859665\n",
      "epoch: 6 batch: 13 current batch loss: 0.05016995221376419\n",
      "epoch: 6 batch: 14 current batch loss: 0.039276815950870514\n",
      "epoch: 6 batch: 15 current batch loss: 0.051945459097623825\n",
      "epoch: 6 batch: 16 current batch loss: 0.045851293951272964\n",
      "epoch: 6 batch: 17 current batch loss: 0.060033366084098816\n",
      "epoch: 6 batch: 18 current batch loss: 0.040500737726688385\n",
      "epoch: 6 batch: 19 current batch loss: 0.040550995618104935\n",
      "epoch: 6 batch: 20 current batch loss: 0.03687437251210213\n",
      "epoch: 6 batch: 21 current batch loss: 0.04030504822731018\n",
      "epoch: 6 batch: 22 current batch loss: 0.045097559690475464\n",
      "epoch: 6 batch: 23 current batch loss: 0.04205002635717392\n",
      "epoch: 6 batch: 24 current batch loss: 0.03144972398877144\n",
      "epoch: 6 batch: 25 current batch loss: 0.04815306141972542\n",
      "epoch: 6 batch: 26 current batch loss: 0.04107951372861862\n",
      "epoch: 6 batch: 27 current batch loss: 0.044853150844573975\n",
      "epoch: 6 batch: 28 current batch loss: 0.049550507217645645\n",
      "epoch: 6 batch: 29 current batch loss: 0.07134531438350677\n",
      "epoch: 7 batch: 0 current batch loss: 0.044160064309835434\n",
      "epoch: 7 batch: 1 current batch loss: 0.037954676896333694\n",
      "epoch: 7 batch: 2 current batch loss: 0.04057689383625984\n",
      "epoch: 7 batch: 3 current batch loss: 0.03204338997602463\n",
      "epoch: 7 batch: 4 current batch loss: 0.039513833820819855\n",
      "epoch: 7 batch: 5 current batch loss: 0.04121645912528038\n",
      "epoch: 7 batch: 6 current batch loss: 0.031591255217790604\n",
      "epoch: 7 batch: 7 current batch loss: 0.03789447993040085\n",
      "epoch: 7 batch: 8 current batch loss: 0.033357273787260056\n",
      "epoch: 7 batch: 9 current batch loss: 0.029263218864798546\n",
      "epoch: 7 batch: 10 current batch loss: 0.035152435302734375\n",
      "epoch: 7 batch: 11 current batch loss: 0.032587189227342606\n",
      "epoch: 7 batch: 12 current batch loss: 0.03293037414550781\n",
      "epoch: 7 batch: 13 current batch loss: 0.038858965039253235\n",
      "epoch: 7 batch: 14 current batch loss: 0.03488472104072571\n",
      "epoch: 7 batch: 15 current batch loss: 0.0416385717689991\n",
      "epoch: 7 batch: 16 current batch loss: 0.03992762044072151\n",
      "epoch: 7 batch: 17 current batch loss: 0.02843748964369297\n",
      "epoch: 7 batch: 18 current batch loss: 0.03233911097049713\n",
      "epoch: 7 batch: 19 current batch loss: 0.028438091278076172\n",
      "epoch: 7 batch: 20 current batch loss: 0.035240210592746735\n",
      "epoch: 7 batch: 21 current batch loss: 0.03493789583444595\n",
      "epoch: 7 batch: 22 current batch loss: 0.032212261110544205\n",
      "epoch: 7 batch: 23 current batch loss: 0.0427129790186882\n",
      "epoch: 7 batch: 24 current batch loss: 0.04648326337337494\n",
      "epoch: 7 batch: 25 current batch loss: 0.03307576850056648\n",
      "epoch: 7 batch: 26 current batch loss: 0.02718961425125599\n",
      "epoch: 7 batch: 27 current batch loss: 0.0392877496778965\n",
      "epoch: 7 batch: 28 current batch loss: 0.04356008768081665\n",
      "epoch: 7 batch: 29 current batch loss: 0.05912702903151512\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Working on {device}\")\n",
    "\n",
    "net = MLP().to(device)\n",
    "optimizer = torch.optim.Adam(net.parameters(), 0.001)   #initial and fixed learning rate of 0.001.\n",
    "\n",
    "net.train()    #it notifies the network layers (especially batchnorm or dropout layers, which we don't use in this example) that we are doing traning\n",
    "for epoch in range(8):  #  an epoch is a training run through the whole data set\n",
    "\n",
    "    for batch, data in enumerate(trainloader):\n",
    "        batch_inputs, batch_labels = data\n",
    "\n",
    "        batch_inputs = batch_inputs.to(device)  #explicitly moving the data to the target device\n",
    "        batch_labels = batch_labels.to(device)\n",
    "\n",
    "        #batch_inputs.squeeze(1)     #alternatively if not for a Flatten layer, squeeze() could be used to remove the second order of the tensor, the Channel, which is one-dimensional (this index can be equal to 0 only)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch_outputs = net(batch_inputs)   #this line calls the forward(self, x) method of the MLP object. Please note, that the last layer of the MLP is linear\n",
    "                                            #and MLP doesn't apply\n",
    "                                            #the nonlinear activation after the last layer\n",
    "        loss = torch.nn.functional.cross_entropy(batch_outputs, batch_labels, reduction = \"mean\") #instead, nonlinear softmax is applied internally in THIS loss function\n",
    "        print(\"epoch:\", epoch, \"batch:\", batch, \"current batch loss:\", loss.item())\n",
    "        loss.backward()       #this computes gradients as we have seen in previous workshops\n",
    "        optimizer.step()     #but this line in fact updates our neural network.\n",
    "                                ####You can experiment - comment this line and check, that the loss DOE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sTe6ViIsHUbV",
   "metadata": {
    "id": "sTe6ViIsHUbV"
   },
   "source": [
    "# Testing\n",
    "----------------------\n",
    "\n",
    "Correct the code below so it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "jcPek-rrHYMi",
   "metadata": {
    "id": "jcPek-rrHYMi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy =  0.9809\n",
      "[tensor([[[[-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "           -0.4242, -0.4242, -0.4242, -0.3224,  1.0650,  2.8088,  2.3760,\n",
      "            0.7086, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "           -0.4242, -0.2587,  0.7850,  2.2742,  2.7960,  2.7960,  2.7960,\n",
      "            1.5741, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.2206,\n",
      "            0.7850,  2.2360,  2.7960,  2.7960,  2.7960,  2.6942,  1.1668,\n",
      "           -0.1951, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "           -0.4242, -0.4242, -0.4242, -0.4242,  0.0849,  0.7977,  2.1978,\n",
      "            2.7960,  2.8088,  2.7960,  2.7960,  2.0960,  0.3904, -0.4242,\n",
      "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "           -0.4242, -0.4242,  0.1358,  1.8923,  2.6306,  2.7960,  2.7960,\n",
      "            2.7960,  2.8088,  2.7960,  2.0960, -0.1187, -0.4242, -0.4242,\n",
      "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "           -0.4242, -0.4242, -0.2333,  0.3395,  0.3395,  1.7141,  2.7960,\n",
      "            2.7960,  2.8088,  2.1214, -0.1315, -0.4242, -0.4242, -0.4242,\n",
      "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "           -0.4242, -0.4242, -0.4242, -0.4242,  0.4668,  2.7197,  2.7960,\n",
      "            2.7960,  2.6942, -0.1569, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "           -0.4242, -0.4242, -0.4242,  0.5304,  2.2105,  2.7960,  2.7960,\n",
      "            2.2105,  0.7468, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "           -0.4242, -0.4242,  0.5813,  2.3633,  2.7960,  2.7960,  2.7960,\n",
      "            1.3323, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "           -0.4242,  0.9123,  2.7578,  2.7960,  2.7960,  2.7960,  0.0085,\n",
      "           -0.4115, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "            0.7850,  2.8088,  2.8088,  2.8088,  2.8088,  0.7722, -0.4242,\n",
      "           -0.4242, -0.4242, -0.4242, -0.4242, -0.3860, -0.2587, -0.2587,\n",
      "           -0.2587, -0.3224, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "            0.9377,  2.7960,  2.7960,  2.7960,  2.1723, -0.2333, -0.4242,\n",
      "           -0.4242, -0.4242, -0.4242, -0.1569,  1.6887,  2.7960,  2.7960,\n",
      "            2.7960,  2.2742, -0.1060, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.0042,\n",
      "            2.3378,  2.7960,  2.7960,  1.2559,  0.3904, -0.4242, -0.4242,\n",
      "           -0.1951,  0.1231,  1.5741,  1.7523,  2.7960,  2.7960,  2.7960,\n",
      "            2.7960,  2.7960,  1.6123, -0.3988, -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.3860,  1.6887,\n",
      "            2.7960,  2.7960,  2.6560,  0.1995, -0.2078,  0.1995,  1.5868,\n",
      "            2.2487,  2.8088,  2.7960,  2.7960,  2.7960,  2.7960,  2.7960,\n",
      "            2.7960,  2.7960,  2.7960, -0.2842, -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.2969,  2.4651,\n",
      "            2.7960,  2.7960,  2.2105, -0.2333,  1.7650,  2.7960,  2.7960,\n",
      "            2.7960,  2.8088,  2.7197,  2.1342,  2.7960,  2.2487,  2.2487,\n",
      "            2.7960,  2.7960,  1.8032, -0.3733, -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.2969,  2.4778,\n",
      "            2.7960,  2.7960,  2.4269,  0.6831,  2.6560,  2.7960,  2.7960,\n",
      "            1.9178,  0.3395,  0.2631, -0.3097,  0.3395,  0.0213,  1.8923,\n",
      "            2.7960,  2.7960,  0.2377, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.2587,  2.7960,\n",
      "            2.7960,  2.7960,  2.7960,  2.5160,  2.7960,  2.7960,  2.7960,\n",
      "            0.7595,  0.6704,  0.6704,  0.6704,  0.9632,  2.3378,  2.7960,\n",
      "            2.7960,  1.2814, -0.3606, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.3988,  1.0395,\n",
      "            2.7960,  2.7960,  2.7960,  2.7960,  2.7960,  2.7960,  2.7960,\n",
      "            2.7960,  2.8088,  2.7960,  2.7960,  2.7960,  2.7960,  2.7960,\n",
      "            1.2814, -0.3606, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.3860,\n",
      "            1.6887,  2.7960,  2.7960,  2.7960,  2.7960,  2.7960,  2.7960,\n",
      "            2.7960,  2.8088,  2.7960,  2.7960,  2.7960,  1.8032,  0.2377,\n",
      "           -0.3606, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "           -0.3351,  0.0213,  1.2559,  2.4396,  2.7960,  2.7960,  2.7960,\n",
      "            2.0578,  1.2559,  1.2559,  1.2559,  0.9759, -0.3733, -0.4242,\n",
      "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242]]]]), tensor([6])]\n"
     ]
    }
   ],
   "source": [
    "good = 0\n",
    "wrong = 0\n",
    "\n",
    "net.eval()              #it notifies the network layers (especially batchnorm or dropout layers, which we don't use in this example) that we are doing evaluation\n",
    "with torch.no_grad():   #it prevents that the net learns during evalution. The gradients are not computed, so this makes it faster, too\n",
    "    for batch, data in enumerate(testloader): #batches in test are of size 1\n",
    "        datapoint, label = data\n",
    "\n",
    "        prediction = net(datapoint.to(device))                  #prediction has values representing the \"prevalence\" of the corresponding class\n",
    "        classification = torch.argmax(prediction)    #the class is the index of maximal \"prevalence\"\n",
    "\n",
    "        if classification.item() == label.item():\n",
    "            good += 1\n",
    "        else:\n",
    "            wrong += 1\n",
    "\n",
    "print(\"accuracy = \", good/(good+wrong))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZJ5c7P6ChB3e",
   "metadata": {
    "id": "ZJ5c7P6ChB3e"
   },
   "source": [
    "# **Homework Assignment - *Do Androids Dream of Electric Sheep?***\n",
    "\n",
    "-------------------------------------  \n",
    "\n",
    "\"Do Androids Dream of Electric Sheep?\"  the famous title of Philip K. Dicks novel  raises a fascinating question: if artificial intelligence could dream, what would it see?  \n",
    "\n",
    "In this assignment, we explore a phenomenon known as **neural network dreams**, where instead of optimizing a neural network's weights, we **optimize the input itself** to achieve a desired classification outcome. Given a fully trained MNIST classification network, your goal is to manipulate its inputs so that it confidently predicts each digit from 0 to 9, starting from pure noise.  \n",
    "\n",
    "## **Tasks Description**  \n",
    "\n",
    "During this class we designed and trained a **MNIST classification neural network**, which takes a **batch of grayscale images** of size **$28 \\times 28$** as input and outputs a probability distribution over the 10 digit classes (09). However, instead of using real MNIST images, you will **treat the input batch itself as a set of trainable parameters** and optimize it so that the network classifies each image as a specific digit.  \n",
    "\n",
    "1. Your first task is to generate **a batch of 10 images**, where each image is\n",
    "   classified as one of the digits **0, 1, 2, ..., 9**, starting from an initial batch of ten random Gaussian noise images.  \n",
    "\n",
    "   Discuss the following question: do the generated images resemble real MNIST digits? Why or why not?  \n",
    "\n",
    "2. Discuss, how you would approach a second task of\n",
    "   generating an image that   \n",
    "   bares similarity to two or more digits simultaneously. **Implement your idea to see the results.**\n",
    "\n",
    "3. Third task: repeat the previous tasks with an additional L2 penalty on noise within the images. Experiment with adding `lambda_l2 * dreamed_input_batch.pow(2).mean()` loss term, with `lambda_l2` being the penalty cooefficient within an exponential progression, say from 0.001 to 10.0. Are the new digits recognized correctly? How does the penalty impact the digit quality? Explain.\n",
    "\n",
    "### **Optimization Process for Task 1**  \n",
    "\n",
    "1. Start with a **batch of 10 random Gaussian noise images** as the initial input and $(0, 1, 2, \\ldots, 9)$ as the expected output batch of target digits.  \n",
    "2. Define the objective: maximize the neural network's confidence for the corresponding target digit for each image in the batch.  \n",
    "3. Use **gradient descent** to modify the pixels in each image, making the network classify each one as the assigned digit.  \n",
    "4. Repeat until the network assigns suffieciently high confidence to each images target class.  \n",
    "\n",
    "### **Implementation Details**  \n",
    "\n",
    "- The neural network weights **must remain frozen** during optimization. You are modifying only the input images.  \n",
    "- The loss function should be the **cross-entropy loss** between the predicted probabilities and the desired class labels (plus an optional weighted L2 penalty regularizing the images in task 3).\n",
    "\n",
    "\n",
    "## **Points to Note**  \n",
    "\n",
    "1. **Visualize** the optimization process: Save images of the generated inputs at different steps and plot the classification confidence evolution over iterations.  \n",
    "3. **Document your findings** and explain the behavior you observe.  \n",
    "\n",
    "## **Task & Deliverables**  \n",
    "\n",
    "- A **Colab notebook** containing solutions for both tasks:\n",
    "  - The full implementation.\n",
    "  - Visualizations of the generated batch of images.\n",
    "  - A written explanation of your observations.\n",
    "- **Bonus:** If you create an **animation** showing the evolution of the input images during optimization, it will be considered a strong enhancement to your submission.\n",
    "  - You can generate an animation programmatically (e.g., using Matplotlib or OpenCV).\n",
    "  - Or, save image frames and use external tools to create a video.\n",
    "  - Provide a **link** to any video files in the README.\n",
    "- Upload your notebook and results to your **GitHub repository** for the course.\n",
    "- In the **README**, include a **link** to the notebook.\n",
    "- In the notebook, include **Open in Colab** badge so it can be launched directly.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992bc84f-dac2-4a74-a703-dfe551a8a237",
   "metadata": {},
   "source": [
    "# Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "05703366-9f45-4e14-b12c-12fa2f5368fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### solution FIRSTYLY WE ARE GOING TO FREEZE MODEL PARAMETERS\n",
    "for parameters in net.parameters():\n",
    "    parameters.requires_grad = False\n",
    "\n",
    "points = torch.randn(10, 1, 28, 28).to(device)\n",
    "points.requires_grad = True\n",
    "\n",
    "true = torch.LongTensor(range(0,10)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "775e617c-e74d-4eb4-8de8-fb2b1aadcff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 current batch loss: 14.50979232788086\n",
      "Iteration 1000 current batch loss: 3.288416624069214\n",
      "Iteration 2000 current batch loss: 0.8411056995391846\n",
      "Iteration 3000 current batch loss: 0.22551503777503967\n",
      "Iteration 4000 current batch loss: 0.10128903388977051\n",
      "Iteration 5000 current batch loss: 0.08204707503318787\n",
      "Iteration 6000 current batch loss: 0.07971294224262238\n",
      "Iteration 7000 current batch loss: 0.07955799996852875\n",
      "Iteration 8000 current batch loss: 0.07955613732337952\n",
      "Iteration 9000 current batch loss: 0.0795603096485138\n",
      "Iteration 10000 current batch loss: 0.07956311106681824\n",
      "Iteration 11000 current batch loss: 0.07956354320049286\n",
      "Iteration 12000 current batch loss: 0.07956083118915558\n",
      "Iteration 13000 current batch loss: 0.07956397533416748\n",
      "Iteration 14000 current batch loss: 0.07956331223249435\n",
      "Iteration 15000 current batch loss: 0.07956582307815552\n",
      "Iteration 16000 current batch loss: 0.07956650853157043\n",
      "Iteration 17000 current batch loss: 0.0795656368136406\n",
      "Iteration 18000 current batch loss: 0.07956496626138687\n",
      "Iteration 19000 current batch loss: 0.07956621050834656\n",
      "Iteration 20000 current batch loss: 0.0795665979385376\n",
      "Iteration 21000 current batch loss: 0.07956736534833908\n",
      "Iteration 22000 current batch loss: 0.07956308871507645\n",
      "Iteration 23000 current batch loss: 0.0795648843050003\n",
      "Iteration 24000 current batch loss: 0.07956446707248688\n",
      "Iteration 25000 current batch loss: 0.07956607639789581\n",
      "Iteration 26000 current batch loss: 0.07956501841545105\n",
      "Iteration 27000 current batch loss: 0.07956485450267792\n",
      "Iteration 28000 current batch loss: 0.0795661062002182\n",
      "Iteration 29000 current batch loss: 0.07956614345312119\n",
      "Iteration 30000 current batch loss: 0.0795663520693779\n",
      "Iteration 31000 current batch loss: 0.07956656068563461\n",
      "Iteration 32000 current batch loss: 0.07956350594758987\n",
      "Iteration 33000 current batch loss: 0.07956717163324356\n",
      "Iteration 34000 current batch loss: 0.07956314086914062\n",
      "Iteration 35000 current batch loss: 0.07956510782241821\n",
      "Iteration 36000 current batch loss: 0.0795668512582779\n",
      "Iteration 37000 current batch loss: 0.07956591248512268\n",
      "Iteration 38000 current batch loss: 0.0795649066567421\n",
      "Iteration 39000 current batch loss: 0.07956500351428986\n",
      "Iteration 40000 current batch loss: 0.07956603169441223\n",
      "Iteration 41000 current batch loss: 0.07956768572330475\n",
      "Iteration 42000 current batch loss: 0.07956759631633759\n",
      "Iteration 43000 current batch loss: 0.07956597954034805\n",
      "Iteration 44000 current batch loss: 0.07956649363040924\n",
      "Iteration 45000 current batch loss: 0.07956556230783463\n",
      "Iteration 46000 current batch loss: 0.07956406474113464\n",
      "Iteration 47000 current batch loss: 0.0795634388923645\n",
      "Iteration 48000 current batch loss: 0.0795648917555809\n",
      "Iteration 49000 current batch loss: 0.07956292480230331\n",
      "Iteration 50000 current batch loss: 0.07956743240356445\n",
      "Iteration 51000 current batch loss: 0.07956534624099731\n",
      "Iteration 52000 current batch loss: 0.07956656813621521\n",
      "Iteration 53000 current batch loss: 0.07956859469413757\n",
      "Iteration 54000 current batch loss: 0.07956589758396149\n",
      "Iteration 55000 current batch loss: 0.07956589758396149\n",
      "Iteration 56000 current batch loss: 0.07956399023532867\n",
      "Iteration 57000 current batch loss: 0.07956673949956894\n",
      "Iteration 58000 current batch loss: 0.07956615090370178\n",
      "Iteration 59000 current batch loss: 0.07956336438655853\n",
      "Iteration 60000 current batch loss: 0.07956482470035553\n",
      "Iteration 61000 current batch loss: 0.07956621795892715\n",
      "Iteration 62000 current batch loss: 0.07956111431121826\n",
      "Iteration 63000 current batch loss: 0.07956428080797195\n",
      "Iteration 64000 current batch loss: 0.07956252247095108\n",
      "Iteration 65000 current batch loss: 0.07956580072641373\n",
      "Iteration 66000 current batch loss: 0.07956491410732269\n",
      "Iteration 67000 current batch loss: 0.07956850528717041\n",
      "Iteration 68000 current batch loss: 0.07956557720899582\n",
      "Iteration 69000 current batch loss: 0.0795670598745346\n",
      "Iteration 70000 current batch loss: 0.07956495136022568\n",
      "Iteration 71000 current batch loss: 0.07956425100564957\n",
      "Iteration 72000 current batch loss: 0.07956025749444962\n",
      "Iteration 73000 current batch loss: 0.07956203073263168\n",
      "Iteration 74000 current batch loss: 0.07956505566835403\n",
      "Iteration 75000 current batch loss: 0.07956647872924805\n",
      "Iteration 76000 current batch loss: 0.07956256717443466\n",
      "Iteration 77000 current batch loss: 0.07956483960151672\n",
      "Iteration 78000 current batch loss: 0.07956963777542114\n",
      "Iteration 79000 current batch loss: 0.07956492900848389\n",
      "Iteration 80000 current batch loss: 0.07956394553184509\n",
      "Iteration 81000 current batch loss: 0.07956362515687943\n",
      "Iteration 82000 current batch loss: 0.0795622244477272\n",
      "Iteration 83000 current batch loss: 0.07956444472074509\n",
      "Iteration 84000 current batch loss: 0.07956398278474808\n",
      "Iteration 85000 current batch loss: 0.07956746220588684\n",
      "Iteration 86000 current batch loss: 0.07956591248512268\n",
      "Iteration 87000 current batch loss: 0.07956395298242569\n",
      "Iteration 88000 current batch loss: 0.07956412434577942\n",
      "Iteration 89000 current batch loss: 0.07956166565418243\n",
      "Iteration 90000 current batch loss: 0.07956710457801819\n",
      "Iteration 91000 current batch loss: 0.07956531643867493\n",
      "Iteration 92000 current batch loss: 0.07956758141517639\n",
      "Iteration 93000 current batch loss: 0.07956665754318237\n",
      "Iteration 94000 current batch loss: 0.07956298440694809\n",
      "Iteration 95000 current batch loss: 0.07956475019454956\n",
      "Iteration 96000 current batch loss: 0.07956589758396149\n",
      "Iteration 97000 current batch loss: 0.0795651376247406\n",
      "Iteration 98000 current batch loss: 0.0795670673251152\n",
      "Iteration 99000 current batch loss: 0.07956700026988983\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam([points], 0.001)   #initial and fixed learning rate of 0.001.\n",
    "lambda_l2 = 10\n",
    "\n",
    "for epoch in range(100000):  #  an epoch is a training run through the whole data set\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    preds = net(points)   #this line calls the forward(self, x) method of the MLP object. Please note, that the last layer of the MLP is linear\n",
    "                                            #and MLP doesn't apply\n",
    "                                            #the nonlinear activation after the last layer\n",
    "    loss = torch.nn.functional.cross_entropy(preds, true, reduction = \"mean\") + lambda_l2 * points.pow(2).mean()\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        print(\"Iteration\", epoch, \"current batch loss:\", loss.item())\n",
    "    \n",
    "    loss.backward()       #this computes gradients as we have seen in previous workshops\n",
    "    optimizer.step()     #but this line in fact updates our neural network.\n",
    "                                ####You can experiment - comment this line and check, that the loss DOE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "06f74aeb-c081-45eb-a7c3-fac1edf39e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.9173, -2.5273, -2.3000, -2.1166, -2.1140, -1.8726, -1.6631, -3.3699,\n",
      "          3.6798, -1.9110]], device='cuda:0', grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "number = 8\n",
    "\n",
    "print(net(points[number]))\n",
    "pyplot.imshow(points[number].squeeze().detach().cpu().clone().numpy())\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68efcbf1-bc04-46bf-b080-5ccc0d3ec457",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
