{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8b794f6",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/SzymonNowakowski/Machine-Learning-2024/blob/master/Lab10-convolutional-neural-network-CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xl_-W_aXqjJ2",
   "metadata": {
    "id": "xl_-W_aXqjJ2"
   },
   "source": [
    "# Lab 10 - Convolutional Neural Network, CNN\n",
    "\n",
    "### Author: Szymon Nowakowski"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Y88jX_qD1WOy",
   "metadata": {
    "id": "Y88jX_qD1WOy"
   },
   "source": [
    "# Presentation on Convolutional Layers\n",
    "--------------------\n",
    "\n",
    "We shall start off by going through a [short presentation on convolusional layers](https://github.com/SzymonNowakowski/Machine-Learning-2024/blob/master/convolutional_layers.pdf). It is best to first download it and then go through it in a slide-show layout."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jBj9rZpumQ2a",
   "metadata": {
    "id": "jBj9rZpumQ2a"
   },
   "source": [
    "# Reading MNIST Dataset\n",
    "----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "K_b3NgK0mT9C",
   "metadata": {
    "id": "K_b3NgK0mT9C"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from matplotlib import pyplot\n",
    "\n",
    "transform = torchvision.transforms.Compose(\n",
    "    [ torchvision.transforms.ToTensor(), #Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0]\n",
    "      torchvision.transforms.Normalize((0.1307), (0.3081))])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data',\n",
    "                                      train=True,\n",
    "                                      download=True,\n",
    "                                      transform=transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset,\n",
    "                                          batch_size=2048,\n",
    "                                          shuffle=True)   #we do shuffle it to give more randomizations to training epochs\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data',\n",
    "                                     train=False,\n",
    "                                     download=True,\n",
    "                                     transform=transform)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset,\n",
    "                                         batch_size=1,\n",
    "                                         shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KpN_zrBRmd6D",
   "metadata": {
    "id": "KpN_zrBRmd6D"
   },
   "source": [
    "# Tensor Sizes\n",
    "-------------------\n",
    "\n",
    "Recall:\n",
    "- Batched labels are of order one. The first (and only) index is a sample index within a batch.\n",
    "- Image batches have order 4. The first order is a batch order, but a second order has dimensionality of 1 and thus it can be indexed by 0 only.\n",
    "  - This index represents a Channel number inserted here by `ToTensor()` transformation, always 0.\n",
    "  - This singleton order should be retained because we want to use convolutional layers, which explicitly require this order. For RGB images we have 3 channels, for B&W images we have only one channel.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FkW1clWYee2a",
   "metadata": {
    "id": "FkW1clWYee2a"
   },
   "source": [
    "# CNN Definition\n",
    "-----------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YAihBeAxndOm",
   "metadata": {
    "id": "YAihBeAxndOm"
   },
   "source": [
    "\n",
    "## Task\n",
    "\n",
    "Your job now is to code the definition of the LeNet5 neural network. You can find the definition [here](https://en.wikipedia.org/wiki/LeNet#/media/File:Comparison_image_neural_networks.svg).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ObfWBGk8nZl_",
   "metadata": {
    "id": "ObfWBGk8nZl_"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LeNet5(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.conv1 = torch.nn.Conv2d(kernel_size = 5, in_channels = 1, out_channels = 6)\n",
    "        self.conv2 = torch.nn.Conv2d(kernel_size = 5, in_channels = 6, out_channels = 16)\n",
    "        self.conv3 = torch.nn.Conv2d(kernel_size = 4, in_channels = 16, out_channels = 120)\n",
    "        self.pool = nn.AvgPool2d(kernel_size = 2, stride = 2)\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.linear1 = torch.nn.Linear(120, 84)\n",
    "        self.linear2 = torch.nn.Linear(84, 10)\n",
    "        self.dropout = torch.nn.Dropout(0.05)\n",
    "\n",
    "    def forward(self, x): # B, 1, 28, 28\n",
    "        x = self.relu(self.conv1(x)) # B, 6, 24, 24\n",
    "        x = self.pool(x) # B, 6, 12, 12\n",
    "        x = self.relu(self.conv2(x)) # B, 16, 8, 8\n",
    "        x = self.pool(x)  # B, 16, 4, 4\n",
    "        x = self.relu(self.conv3(x)) # B, 120, 1, 1\n",
    "        x = x.squeeze(-1).squeeze(-1) #B, 120\n",
    "        x = F.relu(self.linear1(x)) #B, 84\n",
    "        x = self.linear2(x) #B, 84\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NFBpFy6FqLhL",
   "metadata": {
    "id": "NFBpFy6FqLhL"
   },
   "source": [
    "# Training Loop\n",
    "----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "uyEA3Qk3qMj6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uyEA3Qk3qMj6",
    "outputId": "342e3301-66f3-480a-d092-bed624b9674f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on cuda\n",
      "epoch: 0 batch: 0 current batch loss: 2.3014731407165527\n",
      "epoch: 0 batch: 1 current batch loss: 2.2921817302703857\n",
      "epoch: 0 batch: 2 current batch loss: 2.277602195739746\n",
      "epoch: 0 batch: 3 current batch loss: 2.2619822025299072\n",
      "epoch: 0 batch: 4 current batch loss: 2.242037057876587\n",
      "epoch: 0 batch: 5 current batch loss: 2.216381788253784\n",
      "epoch: 0 batch: 6 current batch loss: 2.1963541507720947\n",
      "epoch: 0 batch: 7 current batch loss: 2.1632802486419678\n",
      "epoch: 0 batch: 8 current batch loss: 2.12804913520813\n",
      "epoch: 0 batch: 9 current batch loss: 2.090547561645508\n",
      "epoch: 0 batch: 10 current batch loss: 2.049654483795166\n",
      "epoch: 0 batch: 11 current batch loss: 1.9871011972427368\n",
      "epoch: 0 batch: 12 current batch loss: 1.9218900203704834\n",
      "epoch: 0 batch: 13 current batch loss: 1.8526582717895508\n",
      "epoch: 0 batch: 14 current batch loss: 1.7966206073760986\n",
      "epoch: 0 batch: 15 current batch loss: 1.7323107719421387\n",
      "epoch: 0 batch: 16 current batch loss: 1.615640640258789\n",
      "epoch: 0 batch: 17 current batch loss: 1.536374568939209\n",
      "epoch: 0 batch: 18 current batch loss: 1.4565294981002808\n",
      "epoch: 0 batch: 19 current batch loss: 1.384108543395996\n",
      "epoch: 0 batch: 20 current batch loss: 1.2781156301498413\n",
      "epoch: 0 batch: 21 current batch loss: 1.1838593482971191\n",
      "epoch: 0 batch: 22 current batch loss: 1.122657060623169\n",
      "epoch: 0 batch: 23 current batch loss: 1.061205506324768\n",
      "epoch: 0 batch: 24 current batch loss: 1.0079463720321655\n",
      "epoch: 0 batch: 25 current batch loss: 0.9438564777374268\n",
      "epoch: 0 batch: 26 current batch loss: 0.9211400747299194\n",
      "epoch: 0 batch: 27 current batch loss: 0.849543571472168\n",
      "epoch: 0 batch: 28 current batch loss: 0.8042066097259521\n",
      "epoch: 0 batch: 29 current batch loss: 0.8010666370391846\n",
      "epoch: 1 batch: 0 current batch loss: 0.7195896506309509\n",
      "epoch: 1 batch: 1 current batch loss: 0.719353437423706\n",
      "epoch: 1 batch: 2 current batch loss: 0.6858147978782654\n",
      "epoch: 1 batch: 3 current batch loss: 0.698043704032898\n",
      "epoch: 1 batch: 4 current batch loss: 0.696937084197998\n",
      "epoch: 1 batch: 5 current batch loss: 0.650422990322113\n",
      "epoch: 1 batch: 6 current batch loss: 0.6879688501358032\n",
      "epoch: 1 batch: 7 current batch loss: 0.6339874863624573\n",
      "epoch: 1 batch: 8 current batch loss: 0.6804673671722412\n",
      "epoch: 1 batch: 9 current batch loss: 0.6434295177459717\n",
      "epoch: 1 batch: 10 current batch loss: 0.620425283908844\n",
      "epoch: 1 batch: 11 current batch loss: 0.5410595536231995\n",
      "epoch: 1 batch: 12 current batch loss: 0.6164053678512573\n",
      "epoch: 1 batch: 13 current batch loss: 0.628409206867218\n",
      "epoch: 1 batch: 14 current batch loss: 0.6027656197547913\n",
      "epoch: 1 batch: 15 current batch loss: 0.5855572819709778\n",
      "epoch: 1 batch: 16 current batch loss: 0.5554815530776978\n",
      "epoch: 1 batch: 17 current batch loss: 0.5819581151008606\n",
      "epoch: 1 batch: 18 current batch loss: 0.5851536989212036\n",
      "epoch: 1 batch: 19 current batch loss: 0.5974617004394531\n",
      "epoch: 1 batch: 20 current batch loss: 0.5565509796142578\n",
      "epoch: 1 batch: 21 current batch loss: 0.5193585753440857\n",
      "epoch: 1 batch: 22 current batch loss: 0.567688524723053\n",
      "epoch: 1 batch: 23 current batch loss: 0.6155722737312317\n",
      "epoch: 1 batch: 24 current batch loss: 0.5107420682907104\n",
      "epoch: 1 batch: 25 current batch loss: 0.5185596942901611\n",
      "epoch: 1 batch: 26 current batch loss: 0.5138986706733704\n",
      "epoch: 1 batch: 27 current batch loss: 0.543735921382904\n",
      "epoch: 1 batch: 28 current batch loss: 0.5550353527069092\n",
      "epoch: 1 batch: 29 current batch loss: 0.4400767683982849\n",
      "epoch: 2 batch: 0 current batch loss: 0.49402448534965515\n",
      "epoch: 2 batch: 1 current batch loss: 0.5532451272010803\n",
      "epoch: 2 batch: 2 current batch loss: 0.4900422692298889\n",
      "epoch: 2 batch: 3 current batch loss: 0.5050850510597229\n",
      "epoch: 2 batch: 4 current batch loss: 0.5080495476722717\n",
      "epoch: 2 batch: 5 current batch loss: 0.49960124492645264\n",
      "epoch: 2 batch: 6 current batch loss: 0.4960237443447113\n",
      "epoch: 2 batch: 7 current batch loss: 0.497715026140213\n",
      "epoch: 2 batch: 8 current batch loss: 0.4686659276485443\n",
      "epoch: 2 batch: 9 current batch loss: 0.4859902858734131\n",
      "epoch: 2 batch: 10 current batch loss: 0.4772997796535492\n",
      "epoch: 2 batch: 11 current batch loss: 0.4576975405216217\n",
      "epoch: 2 batch: 12 current batch loss: 0.44728904962539673\n",
      "epoch: 2 batch: 13 current batch loss: 0.47588589787483215\n",
      "epoch: 2 batch: 14 current batch loss: 0.4501661956310272\n",
      "epoch: 2 batch: 15 current batch loss: 0.4464079737663269\n",
      "epoch: 2 batch: 16 current batch loss: 0.4568926990032196\n",
      "epoch: 2 batch: 17 current batch loss: 0.46819448471069336\n",
      "epoch: 2 batch: 18 current batch loss: 0.4357546269893646\n",
      "epoch: 2 batch: 19 current batch loss: 0.42608386278152466\n",
      "epoch: 2 batch: 20 current batch loss: 0.43767184019088745\n",
      "epoch: 2 batch: 21 current batch loss: 0.3963286578655243\n",
      "epoch: 2 batch: 22 current batch loss: 0.40860629081726074\n",
      "epoch: 2 batch: 23 current batch loss: 0.4285276234149933\n",
      "epoch: 2 batch: 24 current batch loss: 0.4321989417076111\n",
      "epoch: 2 batch: 25 current batch loss: 0.4341886341571808\n",
      "epoch: 2 batch: 26 current batch loss: 0.445907324552536\n",
      "epoch: 2 batch: 27 current batch loss: 0.3901046812534332\n",
      "epoch: 2 batch: 28 current batch loss: 0.3759956657886505\n",
      "epoch: 2 batch: 29 current batch loss: 0.3611313998699188\n",
      "epoch: 3 batch: 0 current batch loss: 0.43605026602745056\n",
      "epoch: 3 batch: 1 current batch loss: 0.43561097979545593\n",
      "epoch: 3 batch: 2 current batch loss: 0.4281894862651825\n",
      "epoch: 3 batch: 3 current batch loss: 0.395747572183609\n",
      "epoch: 3 batch: 4 current batch loss: 0.3953040540218353\n",
      "epoch: 3 batch: 5 current batch loss: 0.38689106702804565\n",
      "epoch: 3 batch: 6 current batch loss: 0.40199899673461914\n",
      "epoch: 3 batch: 7 current batch loss: 0.40953755378723145\n",
      "epoch: 3 batch: 8 current batch loss: 0.3543183505535126\n",
      "epoch: 3 batch: 9 current batch loss: 0.40923362970352173\n",
      "epoch: 3 batch: 10 current batch loss: 0.39012375473976135\n",
      "epoch: 3 batch: 11 current batch loss: 0.37041881680488586\n",
      "epoch: 3 batch: 12 current batch loss: 0.3891780972480774\n",
      "epoch: 3 batch: 13 current batch loss: 0.365167498588562\n",
      "epoch: 3 batch: 14 current batch loss: 0.3844115436077118\n",
      "epoch: 3 batch: 15 current batch loss: 0.3829994201660156\n",
      "epoch: 3 batch: 16 current batch loss: 0.3610779345035553\n",
      "epoch: 3 batch: 17 current batch loss: 0.3339245915412903\n",
      "epoch: 3 batch: 18 current batch loss: 0.35310760140419006\n",
      "epoch: 3 batch: 19 current batch loss: 0.3634140193462372\n",
      "epoch: 3 batch: 20 current batch loss: 0.37363487482070923\n",
      "epoch: 3 batch: 21 current batch loss: 0.3433777093887329\n",
      "epoch: 3 batch: 22 current batch loss: 0.32546207308769226\n",
      "epoch: 3 batch: 23 current batch loss: 0.33754804730415344\n",
      "epoch: 3 batch: 24 current batch loss: 0.32983094453811646\n",
      "epoch: 3 batch: 25 current batch loss: 0.3508641719818115\n",
      "epoch: 3 batch: 26 current batch loss: 0.36152124404907227\n",
      "epoch: 3 batch: 27 current batch loss: 0.4035901427268982\n",
      "epoch: 3 batch: 28 current batch loss: 0.32200494408607483\n",
      "epoch: 3 batch: 29 current batch loss: 0.32489508390426636\n",
      "epoch: 4 batch: 0 current batch loss: 0.3483652174472809\n",
      "epoch: 4 batch: 1 current batch loss: 0.33408746123313904\n",
      "epoch: 4 batch: 2 current batch loss: 0.3411044180393219\n",
      "epoch: 4 batch: 3 current batch loss: 0.358239084482193\n",
      "epoch: 4 batch: 4 current batch loss: 0.344788521528244\n",
      "epoch: 4 batch: 5 current batch loss: 0.32696571946144104\n",
      "epoch: 4 batch: 6 current batch loss: 0.3247644603252411\n",
      "epoch: 4 batch: 7 current batch loss: 0.33300909399986267\n",
      "epoch: 4 batch: 8 current batch loss: 0.32872384786605835\n",
      "epoch: 4 batch: 9 current batch loss: 0.3323850929737091\n",
      "epoch: 4 batch: 10 current batch loss: 0.32313501834869385\n",
      "epoch: 4 batch: 11 current batch loss: 0.31527018547058105\n",
      "epoch: 4 batch: 12 current batch loss: 0.2714025676250458\n",
      "epoch: 4 batch: 13 current batch loss: 0.33870333433151245\n",
      "epoch: 4 batch: 14 current batch loss: 0.30957239866256714\n",
      "epoch: 4 batch: 15 current batch loss: 0.283557653427124\n",
      "epoch: 4 batch: 16 current batch loss: 0.2941938042640686\n",
      "epoch: 4 batch: 17 current batch loss: 0.30812978744506836\n",
      "epoch: 4 batch: 18 current batch loss: 0.2763154208660126\n",
      "epoch: 4 batch: 19 current batch loss: 0.30952098965644836\n",
      "epoch: 4 batch: 20 current batch loss: 0.2940507233142853\n",
      "epoch: 4 batch: 21 current batch loss: 0.2668956220149994\n",
      "epoch: 4 batch: 22 current batch loss: 0.3084099590778351\n",
      "epoch: 4 batch: 23 current batch loss: 0.27391937375068665\n",
      "epoch: 4 batch: 24 current batch loss: 0.2938143014907837\n",
      "epoch: 4 batch: 25 current batch loss: 0.2843693792819977\n",
      "epoch: 4 batch: 26 current batch loss: 0.2684805393218994\n",
      "epoch: 4 batch: 27 current batch loss: 0.30475667119026184\n",
      "epoch: 4 batch: 28 current batch loss: 0.3247894048690796\n",
      "epoch: 4 batch: 29 current batch loss: 0.2851739823818207\n",
      "epoch: 5 batch: 0 current batch loss: 0.2828487753868103\n",
      "epoch: 5 batch: 1 current batch loss: 0.29897308349609375\n",
      "epoch: 5 batch: 2 current batch loss: 0.2614203691482544\n",
      "epoch: 5 batch: 3 current batch loss: 0.2670110762119293\n",
      "epoch: 5 batch: 4 current batch loss: 0.27273234724998474\n",
      "epoch: 5 batch: 5 current batch loss: 0.3047647178173065\n",
      "epoch: 5 batch: 6 current batch loss: 0.2949054539203644\n",
      "epoch: 5 batch: 7 current batch loss: 0.2641294300556183\n",
      "epoch: 5 batch: 8 current batch loss: 0.2774162292480469\n",
      "epoch: 5 batch: 9 current batch loss: 0.273012638092041\n",
      "epoch: 5 batch: 10 current batch loss: 0.25726914405822754\n",
      "epoch: 5 batch: 11 current batch loss: 0.2636032700538635\n",
      "epoch: 5 batch: 12 current batch loss: 0.2648200988769531\n",
      "epoch: 5 batch: 13 current batch loss: 0.2530250549316406\n",
      "epoch: 5 batch: 14 current batch loss: 0.2707812786102295\n",
      "epoch: 5 batch: 15 current batch loss: 0.269336074590683\n",
      "epoch: 5 batch: 16 current batch loss: 0.2392910271883011\n",
      "epoch: 5 batch: 17 current batch loss: 0.2595791220664978\n",
      "epoch: 5 batch: 18 current batch loss: 0.25685784220695496\n",
      "epoch: 5 batch: 19 current batch loss: 0.2539176642894745\n",
      "epoch: 5 batch: 20 current batch loss: 0.26736241579055786\n",
      "epoch: 5 batch: 21 current batch loss: 0.25824326276779175\n",
      "epoch: 5 batch: 22 current batch loss: 0.2694593071937561\n",
      "epoch: 5 batch: 23 current batch loss: 0.22653397917747498\n",
      "epoch: 5 batch: 24 current batch loss: 0.24474814534187317\n",
      "epoch: 5 batch: 25 current batch loss: 0.24622265994548798\n",
      "epoch: 5 batch: 26 current batch loss: 0.24056465923786163\n",
      "epoch: 5 batch: 27 current batch loss: 0.2257653772830963\n",
      "epoch: 5 batch: 28 current batch loss: 0.19601425528526306\n",
      "epoch: 5 batch: 29 current batch loss: 0.24434837698936462\n",
      "epoch: 6 batch: 0 current batch loss: 0.25256019830703735\n",
      "epoch: 6 batch: 1 current batch loss: 0.2490459531545639\n",
      "epoch: 6 batch: 2 current batch loss: 0.21626168489456177\n",
      "epoch: 6 batch: 3 current batch loss: 0.23281395435333252\n",
      "epoch: 6 batch: 4 current batch loss: 0.25642889738082886\n",
      "epoch: 6 batch: 5 current batch loss: 0.24627843499183655\n",
      "epoch: 6 batch: 6 current batch loss: 0.21685214340686798\n",
      "epoch: 6 batch: 7 current batch loss: 0.20615829527378082\n",
      "epoch: 6 batch: 8 current batch loss: 0.22620326280593872\n",
      "epoch: 6 batch: 9 current batch loss: 0.21233540773391724\n",
      "epoch: 6 batch: 10 current batch loss: 0.23515500128269196\n",
      "epoch: 6 batch: 11 current batch loss: 0.2169015109539032\n",
      "epoch: 6 batch: 12 current batch loss: 0.2240678071975708\n",
      "epoch: 6 batch: 13 current batch loss: 0.21953929960727692\n",
      "epoch: 6 batch: 14 current batch loss: 0.21899031102657318\n",
      "epoch: 6 batch: 15 current batch loss: 0.19572779536247253\n",
      "epoch: 6 batch: 16 current batch loss: 0.19911301136016846\n",
      "epoch: 6 batch: 17 current batch loss: 0.20493414998054504\n",
      "epoch: 6 batch: 18 current batch loss: 0.22525887191295624\n",
      "epoch: 6 batch: 19 current batch loss: 0.23934493958950043\n",
      "epoch: 6 batch: 20 current batch loss: 0.22116127610206604\n",
      "epoch: 6 batch: 21 current batch loss: 0.21442897617816925\n",
      "epoch: 6 batch: 22 current batch loss: 0.23458127677440643\n",
      "epoch: 6 batch: 23 current batch loss: 0.23510880768299103\n",
      "epoch: 6 batch: 24 current batch loss: 0.2188090980052948\n",
      "epoch: 6 batch: 25 current batch loss: 0.21745134890079498\n",
      "epoch: 6 batch: 26 current batch loss: 0.22035053372383118\n",
      "epoch: 6 batch: 27 current batch loss: 0.23268473148345947\n",
      "epoch: 6 batch: 28 current batch loss: 0.1931045651435852\n",
      "epoch: 6 batch: 29 current batch loss: 0.23087956011295319\n",
      "epoch: 7 batch: 0 current batch loss: 0.2005205899477005\n",
      "epoch: 7 batch: 1 current batch loss: 0.209694504737854\n",
      "epoch: 7 batch: 2 current batch loss: 0.21012645959854126\n",
      "epoch: 7 batch: 3 current batch loss: 0.21742232143878937\n",
      "epoch: 7 batch: 4 current batch loss: 0.19748784601688385\n",
      "epoch: 7 batch: 5 current batch loss: 0.18405352532863617\n",
      "epoch: 7 batch: 6 current batch loss: 0.20633181929588318\n",
      "epoch: 7 batch: 7 current batch loss: 0.19926190376281738\n",
      "epoch: 7 batch: 8 current batch loss: 0.20193664729595184\n",
      "epoch: 7 batch: 9 current batch loss: 0.19760657846927643\n",
      "epoch: 7 batch: 10 current batch loss: 0.17272217571735382\n",
      "epoch: 7 batch: 11 current batch loss: 0.17938882112503052\n",
      "epoch: 7 batch: 12 current batch loss: 0.17868216335773468\n",
      "epoch: 7 batch: 13 current batch loss: 0.20210587978363037\n",
      "epoch: 7 batch: 14 current batch loss: 0.18532419204711914\n",
      "epoch: 7 batch: 15 current batch loss: 0.19998839497566223\n",
      "epoch: 7 batch: 16 current batch loss: 0.19921112060546875\n",
      "epoch: 7 batch: 17 current batch loss: 0.18837900459766388\n",
      "epoch: 7 batch: 18 current batch loss: 0.17901062965393066\n",
      "epoch: 7 batch: 19 current batch loss: 0.20912384986877441\n",
      "epoch: 7 batch: 20 current batch loss: 0.18326671421527863\n",
      "epoch: 7 batch: 21 current batch loss: 0.19555997848510742\n",
      "epoch: 7 batch: 22 current batch loss: 0.2061908096075058\n",
      "epoch: 7 batch: 23 current batch loss: 0.1850021779537201\n",
      "epoch: 7 batch: 24 current batch loss: 0.18526534736156464\n",
      "epoch: 7 batch: 25 current batch loss: 0.15713967382907867\n",
      "epoch: 7 batch: 26 current batch loss: 0.202729731798172\n",
      "epoch: 7 batch: 27 current batch loss: 0.17752254009246826\n",
      "epoch: 7 batch: 28 current batch loss: 0.17461813986301422\n",
      "epoch: 7 batch: 29 current batch loss: 0.1332092136144638\n",
      "epoch: 8 batch: 0 current batch loss: 0.175867959856987\n",
      "epoch: 8 batch: 1 current batch loss: 0.19123801589012146\n",
      "epoch: 8 batch: 2 current batch loss: 0.1786387413740158\n",
      "epoch: 8 batch: 3 current batch loss: 0.17956094443798065\n",
      "epoch: 8 batch: 4 current batch loss: 0.20486587285995483\n",
      "epoch: 8 batch: 5 current batch loss: 0.18782730400562286\n",
      "epoch: 8 batch: 6 current batch loss: 0.1838846653699875\n",
      "epoch: 8 batch: 7 current batch loss: 0.16989795863628387\n",
      "epoch: 8 batch: 8 current batch loss: 0.19015926122665405\n",
      "epoch: 8 batch: 9 current batch loss: 0.1902795284986496\n",
      "epoch: 8 batch: 10 current batch loss: 0.17193903028964996\n",
      "epoch: 8 batch: 11 current batch loss: 0.1768292933702469\n",
      "epoch: 8 batch: 12 current batch loss: 0.17412270605564117\n",
      "epoch: 8 batch: 13 current batch loss: 0.15417301654815674\n",
      "epoch: 8 batch: 14 current batch loss: 0.14570099115371704\n",
      "epoch: 8 batch: 15 current batch loss: 0.15782968699932098\n",
      "epoch: 8 batch: 16 current batch loss: 0.1809106320142746\n",
      "epoch: 8 batch: 17 current batch loss: 0.1850031614303589\n",
      "epoch: 8 batch: 18 current batch loss: 0.1659318506717682\n",
      "epoch: 8 batch: 19 current batch loss: 0.15803919732570648\n",
      "epoch: 8 batch: 20 current batch loss: 0.17169417440891266\n",
      "epoch: 8 batch: 21 current batch loss: 0.18093341588974\n",
      "epoch: 8 batch: 22 current batch loss: 0.1646563559770584\n",
      "epoch: 8 batch: 23 current batch loss: 0.18199624121189117\n",
      "epoch: 8 batch: 24 current batch loss: 0.17731210589408875\n",
      "epoch: 8 batch: 25 current batch loss: 0.14015096426010132\n",
      "epoch: 8 batch: 26 current batch loss: 0.16175377368927002\n",
      "epoch: 8 batch: 27 current batch loss: 0.16615375876426697\n",
      "epoch: 8 batch: 28 current batch loss: 0.1551935225725174\n",
      "epoch: 8 batch: 29 current batch loss: 0.1343516707420349\n",
      "epoch: 9 batch: 0 current batch loss: 0.18092161417007446\n",
      "epoch: 9 batch: 1 current batch loss: 0.16663765907287598\n",
      "epoch: 9 batch: 2 current batch loss: 0.1477711796760559\n",
      "epoch: 9 batch: 3 current batch loss: 0.14966993033885956\n",
      "epoch: 9 batch: 4 current batch loss: 0.14081330597400665\n",
      "epoch: 9 batch: 5 current batch loss: 0.1403518170118332\n",
      "epoch: 9 batch: 6 current batch loss: 0.15821371972560883\n",
      "epoch: 9 batch: 7 current batch loss: 0.1633518487215042\n",
      "epoch: 9 batch: 8 current batch loss: 0.15424013137817383\n",
      "epoch: 9 batch: 9 current batch loss: 0.15596891939640045\n",
      "epoch: 9 batch: 10 current batch loss: 0.1519922912120819\n",
      "epoch: 9 batch: 11 current batch loss: 0.13683916628360748\n",
      "epoch: 9 batch: 12 current batch loss: 0.15139420330524445\n",
      "epoch: 9 batch: 13 current batch loss: 0.15570758283138275\n",
      "epoch: 9 batch: 14 current batch loss: 0.18790677189826965\n",
      "epoch: 9 batch: 15 current batch loss: 0.1585783064365387\n",
      "epoch: 9 batch: 16 current batch loss: 0.1595585197210312\n",
      "epoch: 9 batch: 17 current batch loss: 0.14378118515014648\n",
      "epoch: 9 batch: 18 current batch loss: 0.14128370583057404\n",
      "epoch: 9 batch: 19 current batch loss: 0.1363181471824646\n",
      "epoch: 9 batch: 20 current batch loss: 0.17922888696193695\n",
      "epoch: 9 batch: 21 current batch loss: 0.17412784695625305\n",
      "epoch: 9 batch: 22 current batch loss: 0.1515355408191681\n",
      "epoch: 9 batch: 23 current batch loss: 0.15687891840934753\n",
      "epoch: 9 batch: 24 current batch loss: 0.13879156112670898\n",
      "epoch: 9 batch: 25 current batch loss: 0.14128021895885468\n",
      "epoch: 9 batch: 26 current batch loss: 0.1572684496641159\n",
      "epoch: 9 batch: 27 current batch loss: 0.15060964226722717\n",
      "epoch: 9 batch: 28 current batch loss: 0.14897581934928894\n",
      "epoch: 9 batch: 29 current batch loss: 0.1072877049446106\n",
      "epoch: 10 batch: 0 current batch loss: 0.13191987574100494\n",
      "epoch: 10 batch: 1 current batch loss: 0.14812038838863373\n",
      "epoch: 10 batch: 2 current batch loss: 0.12912291288375854\n",
      "epoch: 10 batch: 3 current batch loss: 0.16724246740341187\n",
      "epoch: 10 batch: 4 current batch loss: 0.1424654722213745\n",
      "epoch: 10 batch: 5 current batch loss: 0.1615070104598999\n",
      "epoch: 10 batch: 6 current batch loss: 0.15571463108062744\n",
      "epoch: 10 batch: 7 current batch loss: 0.13114090263843536\n",
      "epoch: 10 batch: 8 current batch loss: 0.1565970778465271\n",
      "epoch: 10 batch: 9 current batch loss: 0.14980745315551758\n",
      "epoch: 10 batch: 10 current batch loss: 0.14774642884731293\n",
      "epoch: 10 batch: 11 current batch loss: 0.17106899619102478\n",
      "epoch: 10 batch: 12 current batch loss: 0.16147257387638092\n",
      "epoch: 10 batch: 13 current batch loss: 0.16971886157989502\n",
      "epoch: 10 batch: 14 current batch loss: 0.1357109099626541\n",
      "epoch: 10 batch: 15 current batch loss: 0.14339634776115417\n",
      "epoch: 10 batch: 16 current batch loss: 0.11958499252796173\n",
      "epoch: 10 batch: 17 current batch loss: 0.11927219480276108\n",
      "epoch: 10 batch: 18 current batch loss: 0.15146063268184662\n",
      "epoch: 10 batch: 19 current batch loss: 0.13543415069580078\n",
      "epoch: 10 batch: 20 current batch loss: 0.14250341057777405\n",
      "epoch: 10 batch: 21 current batch loss: 0.14360877871513367\n",
      "epoch: 10 batch: 22 current batch loss: 0.1262754201889038\n",
      "epoch: 10 batch: 23 current batch loss: 0.14594149589538574\n",
      "epoch: 10 batch: 24 current batch loss: 0.13633178174495697\n",
      "epoch: 10 batch: 25 current batch loss: 0.14085176587104797\n",
      "epoch: 10 batch: 26 current batch loss: 0.17479056119918823\n",
      "epoch: 10 batch: 27 current batch loss: 0.15328916907310486\n",
      "epoch: 10 batch: 28 current batch loss: 0.14290767908096313\n",
      "epoch: 10 batch: 29 current batch loss: 0.11388136446475983\n",
      "epoch: 11 batch: 0 current batch loss: 0.1386251151561737\n",
      "epoch: 11 batch: 1 current batch loss: 0.1226918026804924\n",
      "epoch: 11 batch: 2 current batch loss: 0.1450546234846115\n",
      "epoch: 11 batch: 3 current batch loss: 0.14371398091316223\n",
      "epoch: 11 batch: 4 current batch loss: 0.14209146797657013\n",
      "epoch: 11 batch: 5 current batch loss: 0.10565091669559479\n",
      "epoch: 11 batch: 6 current batch loss: 0.1295820027589798\n",
      "epoch: 11 batch: 7 current batch loss: 0.13383108377456665\n",
      "epoch: 11 batch: 8 current batch loss: 0.13763824105262756\n",
      "epoch: 11 batch: 9 current batch loss: 0.13992741703987122\n",
      "epoch: 11 batch: 10 current batch loss: 0.1175723671913147\n",
      "epoch: 11 batch: 11 current batch loss: 0.14124885201454163\n",
      "epoch: 11 batch: 12 current batch loss: 0.13099879026412964\n",
      "epoch: 11 batch: 13 current batch loss: 0.15868626534938812\n",
      "epoch: 11 batch: 14 current batch loss: 0.12992966175079346\n",
      "epoch: 11 batch: 15 current batch loss: 0.13878260552883148\n",
      "epoch: 11 batch: 16 current batch loss: 0.13590170443058014\n",
      "epoch: 11 batch: 17 current batch loss: 0.12494261562824249\n",
      "epoch: 11 batch: 18 current batch loss: 0.12232015281915665\n",
      "epoch: 11 batch: 19 current batch loss: 0.12885236740112305\n",
      "epoch: 11 batch: 20 current batch loss: 0.13584493100643158\n",
      "epoch: 11 batch: 21 current batch loss: 0.14913827180862427\n",
      "epoch: 11 batch: 22 current batch loss: 0.12957212328910828\n",
      "epoch: 11 batch: 23 current batch loss: 0.12149453908205032\n",
      "epoch: 11 batch: 24 current batch loss: 0.12698161602020264\n",
      "epoch: 11 batch: 25 current batch loss: 0.15088175237178802\n",
      "epoch: 11 batch: 26 current batch loss: 0.12290333956480026\n",
      "epoch: 11 batch: 27 current batch loss: 0.13461889326572418\n",
      "epoch: 11 batch: 28 current batch loss: 0.13519510626792908\n",
      "epoch: 11 batch: 29 current batch loss: 0.16217930614948273\n",
      "epoch: 12 batch: 0 current batch loss: 0.13620544970035553\n",
      "epoch: 12 batch: 1 current batch loss: 0.1236523687839508\n",
      "epoch: 12 batch: 2 current batch loss: 0.13324670493602753\n",
      "epoch: 12 batch: 3 current batch loss: 0.11885009706020355\n",
      "epoch: 12 batch: 4 current batch loss: 0.11560526490211487\n",
      "epoch: 12 batch: 5 current batch loss: 0.1600433886051178\n",
      "epoch: 12 batch: 6 current batch loss: 0.11958859115839005\n",
      "epoch: 12 batch: 7 current batch loss: 0.13570977747440338\n",
      "epoch: 12 batch: 8 current batch loss: 0.11703101545572281\n",
      "epoch: 12 batch: 9 current batch loss: 0.12405482679605484\n",
      "epoch: 12 batch: 10 current batch loss: 0.14350484311580658\n",
      "epoch: 12 batch: 11 current batch loss: 0.1197529137134552\n",
      "epoch: 12 batch: 12 current batch loss: 0.12006296962499619\n",
      "epoch: 12 batch: 13 current batch loss: 0.11422126740217209\n",
      "epoch: 12 batch: 14 current batch loss: 0.15080082416534424\n",
      "epoch: 12 batch: 15 current batch loss: 0.14903727173805237\n",
      "epoch: 12 batch: 16 current batch loss: 0.12033375352621078\n",
      "epoch: 12 batch: 17 current batch loss: 0.13173681497573853\n",
      "epoch: 12 batch: 18 current batch loss: 0.14160241186618805\n",
      "epoch: 12 batch: 19 current batch loss: 0.1252247542142868\n",
      "epoch: 12 batch: 20 current batch loss: 0.11384668946266174\n",
      "epoch: 12 batch: 21 current batch loss: 0.12194246798753738\n",
      "epoch: 12 batch: 22 current batch loss: 0.11906630545854568\n",
      "epoch: 12 batch: 23 current batch loss: 0.1388266384601593\n",
      "epoch: 12 batch: 24 current batch loss: 0.14524078369140625\n",
      "epoch: 12 batch: 25 current batch loss: 0.1177813783288002\n",
      "epoch: 12 batch: 26 current batch loss: 0.12989407777786255\n",
      "epoch: 12 batch: 27 current batch loss: 0.1269546002149582\n",
      "epoch: 12 batch: 28 current batch loss: 0.14592576026916504\n",
      "epoch: 12 batch: 29 current batch loss: 0.08895653486251831\n",
      "epoch: 13 batch: 0 current batch loss: 0.10490293055772781\n",
      "epoch: 13 batch: 1 current batch loss: 0.1424253284931183\n",
      "epoch: 13 batch: 2 current batch loss: 0.11844196170568466\n",
      "epoch: 13 batch: 3 current batch loss: 0.12716130912303925\n",
      "epoch: 13 batch: 4 current batch loss: 0.12144351005554199\n",
      "epoch: 13 batch: 5 current batch loss: 0.1055804044008255\n",
      "epoch: 13 batch: 6 current batch loss: 0.1303137242794037\n",
      "epoch: 13 batch: 7 current batch loss: 0.11814474314451218\n",
      "epoch: 13 batch: 8 current batch loss: 0.12660661339759827\n",
      "epoch: 13 batch: 9 current batch loss: 0.11040467768907547\n",
      "epoch: 13 batch: 10 current batch loss: 0.12563592195510864\n",
      "epoch: 13 batch: 11 current batch loss: 0.12450502067804337\n",
      "epoch: 13 batch: 12 current batch loss: 0.12153634428977966\n",
      "epoch: 13 batch: 13 current batch loss: 0.10278197377920151\n",
      "epoch: 13 batch: 14 current batch loss: 0.11104486882686615\n",
      "epoch: 13 batch: 15 current batch loss: 0.12475061416625977\n",
      "epoch: 13 batch: 16 current batch loss: 0.11635752022266388\n",
      "epoch: 13 batch: 17 current batch loss: 0.09885971248149872\n",
      "epoch: 13 batch: 18 current batch loss: 0.11093949526548386\n",
      "epoch: 13 batch: 19 current batch loss: 0.1418324112892151\n",
      "epoch: 13 batch: 20 current batch loss: 0.13329124450683594\n",
      "epoch: 13 batch: 21 current batch loss: 0.11747388541698456\n",
      "epoch: 13 batch: 22 current batch loss: 0.11215659976005554\n",
      "epoch: 13 batch: 23 current batch loss: 0.11071515083312988\n",
      "epoch: 13 batch: 24 current batch loss: 0.11469179391860962\n",
      "epoch: 13 batch: 25 current batch loss: 0.11575038731098175\n",
      "epoch: 13 batch: 26 current batch loss: 0.13471807539463043\n",
      "epoch: 13 batch: 27 current batch loss: 0.10770245641469955\n",
      "epoch: 13 batch: 28 current batch loss: 0.11004146933555603\n",
      "epoch: 13 batch: 29 current batch loss: 0.13804858922958374\n",
      "epoch: 14 batch: 0 current batch loss: 0.1143946498632431\n",
      "epoch: 14 batch: 1 current batch loss: 0.12439084053039551\n",
      "epoch: 14 batch: 2 current batch loss: 0.10607670992612839\n",
      "epoch: 14 batch: 3 current batch loss: 0.10507919639348984\n",
      "epoch: 14 batch: 4 current batch loss: 0.10806086659431458\n",
      "epoch: 14 batch: 5 current batch loss: 0.11199965327978134\n",
      "epoch: 14 batch: 6 current batch loss: 0.11876758188009262\n",
      "epoch: 14 batch: 7 current batch loss: 0.11892886459827423\n",
      "epoch: 14 batch: 8 current batch loss: 0.1141558364033699\n",
      "epoch: 14 batch: 9 current batch loss: 0.10849056392908096\n",
      "epoch: 14 batch: 10 current batch loss: 0.12909747660160065\n",
      "epoch: 14 batch: 11 current batch loss: 0.14500734210014343\n",
      "epoch: 14 batch: 12 current batch loss: 0.12563541531562805\n",
      "epoch: 14 batch: 13 current batch loss: 0.09764578193426132\n",
      "epoch: 14 batch: 14 current batch loss: 0.10937744379043579\n",
      "epoch: 14 batch: 15 current batch loss: 0.12031809985637665\n",
      "epoch: 14 batch: 16 current batch loss: 0.10242675989866257\n",
      "epoch: 14 batch: 17 current batch loss: 0.11741778254508972\n",
      "epoch: 14 batch: 18 current batch loss: 0.11589307337999344\n",
      "epoch: 14 batch: 19 current batch loss: 0.11342572420835495\n",
      "epoch: 14 batch: 20 current batch loss: 0.12925556302070618\n",
      "epoch: 14 batch: 21 current batch loss: 0.10600614547729492\n",
      "epoch: 14 batch: 22 current batch loss: 0.11617181450128555\n",
      "epoch: 14 batch: 23 current batch loss: 0.11134980618953705\n",
      "epoch: 14 batch: 24 current batch loss: 0.11665694415569305\n",
      "epoch: 14 batch: 25 current batch loss: 0.12315530329942703\n",
      "epoch: 14 batch: 26 current batch loss: 0.1060706302523613\n",
      "epoch: 14 batch: 27 current batch loss: 0.12147578597068787\n",
      "epoch: 14 batch: 28 current batch loss: 0.10159387439489365\n",
      "epoch: 14 batch: 29 current batch loss: 0.06807903200387955\n",
      "epoch: 15 batch: 0 current batch loss: 0.09613917768001556\n",
      "epoch: 15 batch: 1 current batch loss: 0.10456237941980362\n",
      "epoch: 15 batch: 2 current batch loss: 0.11454703658819199\n",
      "epoch: 15 batch: 3 current batch loss: 0.12140513211488724\n",
      "epoch: 15 batch: 4 current batch loss: 0.10728209465742111\n",
      "epoch: 15 batch: 5 current batch loss: 0.10663675516843796\n",
      "epoch: 15 batch: 6 current batch loss: 0.11205725371837616\n",
      "epoch: 15 batch: 7 current batch loss: 0.1034456342458725\n",
      "epoch: 15 batch: 8 current batch loss: 0.11247856914997101\n",
      "epoch: 15 batch: 9 current batch loss: 0.1116160899400711\n",
      "epoch: 15 batch: 10 current batch loss: 0.09995611757040024\n",
      "epoch: 15 batch: 11 current batch loss: 0.1046985387802124\n",
      "epoch: 15 batch: 12 current batch loss: 0.10364551097154617\n",
      "epoch: 15 batch: 13 current batch loss: 0.10464608669281006\n",
      "epoch: 15 batch: 14 current batch loss: 0.10454337298870087\n",
      "epoch: 15 batch: 15 current batch loss: 0.10408586263656616\n",
      "epoch: 15 batch: 16 current batch loss: 0.0994456335902214\n",
      "epoch: 15 batch: 17 current batch loss: 0.1286928355693817\n",
      "epoch: 15 batch: 18 current batch loss: 0.11385249346494675\n",
      "epoch: 15 batch: 19 current batch loss: 0.10453925281763077\n",
      "epoch: 15 batch: 20 current batch loss: 0.10966330766677856\n",
      "epoch: 15 batch: 21 current batch loss: 0.08023227751255035\n",
      "epoch: 15 batch: 22 current batch loss: 0.10639217495918274\n",
      "epoch: 15 batch: 23 current batch loss: 0.11462093889713287\n",
      "epoch: 15 batch: 24 current batch loss: 0.11146831512451172\n",
      "epoch: 15 batch: 25 current batch loss: 0.10182416439056396\n",
      "epoch: 15 batch: 26 current batch loss: 0.1098196804523468\n",
      "epoch: 15 batch: 27 current batch loss: 0.1186627596616745\n",
      "epoch: 15 batch: 28 current batch loss: 0.0984366163611412\n",
      "epoch: 15 batch: 29 current batch loss: 0.09545735269784927\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Working on {device}\")\n",
    "\n",
    "net = LeNet5().to(device)\n",
    "optimizer = torch.optim.Adam(net.parameters(), 0.001)   #initial and fixed learning rate of 0.001.\n",
    "\n",
    "net.train()    #it notifies the network layers (especially batchnorm or dropout layers, which we don't use in this example) that we are doing traning\n",
    "for epoch in range(16):  #  an epoch is a training run through the whole data set\n",
    "\n",
    "    loss = 0.0\n",
    "    for batch, data in enumerate(trainloader):\n",
    "        batch_inputs, batch_labels = data\n",
    "\n",
    "        batch_inputs = batch_inputs.to(device)  #explicitly moving the data to the target device\n",
    "        batch_labels = batch_labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch_outputs = net(batch_inputs)   #this line calls the forward(self, x) method of the LeNet5 object. Please note,\n",
    "                                            # the nonlinear activation after the last layer is NOT applied\n",
    "        loss = torch.nn.functional.cross_entropy(batch_outputs, batch_labels, reduction = \"mean\") #instead, nonlinear softmax is applied internally in THIS loss function\n",
    "        print(\"epoch:\", epoch, \"batch:\", batch, \"current batch loss:\", loss.item())\n",
    "        loss.backward()       #this computes gradients as we have seen in previous workshops\n",
    "        optimizer.step()     #but this line in fact updates our neural network.\n",
    "                                ####You can experiment - comment this line and check, that the loss DOE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "E2IB8sviqsir",
   "metadata": {
    "id": "E2IB8sviqsir"
   },
   "source": [
    "# Testing\n",
    "----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ubj7gcEjrs50",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ubj7gcEjrs50",
    "outputId": "2a04643c-7b6f-4bd1-a397-806156bed084"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy =  0.9778\n"
     ]
    }
   ],
   "source": [
    "good = 0\n",
    "wrong = 0\n",
    "\n",
    "net.eval()              #it notifies the network layers (especially batchnorm or dropout layers, which we don't use in this example) that we are doing evaluation\n",
    "with torch.no_grad():   #it prevents that the net learns during evalution. The gradients are not computed, so this makes it faster, too\n",
    "    for batch, data in enumerate(testloader): #batches in test are of size 1\n",
    "        datapoint, label = data\n",
    "\n",
    "        prediction = net(datapoint.to(device))                  #prediction has values representing the \"prevalence\" of the corresponding class\n",
    "        classification = torch.argmax(prediction)    #the class is the index of maximal \"prevalence\"\n",
    "\n",
    "        if classification.item() == label.item():\n",
    "            good += 1\n",
    "        else:\n",
    "            wrong += 1\n",
    "\n",
    "print(\"accuracy = \", good/(good+wrong))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WPLFqVFCddlv",
   "metadata": {
    "id": "WPLFqVFCddlv"
   },
   "source": [
    "# Understanding Kernel Size, Stride and Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "D8sofeLvdTy9",
   "metadata": {
    "id": "D8sofeLvdTy9"
   },
   "source": [
    "## Task: Designing a Convolutional Neural Network with a Target Receptive Field\n",
    "\n",
    "Design a convolutional neural network using only 3×3 or 5×5 convolutional layers and 2×2 max-pooling layers without overlap (i.e., with stride 2). Use no padding in any layer. Insert a max-pooling layer after every two or three convolutional layers.\n",
    "\n",
    "Your goal is to construct an architecture such that the receptive field of each output neuron is approximately 60×60 pixels.\n",
    "\n",
    "Compute the receptive field size of the final output layer.\n",
    "\n",
    "Compute the effective stride of the output layer with respect to the input.\n",
    "\n",
    "Assuming an input image size of 400×400, determine the spatial size of the output feature map.\n",
    "\n",
    "Q: Where would you locate ReLU layers?\n",
    "Q: What if we can't match the desired input size with our architecture?\n",
    "Q: Discuss what possible impact padding can have on the network performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2Yt56whYv-Bf",
   "metadata": {
    "id": "2Yt56whYv-Bf"
   },
   "source": [
    "# **Homework Assignment – Adversarial Examples**\n",
    "\n",
    "In this assignment, you will explore how small (invisible to humans) changes to real digits can _fool_ the CNN into misclassifying them, even though the changes are imperceptible to humans.\n",
    "\n",
    "\n",
    "\n",
    "## Task 1 – CNN Dreams: Last Homework Assignment Revisited\n",
    "\n",
    "Re-run the input optimization process (for NN dreams) from the previous MLP-class homework assignment, but this time using the **LeNet-5 CNN model** we trained in this class.\n",
    "\n",
    "\n",
    "1. Starting from ten random noise images, optimize the input so that each image is classified with high confidence as one of the digits 0 through 9.\n",
    "2. Include an **L2 penalty** on the input to keep the images visually closer to realistic digits. Use a range of penalty strengths (e.g., $\\lambda_{l2}$ = 0, and then 0.01 through 10.0).\n",
    "3. Compare the generated images (with and without L2 penalty) to those generated by the MLP:\n",
    "   - Are they more or less readable?\n",
    "   - Do they resemble real MNIST digits more closely or less?\n",
    "   - Why do you think that happens? Consider the CNN’s inductive biases and architectural properties.\n",
    "\n",
    "Use `cross_entropy_loss + lambda_l2 * input.pow(2).mean()` as your objective.\n",
    "\n",
    "Reuse your code: visualize confidence evolution during optimization and generate image grids and (optionally) animations showing how the inputs evolve.\n",
    "\n",
    "\n",
    "## Task 2 – Adversarial Examples: Fooling LeNet-5\n",
    "\n",
    "This is the core focus of the assignment.\n",
    "\n",
    "Using a batch of **real MNIST digits** (e.g., nine examples per class), craft **adversarial examples** by adding subtle, trained noise to the input images. Your goal is to:\n",
    "\n",
    "- **Keep the human-perceived digit the same** (e.g., a \"7\" should still look like a \"7\"),\n",
    "- But **cause LeNet-5 to misclassify it** – as every other class different from the original, hence nine examples per class.\n",
    "\n",
    "### Objective\n",
    "For each image $x$ and its true label $y$, learn a perturbation $\\delta$ such that:\n",
    "\n",
    "- $\\text{LeNet5}(x + \\delta) = y_{\\text{wrong}} $,\n",
    "- and $ \\|\\delta\\|_2 $ is as small as possible (penalize large perturbations), to keep $x + \\delta$ *look* like $x$ for humans.\n",
    "\n",
    "### Optimization\n",
    "Use gradient-based optimization on $\\delta$ (the noise), while keeping the network weights frozen. Your loss might look like:\n",
    "\n",
    "```\n",
    "loss = cross_entropy(model(x + delta), target_wrong_class) +\n",
    "       lambda_l2 * delta.pow(2).mean()\n",
    "```\n",
    "\n",
    "Tune the $\\lambda_{l2}$ to find the best range.\n",
    "\n",
    "### Deliverables for the Second Task\n",
    "- Select some best examples, showing the original digit and its (correct) classification and the perturbed digit (hopefully, still looking the same to humans) and how it gets misclassified. Show them side by side.\n",
    "- Report:\n",
    "  - Success rate of attacks (it doesn't need to be very formal),\n",
    "  - Effect of $\\lambda_{l2}$ on visibility of the noise and success of misclassification,\n",
    "  - Example image grids and confidence plots.\n",
    "\n",
    "\n",
    "\n",
    "## Deliverables for the Homework Assignment\n",
    "- A Google Colab notebook with:\n",
    "  - Complete implementation for both tasks.\n",
    "  - Visualizations and animations (animations are optional but encouraged).\n",
    "  - Clear written analysis of your findings.\n",
    "- Upload the notebook and results to your GitHub repository for the course.\n",
    "- Include a link to the notebook and video (if applicable) in the `README.md`.\n",
    "- In the notebook, include “Open in Colab” badge so it can be launched directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d37017d6-b76c-4edd-974f-8080b4738b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "### solution FIRSTYLY WE ARE GOING TO FREEZE MODEL PARAMETERS\n",
    "for parameters in net.parameters():\n",
    "    parameters.requires_grad = False\n",
    "\n",
    "points = torch.randn(10, 1, 28, 28).to(device)\n",
    "points.requires_grad = True\n",
    "\n",
    "true = torch.LongTensor(range(0,10)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0a620bde-d82c-4e4c-af04-e122ddfb93a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 current batch loss: 104.62568664550781\n",
      "Iteration 1000 current batch loss: 23.305091857910156\n",
      "Iteration 2000 current batch loss: 5.554699420928955\n",
      "Iteration 3000 current batch loss: 1.7771482467651367\n",
      "Iteration 4000 current batch loss: 1.100236415863037\n",
      "Iteration 5000 current batch loss: 0.9954614639282227\n",
      "Iteration 6000 current batch loss: 0.9830889701843262\n",
      "Iteration 7000 current batch loss: 0.9824244976043701\n",
      "Iteration 8000 current batch loss: 0.9824075698852539\n",
      "Iteration 9000 current batch loss: 0.9824130535125732\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam([points], 0.001)   #initial and fixed learning rate of 0.001.\n",
    "lambda_l2 = 100\n",
    "\n",
    "for epoch in range(10000):  #  an epoch is a training run through the whole data set\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    preds = net(points)   #this line calls the forward(self, x) method of the MLP object. Please note, that the last layer of the MLP is linear\n",
    "                                            #and MLP doesn't apply\n",
    "                                            #the nonlinear activation after the last layer\n",
    "    loss = torch.nn.functional.cross_entropy(preds, true, reduction = \"mean\") + lambda_l2 * points.pow(2).mean()\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        print(\"Iteration\", epoch, \"current batch loss:\", loss.item())\n",
    "    \n",
    "    loss.backward()       #this computes gradients as we have seen in previous workshops\n",
    "    optimizer.step()     #but this line in fact updates our neural network.\n",
    "                                ####You can experiment - comment this line and check, that the loss DOE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a966bdcb-5c68-4d54-aa9a-762126a6cb2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.2678, -1.4041, -0.4439, -1.5712, -1.0570, -1.2793, -0.9526, -0.6799,\n",
      "        -0.8322, -0.4328], device='cuda:0', grad_fn=<ViewBackward0>)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "number = 0\n",
    "\n",
    "print(net(points[number]))\n",
    "pyplot.imshow(points[number].squeeze().detach().cpu().clone().numpy())\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de350527-8d9c-44ae-9e1d-e7bfd997d340",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
