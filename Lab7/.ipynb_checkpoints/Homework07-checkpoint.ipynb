{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33676981-93d1-4ab1-a3ca-c22962a5ae55",
   "metadata": {},
   "source": [
    "# Homework Assignment: Understanding Binary Cross-Entropy in the Forward Stagewise Procedure\n",
    "-----------------------------\n",
    "\n",
    "## The Problem\n",
    "\n",
    "In this assignment, you will demonstrate that adding a new constant predictor in the forward stagewise procedure to an already existing predictor from a previous stage, $f_{m-1}(x_i)$ (which we will denote as $f_i$ for ease of notation), may be for certain loss functions fundamentally more challenging than building a constant predictor from scratch. You will work with the binary cross-entropy loss defined as\n",
    "\n",
    "$$\n",
    "L(y, z) = -y \\log(\\sigma(z)) - (1 - y) \\log(1 - \\sigma(z)),\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "- $y_i \\in \\{0,1\\}$ are the binary labels,\n",
    "- $\\sigma(z) = \\frac{1}{1 + e^{-z}}$ is the sigmoid function.\n",
    "\n",
    "For the purposes of this assignment, assume that you are given:\n",
    "\n",
    "- A dataset of binary labels $y_i$.\n",
    "- **Two constants: $m$ (the number of ones) and $k$ (the number of zeros)** in the labels in the dataset.\n",
    "- A set of predictions $f_i = f_{m-1}(x_i)$ obtained from a previous stage, where the $f_i$ values are generated randomly from a normal distribution.\n",
    "\n",
    "### The assignment will explore two scenarios:\n",
    "\n",
    "### Scenario A: Fitting a Constant Predictor from Scratch\n",
    "\n",
    "In this scenario, you are building a predictor from scratch. The task is to find the optimal constant value $\\lambda$ that minimizes the binary cross-entropy loss over the dataset. Formulate the optimization problem as:\n",
    "\n",
    "$$\n",
    "\\lambda^* = \\arg\\min_{\\lambda} \\sum_{i=1}^{n} L(y_i, \\lambda).\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### Scenario B: Fitting the m-th Predictor in the Forward Stagewise Procedure\n",
    "\n",
    "Now assume you already have an existing predictor $f_i = f_{m-1}(x_i)$. Rather than predicting from scratch, you wish to find an optimal additive correction $\\lambda$ such that the updated prediction for each data point becomes\n",
    "\n",
    "$$\n",
    " f_i + \\lambda,\n",
    "$$\n",
    "\n",
    "and the corresponding binary cross-entropy loss is given by\n",
    "\n",
    "$$\n",
    "\\lambda^* = \\arg\\min_{\\lambda} \\sum_{i=1}^{n} L(y_i, f_i + \\lambda).\n",
    "$$\n",
    "\n",
    "\n",
    "## Research Questions\n",
    "\n",
    "In this assignment you will answer the following questions:\n",
    "\n",
    "- Why is finding an optimal additive shift $\\lambda$ in the forward stagewise procedure fundamentally harder than directly fitting a single-parameter predictor from scratch?\n",
    "- How does the complexity of the loss landscape differ between these two scenarios? Discuss the differences in the shape and smoothness of the loss function in both cases.\n",
    "\n",
    "## Tasks & Deliverables\n",
    "\n",
    "1. **Derivation and Analysis**\n",
    "\n",
    "  **Scenario A:**\n",
    "  - Derive explicitly the optimal $\\lambda$ for fitting from scratch, i.e., solve\n",
    "    \n",
    "    $$\n",
    "    \\lambda^* = \\arg\\min_{\\lambda} \\sum_{i=1}^{n} L(y_i, \\lambda)\n",
    "    $$\n",
    "    \n",
    "    and express the answer in terms of $m$ and $k$.\n",
    "    *Hint:* First, write the derivative of the loss with respect to $\\lambda$ and set it to zero to obtain an implicit equation.\n",
    "\n",
    "  - **Interpretation:**\n",
    "  Provide a clear interpretation of your derived optimal $\\lambda$. What does this constant represent in terms of the dataset's label distribution?\n",
    "\n",
    "  **Scenario B:**\n",
    "  - Derive the implicit equation that $\\lambda$ must satisfy in the additive shift scenario:\n",
    "    \n",
    "    $$\n",
    "    \\lambda^* = \\arg\\min_{\\lambda} \\sum_{i=1}^{n} L(y_i, f_i + \\lambda).\n",
    "    $$\n",
    "    *Hint:* First, write the derivative of the loss with respect to $\\lambda$ and set it to zero to obtain an implicit equation.\n",
    "\n",
    "  - **Discussion:**\n",
    "    Explain clearly why this implicit equation has no simple closed-form solution, unlike the previous case. What role does the non-linearity of the sigmoid function (in the binary cross-entropy loss) play in this difficulty?\n",
    "\n",
    "2. **Loss Landscape Exploration (Python)**\n",
    "\n",
    "  Write a Python code that:\n",
    "\n",
    "  - Uses provided values for $n$, and for $k$ and $m$ (the number of zeros and ones in the labels, respectively), $k+m=n$.\n",
    "  - Generates a set of predictions $f_i$ by sampling from a normal distribution.\n",
    "  - Plots the binary cross-entropy loss as a function of $\\lambda$ for:\n",
    "    - Scenario A:\n",
    "      $$\n",
    "      \\text{plot } \\sum_{i=1}^{n} L(y_i, \\lambda) \\text{ as a function of }\\lambda\n",
    "      $$\n",
    "    - Scenario B:\n",
    "      $$\n",
    "      \\text{plot } \\sum_{i=1}^{n} L(y_i, f_i + \\lambda) \\text{ as a function of }\\lambda\n",
    "      $$\n",
    "  - **Discussion:**\n",
    "  Is the loss landscape in Scenario A simpler or more complex than in Scenario B? Is it multimodal or unimodal? If so, is it thinkable the lambda minimizer in Scenario B can be found numerically? Where does the difficulty in Scenario B come from: the non-linearity of the problem or a complex loss landscape?\n",
    "\n",
    "3. **Report**  \n",
    "   - Summarize your theoretical insights and empirical findings in a **Colab notebook**.\n",
    "   - Include the relevant proofs, code, discussion, and conclusions.\n",
    "   - Place the notebook in your **GitHub repository** for this course, add a link to it in your README.md and add an **“Open in Colab”** badge in the notebook so it can be launched directly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0318a845-23f8-4aa9-885e-e233b4198c77",
   "metadata": {},
   "source": [
    "# Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81b4229-a0cb-4048-af7c-43cf55d588ab",
   "metadata": {},
   "source": [
    "Firstly, we are going to find closed form solution for **Scenerio A**. We have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a3a362-803e-437f-851b-c60ec95974a6",
   "metadata": {},
   "source": [
    "$$\n",
    "\\lambda^*\n",
    "= \\arg\\min_{\\lambda} \\sum_{i=1}^n L(y_i,\\lambda)\n",
    "= \\arg\\min_{\\lambda} \\Bigl[-m\\ln\\sigma(\\lambda) - k\\ln\\bigl(1-\\sigma(\\lambda)\\bigr)\\Bigr]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72ae4f6-0685-4458-99ac-2871de120612",
   "metadata": {},
   "source": [
    "Let's calculate the derivative of $\\Bigl[-m\\ln\\sigma(\\lambda) - k\\ln\\bigl(1-\\sigma(\\lambda)\\bigr)\\Bigr]$ with respect to $\\lambda$. Notice that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b621e7-d0e8-4054-aa16-5f1882dd3dcf",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sigma'(\\lambda) = \\sigma (\\lambda) (1 - \\sigma(\\lambda)).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01affed9-d2f8-45ee-935a-665287d208d5",
   "metadata": {},
   "source": [
    "And using this fact we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7406df8-5434-4d31-be45-ba38b6ff7a65",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Bigl[-m\\ln\\sigma(\\lambda) - k\\ln\\bigl(1-\\sigma(\\lambda)\\bigr)\\Bigr]' = -m(1 - \\sigma(\\lambda)) + k \\sigma(\\lambda)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7766391e-fc0c-4faa-927b-2d997c1005fe",
   "metadata": {},
   "source": [
    "By setting this to zero we can conclude"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473cb37d-eb34-489c-826e-5bfd4c78aaad",
   "metadata": {},
   "source": [
    "$$\n",
    "-m + (m+k) \\sigma (\\lambda) = 0 \\implies \\sigma(\\lambda ^*) = \\frac{m}{n}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d71ae5e-244c-45a3-992b-b69e9e480b01",
   "metadata": {},
   "source": [
    "Finally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ac4a1f-261e-4a1c-b62b-771272f59945",
   "metadata": {},
   "source": [
    "$$\n",
    "\\lambda^* = \\log \\frac{m}{k}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d7f341-0ca2-4f17-a277-b1c7d0e5f7bd",
   "metadata": {},
   "source": [
    "which has a clearly interpretation - it is log odds of positive vs negative labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48b7689-c998-40de-a085-98d4257c6b9b",
   "metadata": {},
   "source": [
    "Let's focus on **Scenerio B**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec4fae1-ad76-41f9-922c-234da5317367",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
