{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf6611d0-e0e8-4735-ba13-bec4504b1441",
   "metadata": {},
   "source": [
    "# **Last Homework – Finding Odd Shapes**\n",
    "\n",
    "Due to the very sad events that took place on the Krakowskie Przedmieście Campus, the semester schedule has changed for the Monday and Thursday groups. To accommodate these changes, students in all groups receive a **combined homework assignment for both Class 13 and Class 14**.\n",
    "\n",
    "## **RULES**\n",
    "\n",
    "### **Deadline**\n",
    "\n",
    "**The deadline for this homework is July 2nd, 23:59.**\n",
    "\n",
    "### **Grading Criteria**\n",
    "\n",
    "This homework will be graded on a scale from 0 to 4 points - plus a **bonus**.\n",
    "\n",
    "- **4 points** (equivalent to completing two homework assignments worth 2 points each):\n",
    "  - **the solution must attain an RMSE of 5.0 pixels or lower** (on 25,000 samples).\n",
    "  - the sizes of all tensors must be annotated in comments\n",
    "  - training loss curve must be plotted with a clear indication of a 5.0 pixels level\n",
    "  - an in-depth textual description of the solution must be provided\n",
    "  - clear attention diagrams with discussion must be included\n",
    "\n",
    "- **3 points** (equivalent to completing one homework assignment worth 2 points):\n",
    "  - training loss curve must be plotted with a clear indication of a 5.0 pixels level\n",
    "  - the sizes of all tensors must be annotated in comments\n",
    "  - an in-depth textual description of the solution must be provided\n",
    "  - clear attention diagrams with discussion must be included\n",
    "\n",
    "- **2 points** (equivalent to completing one homework assignment worth 2 points):\n",
    "  - missing any one of the following elements:\n",
    "    - training loss curve plot with a clear indication of a 5.0 pixels level\n",
    "    - tensor size annotations in comments\n",
    "    - in-depth textual description of the solution\n",
    "    - attention diagrams with discussion\n",
    "\n",
    "- **1 point** (equivalent to submitting an incomplete homework assignment):\n",
    "  - missing any two of the following elements:\n",
    "    - training loss curve plot with a clear indication of a 5.0 pixels level\n",
    "    - tensor size annotations in comments\n",
    "    - in-depth textual description of the solution\n",
    "    - attention diagrams with discussion\n",
    "\n",
    "- **0 points** (equivalent to not completing the homework assignment):\n",
    "  - missing any three of the following:\n",
    "    - training loss curve plot with a clear indication of a 5.0 pixels level\n",
    "    - tensor size annotations in comments\n",
    "    - in-depth textual description of the solution\n",
    "    - attention diagrams with discussion\n",
    "\n",
    "- **BONUS** – This homework encompasses all key elements from previous classes on deep neural networks.  \n",
    "As a result, any student who obtains an **RMSE of 3.0 pixels or below** (on 25,000 samples), and whose solution would otherwise qualify for 4 points, **will receive the full 14 points** — equivalent to completing all homework assignments related to deep learning — regardless of their previous scores.\n",
    "\n",
    "\n",
    "\n",
    "## **HOMEWORK ASSIGNMENT DESCRIPTION AND INSTRUCTIONS**\n",
    "\n",
    "### **Online Dataset**\n",
    "\n",
    "The code provided below generates an *online* dataset `OddXYDataset`.\n",
    "\n",
    "The term *online* means that the dataset does not have a fixed set of pre-generated examples (like a traditional training set of fixed size). Instead, new samples are generated dynamically each time they are requested.\n",
    "\n",
    "While the number of possible examples is finite in principle, it is so large that — for practical purposes — we can consider it to be effectively infinite.\n",
    "\n",
    "In this setting, there is no need to use a separate validation or test set: the training error itself is a good estimate of the generalization error, since every training sample is new and unseen.\n",
    "\n",
    "Consequently, the notion of an *epoch* becomes a matter of convention. For this assignment, we define one epoch as processing 25,000 training examples.\n",
    "\n",
    "### **Training strategies**\n",
    "\n",
    "Overfitting is not a problem in the *online* setting — but training can still stagnate in local minima or flat regions of the loss landscape.  \n",
    "To address this, you will likely need to try one or more of the following strategies:\n",
    "\n",
    "- **Multiple restarts** with different random seed values;\n",
    "- **Adaptive learning rate** — consider researching training schedulers (this topic was not covered in class);\n",
    "- **Progressive model growth** — start with a simpler architecture and gradually add components during training,\n",
    "  so that the parts already present can learn what to do before the rest is introduced.\n",
    "\n",
    "### **DataLoader**\n",
    "\n",
    "The `show_examples()` function demonstrate how to wrap the dataset into a `torch.utils.data.DataLoader` so it becomes directly usable for the training/testing of a neural network.\n",
    "\n",
    "### **Data Description**\n",
    "\n",
    "By examining the provided code and a few sample images generated from this dataset, you will notice that:\n",
    "\n",
    "1. **Each data sample** is a 64×64 black-and-white image with the following characteristics:\n",
    "  - It contains several shapes of the same type (either circles, triangles, or squares), randomly placed and varying in size;\n",
    "  - It includes one additional shape of a different type — the *odd* shape — also placed at a random location;\n",
    "  \n",
    "  **Note that these shapes may overlap partially or even completely, potentially hiding the odd shape.**\n",
    "\n",
    "2. **The label** associated with each image is a 2D point indicating the coordinates of the center of the odd shape.\n",
    "\n",
    "### **The Homework Objective**\n",
    "\n",
    "Students should design an architecture of an **attention-based neural network** and train it so that it attains an RMSE (Root Mean Square Error, defined as the square root of the MSE) of **5.0 or lower**. Due to the nature of the online dataset, there is no need to test the solution on a separate test set.\n",
    "\n",
    "Students should visualize the attention matrices in the trained network and **discuss what they observe** — not just describe them.  \n",
    "Focus on interpreting the patterns: Where is the model attending? Are there any consistent behaviors across samples? Does attention correlate with the position of the odd shape? What do surprising or unclear patterns tell us?\n",
    "\n",
    "A few technical requirements to observe:\n",
    "\n",
    "- Seed all random number generators so that (1) your results are replicable and (2) I can rerun your solution and obtain the same output — *in case I need to check something*.\n",
    "\n",
    "- Make sure your Colab file contains a **fully trained solution** with:\n",
    "  - printed training output,\n",
    "  - attention diagrams,\n",
    "  - and a plot of the training loss curve.\n",
    "\n",
    "This way, I don’t have to rerun your code unless absolutely necessary.\n",
    "\n",
    "### **Publish on GitHub**\n",
    "- Upload your Colab notebook to your **GitHub repository** for this course.\n",
    "- In your repository’s **README**, include a **link** to the notebook.\n",
    "- In the notebook include **“Open in Colab”** badge so the notebook can be launched directly from GitHub.\n",
    "\n",
    "## **SOLUTION SUGGESTIONS**\n",
    "\n",
    "Students **do not need to follow these suggestions** — these are simply the strategies that worked for me.\n",
    "\n",
    "The goal of the homework is, of course, to find the location of the odd shape. For that reason, it seems worthwhile to consider a *position-aware* variant of the post-processing of attention results. The solution I propose consists of the following components:\n",
    "\n",
    "\n",
    "0. **Input**  \n",
    "   Our input is a black-and-white image with 1 channel, of size 64×64 pixels.\n",
    "\n",
    "1. **Embedding**  \n",
    "  In class, we embedded a sequence of tokens in a multidimensional space, resulting in a sequence of embeddings. We also saw how such sequences are compatible with positional encoding and attention mechanisms used in subsequent stages.\n",
    "\n",
    "  Here, we need to process an image that contains shapes of interest at various spatial locations.  \n",
    "  To do this, I designed a Convolutional Neural Network with:\n",
    "  - 1 input channel,\n",
    "  - an input grid of 64x64 pixels,\n",
    "  - and an output feature map arranged as a 12x12 grid with 16 channels.\n",
    "\n",
    "  I used **no padding**, and each output neuron has a **receptive field of size 20x20** with a **stride of 4x4**.  \n",
    "  This architectural choice yields **144 distinct positions** (12x12), each represented by a 16-dimensional feature vector.\n",
    "\n",
    "  We can treat this as a sequence of 144 embeddings in 16-dimensional space — making it fully compatible with the attention mechanisms used later in the model.\n",
    "\n",
    "\n",
    "2. **Positional Encoding**  \n",
    "  I applied sinusoidal positional encoding, just like in class, using 16 positional dimensions to match the 16 feature dimensions of the image embeddings.  \n",
    "\n",
    "  Positions were encoded based on their indices from 0 to 143.\n",
    "\n",
    "3. **Attention**  \n",
    "   I used full self-attention, where input tokens are linearly projected into Query, Key, and Value vectors using learned matrices.\n",
    "\n",
    "4. **Post-Processing**  \n",
    "   The classifier was implemented as a 2-layer MLP and applied **token-wise**, without averaging over positions.  \n",
    "\n",
    "   This corresponds to the **position-aware** variant of post-processing the attention results, as discussed in class.\n",
    "\n",
    "   As a result, the model produced a **logit for each of the 144 spatial locations**, which was then converted with `softmax` into a **probability distribution** over positions — representing the likelihood of the odd shape being located at each position.\n",
    "\n",
    "\n",
    "5. **Final Prediction**  \n",
    "   We know the exact position (center) of each of the 144 rectangular receptive fields of the embedding network. Since we also have the **probability** of each field being the target location (from the classifier), we can compute the **expected position** as a weighted average of receptive field centers. In what follows I will call it *soft argmax*.\n",
    "\n",
    "   **Example:**\n",
    "\n",
    "   Suppose we had only 4 output rectangles (instead of 144), with centers at:  \n",
    "   $$(16, 16),\\ (16, 48),\\ (48, 16),\\ \\text{and } (48, 48).$$  \n",
    "   and suppose the probabilities from the post-processing stage were:  \n",
    "   $$(0.1,\\ 0.2,\\ 0.69999,\\ 0.00001).$$  \n",
    "   Then the predicted center would be:\n",
    "   $$\n",
    "   \\begin{align*}\n",
    "   x &= 16 \\cdot 0.1 + 16 \\cdot 0.2 + 48 \\cdot 0.69999 + 48 \\cdot 0.00001 \\\\\n",
    "   y &= 16 \\cdot 0.1 + 48 \\cdot 0.2 + 16 \\cdot 0.69999 + 48 \\cdot 0.00001\n",
    "   \\end{align*}\n",
    "   $$\n",
    "\n",
    "   which yields an interpretable, differentiable prediction for the (x, y) location of the odd shape (*soft argmax*).\n",
    "\n",
    "**The above steps bring the RMSE below 4.0 pixels.**  \n",
    "However, the result depends on the network initialization — it is seed-dependent — so it's worth restarting the training a few times with different seeds to find a better-performing run.\n",
    "\n",
    "To improve this result further, though, we'll have to work a bit harder. Here's how:\n",
    "\n",
    "\n",
    "6. **Offset Regressor**\n",
    "\n",
    "  The soft-argmax mechanism in step 5 identifies the **center** of the most probable receptive field, but it cannot fine-tune the prediction within that field. For example, if the odd shape is located in the **lower-left corner** of a receptive field, the best the model can do is predict the **center** of that rectangle — introducing a systematic error.\n",
    "\n",
    "  *One may argue:* the design naturally accounts for a more nuanced case. When the odd shape lies **across the boundary between two receptive fields**, the attention distribution may spread across both regions. In such cases, the soft-argmax prediction becomes a **weighted average** of the centers of the adjacent fields. This behavior is a built-in **feature of the design** that allows the model to predict positions **off-center**, somewhere in between fields.\n",
    "\n",
    "  *To that I would reply:* this is true — but it comes with a trade-off. When attention is distributed across multiple regions, it becomes **less clear which shapes are *regular* and which one is *odd***. This added ambiguity may make it harder for the network to reach a confident decision, especially in the early stages of training.\n",
    "\n",
    "  To address this systematic limitation in a more structured way, I introduced an additional **2-layer MLP regressor**, applied **token-wise** to each of the 144 positions. This regressor takes the same attention output used by the classifier in step 4 and predicts a **local offset** $(\\Delta x, \\Delta y)$ within each receptive field. These offsets are then **aggregated** using the probabilities from step 4, resulting in a **soft average correction vector**.\n",
    "\n",
    "  Note that the attention mechanism must learn to extract — from the original features — both:\n",
    "  - the **probability** that the odd shape is located at a given position, and  \n",
    "  - the **local coordinates** of that shape *within* the respective receptive field.\n",
    "\n",
    "\n",
    "7. **Refined Prediction**\n",
    "\n",
    "  The final predicted position is obtained by summing the **coarse prediction** from step 5 and the **fine-grained correction** from step 6. This allows the model to make accurate, differentiable predictions at **sub-receptive-field resolution**, resulting in significantly improved localization.\n",
    "\n",
    "  **In summary:**\n",
    "\n",
    "  - Step 5: predicts the expected center of the relevant receptive field (via soft-argmax);\n",
    "  - Step 6: estimates a fine-grained, attention-weighted offset within that field;\n",
    "  - Step 7: adds both components to produce the final prediction:\n",
    "\n",
    "$$\n",
    "\\text{final prediction} = \\underbrace{\\sum_i p_i \\cdot C_i}_{\\text{soft argmax}} + \\underbrace{\\sum_i p_i \\cdot (\\Delta x, \\Delta y)_i}_{\\text{fine correction offset}}\n",
    "$$\n",
    "\n",
    "\n",
    "However, I was not able to train this network end-to-end from scratch — it seems that the two heads (the classifier inferring probabilities and the regressor inferring fine-grained corrections) were not able to learn their roles *simultaneously*.  \n",
    "\n",
    "To solve this, I adopted a **progressive model growth** training strategy:\n",
    "\n",
    "- First, I trained a one-headed version of the network (without steps 6 and 7) until the RMSE reached approximately 4.0 — a clear indication that the probability distribution was being inferred correctly.\n",
    "- Then, I copied all weights into a new, complete two-headed network and continued training from that point.\n",
    "\n",
    "**This strategy brought the RMSE down to 2.65 pixels.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c44bc518-0e96-415d-a39c-36860205a780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 0. Imports & device\n",
    "# ============================================================\n",
    "import torch, random, math, numpy as np\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1. Online dataset: 64×64, target = (cx,cy) ∈ [0,63]²  (float32)\n",
    "# ============================================================\n",
    "IMAGE = 64\n",
    "SHAPES = (\"circle\", \"square\", \"triangle\")\n",
    "\n",
    "def draw_shape(drawer, shape_type, center_x, center_y, radius):\n",
    "    if shape_type == \"circle\":\n",
    "        drawer.ellipse([center_x - radius, center_y - radius,\n",
    "                        center_x + radius, center_y + radius], fill=\"black\")\n",
    "    elif shape_type == \"square\":\n",
    "        drawer.rectangle([center_x - radius, center_y - radius,\n",
    "                          center_x + radius, center_y + radius], fill=\"black\")\n",
    "    else:  # triangle\n",
    "        drawer.polygon([\n",
    "            (center_x, center_y - radius),\n",
    "            (center_x - radius, center_y + radius),\n",
    "            (center_x + radius, center_y + radius)\n",
    "        ], fill=\"black\")\n",
    "\n",
    "class OddXYDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Generates images on-the-fly:\n",
    "      * several random shapes of the same type\n",
    "      * 1 additional shape of a different type\n",
    "    Returns:\n",
    "      * image (1×64×64 tensor, float32 normalized to [0,1])\n",
    "      * label: float tensor [cx, cy] with center of the odd shape\n",
    "    Arguments:\n",
    "        num_samples             – total number of samples in the dataset\n",
    "        same_shape_count_range – tuple (min, max), number of identical shapes\n",
    "        shape_radius_range      – tuple (min_radius, max_radius) for shape size\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 num_samples,\n",
    "                 same_shape_count_range=(3, 6),\n",
    "                 shape_radius_range=(4, 10)):\n",
    "        self.num_samples = num_samples\n",
    "        self.same_shape_count_range = same_shape_count_range\n",
    "        self.radius_min, self.radius_max = shape_radius_range\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        base_shape = random.choice(SHAPES)\n",
    "        odd_shape = random.choice([s for s in SHAPES if s != base_shape])\n",
    "\n",
    "        img = Image.new(\"L\", (IMAGE, IMAGE), \"white\")\n",
    "        drawer = ImageDraw.Draw(img)\n",
    "\n",
    "        # draw identical shapes\n",
    "        for _ in range(random.randint(*self.same_shape_count_range)):\n",
    "            radius = random.randint(self.radius_min, self.radius_max)\n",
    "            cx = random.randint(radius, IMAGE - radius - 1)\n",
    "            cy = random.randint(radius, IMAGE - radius - 1)\n",
    "            draw_shape(drawer, base_shape, cx, cy, radius)\n",
    "\n",
    "        # draw the odd shape (with known center)\n",
    "        radius = random.randint(self.radius_min, self.radius_max)\n",
    "        cx = random.randint(radius, IMAGE - radius - 1)\n",
    "        cy = random.randint(radius, IMAGE - radius - 1)\n",
    "        draw_shape(drawer, odd_shape, cx, cy, radius)\n",
    "\n",
    "        img_tensor = torch.tensor(np.array(img), dtype=torch.float32).unsqueeze(0) / 255.\n",
    "        label_tensor = torch.tensor([float(cx), float(cy)], dtype=torch.float32)\n",
    "        return img_tensor, label_tensor\n",
    "\n",
    "def show_examples(num_examples=10):\n",
    "    \"\"\"\n",
    "    Displays a grid of image samples from OddXYDataset using a DataLoader with batch_size=1.\n",
    "\n",
    "    Args:\n",
    "        num_examples (int): Number of examples to display.\n",
    "    \"\"\"\n",
    "    dataset = OddXYDataset(num_samples=num_examples)\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    num_columns = 5\n",
    "    num_rows = math.ceil(num_examples / num_columns)\n",
    "\n",
    "    fig, axes = plt.subplots(num_rows, num_columns, figsize=(num_columns * 2, num_rows * 2))\n",
    "\n",
    "    for i, (image, label) in enumerate(dataloader):\n",
    "        if i >= num_examples:\n",
    "            break\n",
    "        image_np = image.squeeze(0).squeeze(0).numpy()  # B=1,C=1,H,W --> H,W\n",
    "        cx, cy = label.squeeze(0)                       # B=1,2       --> 2\n",
    "        axes.flat[i].imshow(image_np, cmap=\"gray\")\n",
    "        axes.flat[i].set_title(f\"({cx:.0f},{cy:.0f})\")\n",
    "        axes.flat[i].axis(\"off\")\n",
    "        axes.flat[i].scatter(cx, cy) # added\n",
    "\n",
    "    for j in range(i + 1, num_rows * num_columns):\n",
    "        axes.flat[j].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "show_examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a784e4d-523b-4f53-a0ae-618d468c45f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = OddXYDataset(25_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fae8d072-f836-4276-ac3f-046bc65c3e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=32 #bylo256\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f43bc618-a76b-4eba-9ae7-951ea6ea968b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedCNN(nn.Module):\n",
    "    def __init__(self, in_ch=1, out_ch=16, kernel=20, stride=4):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_ch, out_ch, kernel_size=kernel, stride=stride, padding=0)\n",
    "\n",
    "    def forward(self, x):              # x: [B, 1, 64, 64]\n",
    "        x = self.conv(x)               # -> [B, 16, 12, 12]\n",
    "        x = x.flatten(2).transpose(1, 2)   # -> [B, 144, 16]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4418a7b0-e93d-4615-a91c-fa04af016ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model=16, max_len=144):\n",
    "        super().__init__()\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "\n",
    "        pos = torch.arange(max_len).float().unsqueeze(1)          # [L, 1]\n",
    "\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-math.log(10000) / d_model)\n",
    "        )                                                         # [d_model // 2]\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(pos * div_term)\n",
    "        pe[:, 1::2] = torch.cos(pos * div_term)\n",
    "\n",
    "        self.pe = pe\n",
    "\n",
    "    def forward(self, x):          # x: [B, L, d_model]\n",
    "        L = x.size(1)              # actual sequence length\n",
    "        return x + self.pe[:L].to(x.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aef97b36-8fbd-4a06-bb76-87b144b1bebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(torch.nn.Module):\n",
    "    def __init__(self, input_dim = 16, output_dim = 16, k = 8):\n",
    "        super().__init__()\n",
    "        self.Q = torch.nn.Linear(input_dim, k)        # query projection d x k\n",
    "        self.K = torch.nn.Linear(input_dim, k)        # key projection d x k\n",
    "        self.V = torch.nn.Linear(input_dim, output_dim)      # value projection d x d_v\n",
    "        self.k = k\n",
    "        self.attention = None  # will store softmaxed attention weights for inspection later\n",
    "\n",
    "    def forward(self, features):                        # size: batch, words, d\n",
    "        queries = self.Q(features)                      # size: batch, words, k\n",
    "        keys = self.K(features)                         # size: batch, words, k\n",
    "        values = self.V(features)                       # size: batch, words, d_v\n",
    "\n",
    "        # Compute energy scores:\n",
    "        energies = queries @ keys.transpose(-2, -1)     # size: batch, words, words\n",
    "        energies = energies / (self.k ** 0.5)                # it is an additional rescale helping the stability\n",
    "\n",
    "        # Normalize energies into attention weights\n",
    "        self.attention = F.softmax(energies, dim=-1)         # size: batch, words, words\n",
    "\n",
    "        # Apply attention weights to values:\n",
    "        output = self.attention @ values                     # size: batch, words, d_v\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca702ac5-ca79-4918-8bff-6abbb317d8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=16, hidden=32):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, 1)         # logit per token\n",
    "        )\n",
    "\n",
    "    def forward(self, x):                # [B, L, 16]\n",
    "        logits = self.mlp(x).squeeze(-1) # [B, L]\n",
    "        probs  = F.softmax(logits, dim=-1)\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0f3d2f5-73dc-471d-a3a9-72e90ba87b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftArgmax(nn.Module):\n",
    "    def __init__(self, kernel=20, stride=4, grid=12):\n",
    "        super().__init__()\n",
    "\n",
    "        # centers of 20×20 windows, sampled every 4 px\n",
    "        xx, yy = torch.meshgrid(\n",
    "            torch.arange(grid), torch.arange(grid), indexing=\"xy\"\n",
    "        )                                           # [G, G]\n",
    "        centers = torch.stack([xx, yy], -1).float() \\\n",
    "                  * stride + (kernel - 1) / 2       # [G, G, 2]\n",
    "\n",
    "        self.centers = centers.reshape(-1, 2)       # [144, 2]\n",
    "\n",
    "    def forward(self, probs):       # probs: [B, 144] (rows sum to 1)\n",
    "        return probs @ self.centers.to(probs.device)  # [B, 2]  → (y, x)\n",
    "\n",
    "\n",
    "class OddShapeDetector(nn.Module):\n",
    "    def __init__(self, input_attention=64 ,output_attention=64, k=32, hidden=128):  #64,64,32,128\n",
    "        super().__init__()\n",
    "        self.patch = PatchEmbedCNN(out_ch = input_attention)\n",
    "        self.pos   = PositionalEncoding(d_model = input_attention)\n",
    "        self.attn  = SelfAttention(input_dim = input_attention, output_dim = output_attention, k = k)\n",
    "        self.cls   = TokenClassifier(input_dim = output_attention, hidden = hidden)\n",
    "        self.sarg  = SoftArgmax()\n",
    "\n",
    "    def forward(self, img):            # [B, 1, 64, 64]\n",
    "        x = self.patch(img)            # [B, 144, 16]\n",
    "        x = self.pos(x)\n",
    "        x = self.attn(x)\n",
    "        probs = self.cls(x)    # [B, 144]\n",
    "        coords = self.sarg(probs)      # [B, 2] (x, y w px)\n",
    "        return coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91dfcb77-aba8-4992-82fa-fe2b44d61396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  1  loss 178.9685  lr 1.000e-03\n",
      "epoch  2  loss 170.1701  lr 1.000e-03\n",
      "epoch  3  loss 159.1824  lr 1.000e-03\n",
      "epoch  4  loss 147.6074  lr 1.000e-03\n",
      "epoch  5  loss 133.4387  lr 1.000e-03\n",
      "epoch  6  loss 123.1566  lr 1.000e-03\n",
      "epoch  7  loss 117.9666  lr 1.000e-03\n",
      "epoch  8  loss 114.3325  lr 1.000e-03\n",
      "epoch  9  loss 110.1476  lr 1.000e-03\n",
      "epoch 10  loss 107.0695  lr 1.000e-03\n",
      "epoch 11  loss 106.3147  lr 1.000e-03\n",
      "epoch 12  loss 104.7999  lr 1.000e-03\n",
      "epoch 13  loss 104.0030  lr 1.000e-03\n",
      "epoch 14  loss 100.0877  lr 1.000e-03\n",
      "epoch 15  loss 98.0076  lr 1.000e-03\n",
      "epoch 16  loss 97.6103  lr 1.000e-03\n",
      "epoch 17  loss 98.1945  lr 1.000e-03\n",
      "epoch 18  loss 95.9789  lr 1.000e-03\n",
      "epoch 19  loss 96.9719  lr 1.000e-03\n",
      "epoch 20  loss 92.3379  lr 1.000e-03\n",
      "epoch 21  loss 93.0692  lr 1.000e-03\n",
      "epoch 22  loss 92.7252  lr 1.000e-03\n",
      "epoch 23  loss 91.1558  lr 1.000e-03\n",
      "epoch 24  loss 91.2046  lr 1.000e-03\n",
      "epoch 25  loss 90.8252  lr 1.000e-03\n",
      "epoch 26  loss 88.2085  lr 1.000e-03\n",
      "epoch 27  loss 88.1641  lr 1.000e-03\n",
      "epoch 28  loss 89.1685  lr 1.000e-03\n",
      "epoch 29  loss 89.2329  lr 1.000e-03\n",
      "epoch 30  loss 85.4025  lr 1.000e-03\n",
      "epoch 31  loss 85.5788  lr 1.000e-03\n",
      "epoch 32  loss 87.3179  lr 1.000e-03\n",
      "epoch 33  loss 83.3218  lr 1.000e-03\n",
      "epoch 34  loss 83.2475  lr 1.000e-03\n",
      "epoch 35  loss 81.0322  lr 1.000e-03\n",
      "epoch 36  loss 80.9768  lr 1.000e-03\n",
      "epoch 37  loss 81.3558  lr 1.000e-03\n",
      "epoch 38  loss 81.5280  lr 1.000e-03\n",
      "epoch 39  loss 81.8791  lr 1.000e-03\n",
      "epoch 40  loss 80.8833  lr 1.000e-03\n",
      "epoch 41  loss 79.3426  lr 1.000e-03\n",
      "epoch 42  loss 78.2044  lr 1.000e-03\n",
      "epoch 43  loss 79.8350  lr 1.000e-03\n",
      "epoch 44  loss 79.7384  lr 1.000e-03\n",
      "epoch 45  loss 80.0815  lr 1.000e-03\n",
      "epoch 46  loss 79.0899  lr 1.000e-03\n",
      "epoch 47  loss 76.5616  lr 1.000e-03\n",
      "epoch 48  loss 79.9575  lr 1.000e-03\n",
      "epoch 49  loss 79.3574  lr 1.000e-03\n",
      "epoch 50  loss 76.0944  lr 1.000e-03\n",
      "epoch 51  loss 76.4217  lr 1.000e-03\n",
      "epoch 52  loss 77.9373  lr 1.000e-03\n",
      "epoch 53  loss 77.1077  lr 1.000e-03\n",
      "epoch 54  loss 74.3274  lr 1.000e-03\n",
      "epoch 55  loss 76.4499  lr 1.000e-03\n",
      "epoch 56  loss 77.2052  lr 1.000e-03\n",
      "epoch 57  loss 73.3947  lr 1.000e-03\n",
      "epoch 58  loss 75.9321  lr 1.000e-03\n",
      "epoch 59  loss 75.6484  lr 1.000e-03\n",
      "epoch 60  loss 74.9197  lr 1.000e-03\n",
      "epoch 61  loss 73.8471  lr 1.000e-03\n",
      "epoch 62  loss 76.4033  lr 1.000e-03\n",
      "epoch 63  loss 73.0321  lr 1.000e-03\n",
      "epoch 64  loss 72.1291  lr 1.000e-03\n",
      "epoch 65  loss 73.7158  lr 1.000e-03\n",
      "epoch 66  loss 72.3986  lr 1.000e-03\n",
      "epoch 67  loss 72.2893  lr 1.000e-03\n",
      "epoch 68  loss 74.7679  lr 1.000e-03\n",
      "epoch 69  loss 73.9329  lr 1.000e-03\n",
      "epoch 70  loss 71.9447  lr 1.000e-03\n",
      "epoch 71  loss 72.0205  lr 1.000e-03\n",
      "epoch 72  loss 73.6568  lr 1.000e-03\n",
      "epoch 73  loss 72.2808  lr 1.000e-03\n",
      "epoch 74  loss 73.2422  lr 1.000e-03\n",
      "epoch 75  loss 72.3471  lr 1.000e-03\n",
      "epoch 76  loss 70.3502  lr 1.000e-03\n",
      "epoch 77  loss 72.9265  lr 1.000e-03\n",
      "epoch 78  loss 70.2178  lr 1.000e-03\n",
      "epoch 79  loss 72.1676  lr 1.000e-03\n",
      "epoch 80  loss 70.0675  lr 1.000e-03\n",
      "epoch 81  loss 71.9495  lr 1.000e-03\n",
      "epoch 82  loss 69.2541  lr 1.000e-03\n",
      "epoch 83  loss 69.6519  lr 1.000e-03\n",
      "epoch 84  loss 68.7153  lr 1.000e-03\n",
      "epoch 85  loss 69.0672  lr 1.000e-03\n",
      "epoch 86  loss 68.6948  lr 1.000e-03\n",
      "epoch 87  loss 69.3202  lr 1.000e-03\n",
      "epoch 88  loss 69.1802  lr 1.000e-03\n",
      "epoch 89  loss 68.2789  lr 1.000e-03\n",
      "epoch 90  loss 70.0244  lr 1.000e-03\n",
      "epoch 91  loss 68.9854  lr 1.000e-03\n",
      "epoch 92  loss 68.9332  lr 1.000e-03\n",
      "epoch 93  loss 67.9118  lr 1.000e-03\n",
      "epoch 94  loss 66.7228  lr 1.000e-03\n",
      "epoch 95  loss 66.8851  lr 1.000e-03\n",
      "epoch 96  loss 68.4184  lr 1.000e-03\n",
      "epoch 97  loss 66.3288  lr 1.000e-03\n",
      "epoch 98  loss 67.8152  lr 1.000e-03\n",
      "epoch 99  loss 68.0282  lr 1.000e-03\n",
      "epoch 100  loss 68.3267  lr 1.000e-03\n",
      "epoch 101  loss 67.5708  lr 1.000e-03\n",
      "epoch 102  loss 68.1906  lr 1.000e-03\n",
      "epoch 103  loss 67.6262  lr 1.000e-03\n",
      "epoch 104  loss 65.1894  lr 1.000e-03\n",
      "epoch 105  loss 68.6809  lr 1.000e-03\n",
      "epoch 106  loss 66.0805  lr 1.000e-03\n",
      "epoch 107  loss 65.8907  lr 1.000e-03\n",
      "epoch 108  loss 66.5848  lr 1.000e-03\n",
      "epoch 109  loss 66.1213  lr 1.000e-03\n",
      "epoch 110  loss 66.2237  lr 1.000e-03\n",
      "epoch 111  loss 66.3362  lr 1.000e-03\n",
      "epoch 112  loss 65.6237  lr 1.000e-03\n",
      "epoch 113  loss 67.0318  lr 1.000e-03\n",
      "epoch 114  loss 65.5661  lr 1.000e-03\n",
      "epoch 115  loss 63.8882  lr 1.000e-03\n",
      "epoch 116  loss 63.4859  lr 1.000e-03\n",
      "epoch 117  loss 63.1525  lr 1.000e-03\n",
      "epoch 118  loss 67.3934  lr 1.000e-03\n",
      "epoch 119  loss 65.2615  lr 1.000e-03\n",
      "epoch 120  loss 67.5195  lr 1.000e-03\n",
      "epoch 121  loss 65.2800  lr 1.000e-03\n",
      "epoch 122  loss 63.7163  lr 1.000e-03\n",
      "epoch 123  loss 65.4268  lr 1.000e-03\n",
      "epoch 124  loss 66.5470  lr 1.000e-03\n",
      "epoch 125  loss 67.1249  lr 1.000e-03\n",
      "epoch 126  loss 64.4474  lr 1.000e-03\n",
      "epoch 127  loss 64.6117  lr 1.000e-03\n",
      "epoch 128  loss 61.7507  lr 1.000e-03\n",
      "epoch 129  loss 63.3852  lr 1.000e-03\n",
      "epoch 130  loss 64.5955  lr 1.000e-03\n",
      "epoch 131  loss 63.9712  lr 1.000e-03\n",
      "epoch 132  loss 63.7546  lr 1.000e-03\n",
      "epoch 133  loss 64.8896  lr 1.000e-03\n",
      "epoch 134  loss 62.8581  lr 1.000e-03\n",
      "epoch 135  loss 63.3872  lr 1.000e-03\n",
      "epoch 136  loss 63.2523  lr 1.000e-03\n",
      "epoch 137  loss 62.8432  lr 1.000e-03\n",
      "epoch 138  loss 65.2408  lr 1.000e-03\n",
      "epoch 139  loss 65.3427  lr 1.000e-03\n",
      "epoch 140  loss 62.6402  lr 1.000e-03\n",
      "epoch 141  loss 65.5175  lr 1.000e-03\n",
      "epoch 142  loss 62.4731  lr 1.000e-03\n",
      "epoch 143  loss 62.9867  lr 1.000e-03\n",
      "epoch 144  loss 64.4383  lr 1.000e-03\n",
      "epoch 145  loss 62.8286  lr 1.000e-03\n",
      "epoch 146  loss 63.3316  lr 1.000e-03\n",
      "epoch 147  loss 61.3090  lr 1.000e-03\n",
      "epoch 148  loss 63.6100  lr 1.000e-03\n",
      "epoch 149  loss 64.0744  lr 1.000e-03\n",
      "epoch 150  loss 61.3732  lr 1.000e-03\n",
      "epoch 151  loss 61.9365  lr 1.000e-03\n",
      "epoch 152  loss 62.1183  lr 1.000e-03\n",
      "epoch 153  loss 62.1595  lr 1.000e-03\n",
      "epoch 154  loss 63.3997  lr 1.000e-03\n",
      "epoch 155  loss 62.0579  lr 1.000e-03\n",
      "epoch 156  loss 61.7473  lr 1.000e-03\n",
      "epoch 157  loss 61.3014  lr 1.000e-03\n",
      "epoch 158  loss 62.1265  lr 1.000e-03\n",
      "epoch 159  loss 62.3751  lr 1.000e-03\n",
      "epoch 160  loss 63.6183  lr 1.000e-03\n",
      "epoch 161  loss 61.0424  lr 1.000e-03\n",
      "epoch 162  loss 61.6483  lr 1.000e-03\n",
      "epoch 163  loss 60.9316  lr 1.000e-03\n",
      "epoch 164  loss 62.7168  lr 1.000e-03\n",
      "epoch 165  loss 60.4842  lr 1.000e-03\n",
      "epoch 166  loss 60.5263  lr 1.000e-03\n",
      "epoch 167  loss 61.8535  lr 1.000e-03\n",
      "epoch 168  loss 62.5722  lr 1.000e-03\n",
      "epoch 169  loss 62.4196  lr 1.000e-03\n",
      "epoch 170  loss 61.3054  lr 1.000e-03\n",
      "epoch 171  loss 61.1382  lr 1.000e-03\n",
      "epoch 172  loss 60.9540  lr 1.000e-03\n",
      "epoch 173  loss 61.9342  lr 1.000e-03\n",
      "epoch 174  loss 62.1868  lr 1.000e-03\n",
      "epoch 175  loss 61.3800  lr 1.000e-03\n",
      "epoch 176  loss 61.3494  lr 1.000e-03\n",
      "epoch 177  loss 61.5837  lr 1.000e-03\n",
      "epoch 178  loss 61.2562  lr 1.000e-03\n",
      "epoch 179  loss 60.2476  lr 1.000e-03\n",
      "epoch 180  loss 59.8062  lr 1.000e-03\n",
      "epoch 181  loss 60.9745  lr 1.000e-03\n",
      "epoch 182  loss 61.8116  lr 1.000e-03\n",
      "epoch 183  loss 59.4287  lr 1.000e-03\n",
      "epoch 184  loss 60.4579  lr 1.000e-03\n",
      "epoch 185  loss 59.9362  lr 1.000e-03\n",
      "epoch 186  loss 59.6630  lr 1.000e-03\n",
      "epoch 187  loss 60.8599  lr 1.000e-03\n",
      "epoch 188  loss 61.9012  lr 1.000e-03\n",
      "epoch 189  loss 62.0439  lr 1.000e-03\n",
      "epoch 190  loss 58.6594  lr 1.000e-03\n",
      "epoch 191  loss 60.3113  lr 1.000e-03\n",
      "epoch 192  loss 59.0781  lr 1.000e-03\n",
      "epoch 193  loss 59.5034  lr 1.000e-03\n",
      "epoch 194  loss 61.0637  lr 1.000e-03\n",
      "epoch 195  loss 60.5619  lr 1.000e-03\n",
      "epoch 196  loss 60.8061  lr 1.000e-03\n",
      "epoch 197  loss 59.2408  lr 1.000e-03\n",
      "epoch 198  loss 60.6767  lr 1.000e-03\n",
      "epoch 199  loss 59.0279  lr 1.000e-03\n",
      "epoch 200  loss 60.2430  lr 1.000e-03\n",
      "epoch 201  loss 60.4456  lr 1.000e-03\n",
      "epoch 202  loss 61.9735  lr 5.000e-04\n",
      "epoch 203  loss 52.9078  lr 5.000e-04\n",
      "epoch 204  loss 50.5302  lr 5.000e-04\n",
      "epoch 205  loss 51.8571  lr 5.000e-04\n",
      "epoch 206  loss 52.0612  lr 5.000e-04\n",
      "epoch 207  loss 53.1582  lr 5.000e-04\n",
      "epoch 208  loss 51.5155  lr 5.000e-04\n",
      "epoch 209  loss 51.7989  lr 5.000e-04\n",
      "epoch 210  loss 50.0533  lr 5.000e-04\n",
      "epoch 211  loss 51.9639  lr 5.000e-04\n",
      "epoch 212  loss 53.3166  lr 5.000e-04\n",
      "epoch 213  loss 52.7537  lr 5.000e-04\n",
      "epoch 214  loss 50.2568  lr 5.000e-04\n",
      "epoch 215  loss 51.4167  lr 5.000e-04\n",
      "epoch 216  loss 51.8945  lr 5.000e-04\n",
      "epoch 217  loss 52.5547  lr 5.000e-04\n",
      "epoch 218  loss 52.1956  lr 5.000e-04\n",
      "epoch 219  loss 51.3808  lr 5.000e-04\n",
      "epoch 220  loss 50.9104  lr 5.000e-04\n",
      "epoch 221  loss 50.7990  lr 5.000e-04\n",
      "epoch 222  loss 50.7237  lr 5.000e-04\n",
      "epoch 223  loss 49.8003  lr 5.000e-04\n",
      "epoch 224  loss 51.1074  lr 5.000e-04\n",
      "epoch 225  loss 52.5451  lr 5.000e-04\n",
      "epoch 226  loss 50.6540  lr 5.000e-04\n",
      "epoch 227  loss 51.6935  lr 5.000e-04\n",
      "epoch 228  loss 52.0926  lr 5.000e-04\n",
      "epoch 229  loss 51.3479  lr 5.000e-04\n",
      "epoch 230  loss 50.8157  lr 5.000e-04\n",
      "epoch 231  loss 50.0543  lr 5.000e-04\n",
      "epoch 232  loss 50.8945  lr 5.000e-04\n",
      "epoch 233  loss 51.6011  lr 5.000e-04\n",
      "epoch 234  loss 50.7763  lr 5.000e-04\n",
      "epoch 235  loss 50.0573  lr 5.000e-04\n",
      "epoch 236  loss 49.7021  lr 5.000e-04\n",
      "epoch 237  loss 49.6993  lr 5.000e-04\n",
      "epoch 238  loss 49.3692  lr 5.000e-04\n",
      "epoch 239  loss 49.7900  lr 5.000e-04\n",
      "epoch 240  loss 50.7026  lr 5.000e-04\n",
      "epoch 241  loss 50.2449  lr 5.000e-04\n",
      "epoch 242  loss 50.5855  lr 5.000e-04\n",
      "epoch 243  loss 50.4466  lr 5.000e-04\n",
      "epoch 244  loss 50.1956  lr 5.000e-04\n",
      "epoch 245  loss 50.3388  lr 5.000e-04\n",
      "epoch 246  loss 49.9197  lr 5.000e-04\n",
      "epoch 247  loss 49.6078  lr 5.000e-04\n",
      "epoch 248  loss 51.2239  lr 5.000e-04\n",
      "epoch 249  loss 49.8541  lr 5.000e-04\n",
      "epoch 250  loss 49.7142  lr 5.000e-04\n",
      "epoch 251  loss 51.3937  lr 5.000e-04\n",
      "epoch 252  loss 50.0696  lr 5.000e-04\n",
      "epoch 253  loss 49.2020  lr 5.000e-04\n",
      "epoch 254  loss 49.6086  lr 5.000e-04\n",
      "epoch 255  loss 48.9967  lr 5.000e-04\n",
      "epoch 256  loss 50.6003  lr 5.000e-04\n",
      "epoch 257  loss 50.7936  lr 5.000e-04\n",
      "epoch 258  loss 50.4829  lr 5.000e-04\n",
      "epoch 259  loss 50.4447  lr 5.000e-04\n",
      "epoch 260  loss 51.0997  lr 5.000e-04\n",
      "epoch 261  loss 49.5031  lr 5.000e-04\n",
      "epoch 262  loss 49.6703  lr 5.000e-04\n",
      "epoch 263  loss 50.5690  lr 5.000e-04\n",
      "epoch 264  loss 48.8019  lr 5.000e-04\n",
      "epoch 265  loss 48.3340  lr 5.000e-04\n",
      "epoch 266  loss 48.7684  lr 5.000e-04\n",
      "epoch 267  loss 49.9423  lr 5.000e-04\n",
      "epoch 268  loss 48.9880  lr 5.000e-04\n",
      "epoch 269  loss 49.5301  lr 5.000e-04\n",
      "epoch 270  loss 49.9860  lr 5.000e-04\n",
      "epoch 271  loss 50.8529  lr 5.000e-04\n",
      "epoch 272  loss 49.8788  lr 5.000e-04\n",
      "epoch 273  loss 49.5280  lr 5.000e-04\n",
      "epoch 274  loss 48.6891  lr 5.000e-04\n",
      "epoch 275  loss 50.8570  lr 5.000e-04\n",
      "epoch 276  loss 50.0648  lr 5.000e-04\n",
      "epoch 277  loss 50.6803  lr 5.000e-04\n",
      "epoch 278  loss 48.9293  lr 5.000e-04\n",
      "epoch 279  loss 50.0375  lr 5.000e-04\n",
      "epoch 280  loss 47.6163  lr 5.000e-04\n",
      "epoch 281  loss 49.6266  lr 5.000e-04\n",
      "epoch 282  loss 50.2649  lr 5.000e-04\n",
      "epoch 283  loss 48.7772  lr 5.000e-04\n",
      "epoch 284  loss 49.2906  lr 5.000e-04\n",
      "epoch 285  loss 49.7062  lr 5.000e-04\n",
      "epoch 286  loss 49.2903  lr 5.000e-04\n",
      "epoch 287  loss 49.0459  lr 5.000e-04\n",
      "epoch 288  loss 49.2996  lr 5.000e-04\n",
      "epoch 289  loss 49.6595  lr 5.000e-04\n",
      "epoch 290  loss 50.1663  lr 5.000e-04\n",
      "epoch 291  loss 49.2398  lr 5.000e-04\n",
      "epoch 292  loss 48.5356  lr 5.000e-04\n",
      "epoch 293  loss 49.6941  lr 5.000e-04\n",
      "epoch 294  loss 49.7659  lr 5.000e-04\n",
      "epoch 295  loss 49.6375  lr 5.000e-04\n",
      "epoch 296  loss 48.6974  lr 5.000e-04\n",
      "epoch 297  loss 50.4293  lr 5.000e-04\n",
      "epoch 298  loss 50.9573  lr 5.000e-04\n",
      "epoch 299  loss 48.6854  lr 5.000e-04\n",
      "epoch 300  loss 48.4460  lr 5.000e-04\n",
      "epoch 301  loss 47.3684  lr 5.000e-04\n",
      "epoch 302  loss 50.0025  lr 5.000e-04\n",
      "epoch 303  loss 48.4323  lr 5.000e-04\n",
      "epoch 304  loss 46.8309  lr 5.000e-04\n",
      "epoch 305  loss 48.1442  lr 5.000e-04\n",
      "epoch 306  loss 48.3421  lr 5.000e-04\n",
      "epoch 307  loss 47.5435  lr 5.000e-04\n",
      "epoch 308  loss 48.4020  lr 5.000e-04\n",
      "epoch 309  loss 48.8544  lr 5.000e-04\n",
      "epoch 310  loss 49.3528  lr 5.000e-04\n",
      "epoch 311  loss 49.6215  lr 5.000e-04\n",
      "epoch 312  loss 48.3045  lr 5.000e-04\n",
      "epoch 313  loss 49.4108  lr 5.000e-04\n",
      "epoch 314  loss 47.9417  lr 5.000e-04\n",
      "epoch 315  loss 47.8673  lr 5.000e-04\n",
      "epoch 316  loss 49.3434  lr 5.000e-04\n",
      "epoch 317  loss 47.3377  lr 5.000e-04\n",
      "epoch 318  loss 49.4382  lr 5.000e-04\n",
      "epoch 319  loss 48.5142  lr 5.000e-04\n",
      "epoch 320  loss 47.3599  lr 5.000e-04\n",
      "epoch 321  loss 47.8006  lr 5.000e-04\n",
      "epoch 322  loss 48.1133  lr 5.000e-04\n",
      "epoch 323  loss 49.4535  lr 5.000e-04\n",
      "epoch 324  loss 48.4112  lr 5.000e-04\n",
      "epoch 325  loss 48.3244  lr 5.000e-04\n",
      "epoch 326  loss 50.3084  lr 5.000e-04\n",
      "epoch 327  loss 48.3655  lr 5.000e-04\n",
      "epoch 328  loss 48.2772  lr 5.000e-04\n",
      "epoch 329  loss 47.1878  lr 5.000e-04\n",
      "epoch 330  loss 47.5691  lr 5.000e-04\n",
      "epoch 331  loss 48.1741  lr 5.000e-04\n",
      "epoch 332  loss 47.8366  lr 5.000e-04\n",
      "epoch 333  loss 47.8591  lr 5.000e-04\n",
      "epoch 334  loss 48.3304  lr 5.000e-04\n",
      "epoch 335  loss 46.8314  lr 5.000e-04\n",
      "epoch 336  loss 49.1535  lr 5.000e-04\n",
      "epoch 337  loss 47.7630  lr 5.000e-04\n",
      "epoch 338  loss 46.9502  lr 5.000e-04\n",
      "epoch 339  loss 47.5007  lr 2.500e-04\n",
      "epoch 340  loss 45.8743  lr 2.500e-04\n",
      "epoch 341  loss 44.0823  lr 2.500e-04\n",
      "epoch 342  loss 44.9534  lr 2.500e-04\n",
      "epoch 343  loss 41.9470  lr 2.500e-04\n",
      "epoch 344  loss 42.9262  lr 2.500e-04\n",
      "epoch 345  loss 44.7754  lr 2.500e-04\n",
      "epoch 346  loss 44.3796  lr 2.500e-04\n",
      "epoch 347  loss 43.9609  lr 2.500e-04\n",
      "epoch 348  loss 43.0862  lr 2.500e-04\n",
      "epoch 349  loss 44.6436  lr 2.500e-04\n",
      "epoch 350  loss 45.1959  lr 2.500e-04\n",
      "epoch 351  loss 44.7710  lr 2.500e-04\n",
      "epoch 352  loss 43.1136  lr 2.500e-04\n",
      "epoch 353  loss 44.2796  lr 2.500e-04\n",
      "epoch 354  loss 43.4428  lr 2.500e-04\n",
      "epoch 355  loss 43.6003  lr 2.500e-04\n",
      "epoch 356  loss 43.7979  lr 2.500e-04\n",
      "epoch 357  loss 43.6130  lr 2.500e-04\n",
      "epoch 358  loss 43.7785  lr 2.500e-04\n",
      "epoch 359  loss 42.0793  lr 2.500e-04\n",
      "epoch 360  loss 44.2983  lr 2.500e-04\n",
      "epoch 361  loss 45.1134  lr 2.500e-04\n",
      "epoch 362  loss 43.5324  lr 2.500e-04\n",
      "epoch 363  loss 42.8013  lr 2.500e-04\n",
      "epoch 364  loss 42.9593  lr 2.500e-04\n",
      "epoch 365  loss 42.3054  lr 2.500e-04\n",
      "epoch 366  loss 41.9798  lr 2.500e-04\n",
      "epoch 367  loss 43.8462  lr 2.500e-04\n",
      "epoch 368  loss 43.1100  lr 2.500e-04\n",
      "epoch 369  loss 44.3187  lr 2.500e-04\n",
      "epoch 370  loss 43.1753  lr 2.500e-04\n",
      "epoch 371  loss 43.9060  lr 2.500e-04\n",
      "epoch 372  loss 43.1408  lr 2.500e-04\n",
      "epoch 373  loss 41.8227  lr 2.500e-04\n",
      "epoch 374  loss 41.3287  lr 2.500e-04\n",
      "epoch 375  loss 42.2247  lr 2.500e-04\n",
      "epoch 376  loss 42.1429  lr 2.500e-04\n",
      "epoch 377  loss 43.9811  lr 2.500e-04\n",
      "epoch 378  loss 43.4949  lr 2.500e-04\n",
      "epoch 379  loss 43.2640  lr 2.500e-04\n",
      "epoch 380  loss 41.5156  lr 2.500e-04\n",
      "epoch 381  loss 43.5414  lr 2.500e-04\n",
      "epoch 382  loss 42.6022  lr 2.500e-04\n",
      "epoch 383  loss 42.8110  lr 2.500e-04\n",
      "epoch 384  loss 43.3381  lr 2.500e-04\n",
      "epoch 385  loss 43.8451  lr 2.500e-04\n",
      "epoch 386  loss 42.5805  lr 2.500e-04\n",
      "epoch 387  loss 42.8805  lr 2.500e-04\n",
      "epoch 388  loss 42.8870  lr 2.500e-04\n",
      "epoch 389  loss 41.6943  lr 2.500e-04\n",
      "epoch 390  loss 43.4296  lr 2.500e-04\n",
      "epoch 391  loss 43.2384  lr 2.500e-04\n",
      "epoch 392  loss 42.3477  lr 2.500e-04\n",
      "epoch 393  loss 43.0957  lr 2.500e-04\n",
      "epoch 394  loss 43.7921  lr 2.500e-04\n",
      "epoch 395  loss 42.2489  lr 2.500e-04\n",
      "epoch 396  loss 41.5741  lr 2.500e-04\n",
      "epoch 397  loss 42.8107  lr 2.500e-04\n",
      "epoch 398  loss 42.9884  lr 2.500e-04\n",
      "epoch 399  loss 42.6503  lr 2.500e-04\n",
      "epoch 400  loss 41.6348  lr 2.500e-04\n",
      "epoch 401  loss 42.5760  lr 2.500e-04\n",
      "epoch 402  loss 42.6093  lr 2.500e-04\n",
      "epoch 403  loss 44.7305  lr 2.500e-04\n",
      "epoch 404  loss 42.3933  lr 2.500e-04\n",
      "epoch 405  loss 42.3574  lr 2.500e-04\n",
      "epoch 406  loss 41.6241  lr 2.500e-04\n",
      "epoch 407  loss 42.3320  lr 2.500e-04\n",
      "epoch 408  loss 41.6533  lr 2.500e-04\n",
      "epoch 409  loss 42.4564  lr 2.500e-04\n",
      "epoch 410  loss 43.2602  lr 2.500e-04\n",
      "epoch 411  loss 42.3186  lr 2.500e-04\n",
      "epoch 412  loss 43.5898  lr 2.500e-04\n",
      "epoch 413  loss 41.3578  lr 2.500e-04\n",
      "epoch 414  loss 42.2485  lr 2.500e-04\n",
      "epoch 415  loss 42.1124  lr 2.500e-04\n",
      "epoch 416  loss 41.3696  lr 2.500e-04\n",
      "epoch 417  loss 42.8912  lr 2.500e-04\n",
      "epoch 418  loss 42.7297  lr 2.500e-04\n",
      "epoch 419  loss 41.6936  lr 2.500e-04\n",
      "epoch 420  loss 42.6802  lr 2.500e-04\n",
      "epoch 421  loss 41.7925  lr 2.500e-04\n",
      "epoch 422  loss 40.8343  lr 2.500e-04\n",
      "epoch 423  loss 41.7907  lr 2.500e-04\n",
      "epoch 424  loss 41.5500  lr 2.500e-04\n",
      "epoch 425  loss 42.0943  lr 2.500e-04\n",
      "epoch 426  loss 42.0732  lr 2.500e-04\n",
      "epoch 427  loss 42.5943  lr 2.500e-04\n",
      "epoch 428  loss 41.7620  lr 2.500e-04\n",
      "epoch 429  loss 43.0111  lr 2.500e-04\n",
      "epoch 430  loss 42.5465  lr 2.500e-04\n",
      "epoch 431  loss 41.3580  lr 2.500e-04\n",
      "epoch 432  loss 41.2744  lr 2.500e-04\n",
      "epoch 433  loss 42.9685  lr 2.500e-04\n",
      "epoch 434  loss 41.2527  lr 2.500e-04\n",
      "epoch 435  loss 41.5110  lr 2.500e-04\n",
      "epoch 436  loss 43.4054  lr 2.500e-04\n",
      "epoch 437  loss 41.7761  lr 2.500e-04\n",
      "epoch 438  loss 41.0036  lr 2.500e-04\n",
      "epoch 439  loss 41.8857  lr 2.500e-04\n",
      "epoch 440  loss 41.8798  lr 2.500e-04\n",
      "epoch 441  loss 41.3373  lr 2.500e-04\n",
      "epoch 442  loss 41.8972  lr 2.500e-04\n",
      "epoch 443  loss 41.6791  lr 2.500e-04\n",
      "epoch 444  loss 42.1222  lr 2.500e-04\n",
      "epoch 445  loss 41.9374  lr 2.500e-04\n",
      "epoch 446  loss 42.4172  lr 2.500e-04\n",
      "epoch 447  loss 42.6424  lr 2.500e-04\n",
      "epoch 448  loss 42.9772  lr 2.500e-04\n",
      "epoch 449  loss 41.1321  lr 2.500e-04\n",
      "epoch 450  loss 42.9088  lr 2.500e-04\n",
      "epoch 451  loss 42.5268  lr 2.500e-04\n",
      "epoch 452  loss 41.9832  lr 2.500e-04\n",
      "epoch 453  loss 41.2355  lr 2.500e-04\n",
      "epoch 454  loss 41.6300  lr 2.500e-04\n",
      "epoch 455  loss 42.1175  lr 2.500e-04\n",
      "epoch 456  loss 42.8531  lr 2.500e-04\n",
      "epoch 457  loss 40.6554  lr 2.500e-04\n",
      "epoch 458  loss 40.7848  lr 2.500e-04\n",
      "epoch 459  loss 40.2062  lr 2.500e-04\n",
      "epoch 460  loss 41.1897  lr 2.500e-04\n",
      "epoch 461  loss 42.6308  lr 2.500e-04\n",
      "epoch 462  loss 42.4220  lr 2.500e-04\n",
      "epoch 463  loss 41.3655  lr 2.500e-04\n",
      "epoch 464  loss 41.1280  lr 2.500e-04\n",
      "epoch 465  loss 41.6215  lr 2.500e-04\n",
      "epoch 466  loss 41.2210  lr 2.500e-04\n",
      "epoch 467  loss 40.7318  lr 2.500e-04\n",
      "epoch 468  loss 41.8547  lr 2.500e-04\n",
      "epoch 469  loss 41.6476  lr 2.500e-04\n",
      "epoch 470  loss 42.0731  lr 2.500e-04\n",
      "epoch 471  loss 41.0684  lr 2.500e-04\n",
      "epoch 472  loss 42.2710  lr 2.500e-04\n",
      "epoch 473  loss 42.3358  lr 2.500e-04\n",
      "epoch 474  loss 41.7865  lr 2.500e-04\n",
      "epoch 475  loss 40.7345  lr 2.500e-04\n",
      "epoch 476  loss 41.2702  lr 2.500e-04\n",
      "epoch 477  loss 40.9333  lr 2.500e-04\n",
      "epoch 478  loss 40.9209  lr 2.500e-04\n",
      "epoch 479  loss 41.4086  lr 2.500e-04\n",
      "epoch 480  loss 42.1286  lr 2.500e-04\n",
      "epoch 481  loss 41.5669  lr 2.500e-04\n",
      "epoch 482  loss 40.8078  lr 2.500e-04\n",
      "epoch 483  loss 41.4233  lr 2.500e-04\n",
      "epoch 484  loss 41.5519  lr 2.500e-04\n",
      "epoch 485  loss 40.7445  lr 2.500e-04\n",
      "epoch 486  loss 40.7969  lr 2.500e-04\n",
      "epoch 487  loss 40.9230  lr 2.500e-04\n",
      "epoch 488  loss 40.0607  lr 2.500e-04\n",
      "epoch 489  loss 41.6626  lr 2.500e-04\n",
      "epoch 490  loss 41.3303  lr 2.500e-04\n",
      "epoch 491  loss 41.2504  lr 2.500e-04\n",
      "epoch 492  loss 42.1513  lr 2.500e-04\n",
      "epoch 493  loss 41.7946  lr 2.500e-04\n",
      "epoch 494  loss 41.9271  lr 2.500e-04\n",
      "epoch 495  loss 41.8626  lr 2.500e-04\n",
      "epoch 496  loss 40.1607  lr 2.500e-04\n",
      "epoch 497  loss 40.8771  lr 2.500e-04\n",
      "epoch 498  loss 41.1142  lr 2.500e-04\n",
      "epoch 499  loss 40.6123  lr 2.500e-04\n",
      "epoch 500  loss 41.0449  lr 2.500e-04\n",
      "epoch 501  loss 42.2211  lr 2.500e-04\n",
      "epoch 502  loss 41.7471  lr 2.500e-04\n",
      "epoch 503  loss 40.0648  lr 2.500e-04\n",
      "epoch 504  loss 40.5822  lr 2.500e-04\n",
      "epoch 505  loss 42.0916  lr 2.500e-04\n",
      "epoch 506  loss 40.6939  lr 2.500e-04\n",
      "epoch 507  loss 41.3229  lr 2.500e-04\n",
      "epoch 508  loss 42.3504  lr 2.500e-04\n",
      "epoch 509  loss 40.5314  lr 2.500e-04\n",
      "epoch 510  loss 41.9439  lr 2.500e-04\n",
      "epoch 511  loss 41.1411  lr 1.250e-04\n",
      "epoch 512  loss 38.5061  lr 1.250e-04\n",
      "epoch 513  loss 38.5108  lr 1.250e-04\n",
      "epoch 514  loss 40.0943  lr 1.250e-04\n",
      "epoch 515  loss 39.8247  lr 1.250e-04\n",
      "epoch 516  loss 39.1755  lr 1.250e-04\n",
      "epoch 517  loss 39.1132  lr 1.250e-04\n",
      "epoch 518  loss 38.3487  lr 1.250e-04\n",
      "epoch 519  loss 37.7662  lr 1.250e-04\n",
      "epoch 520  loss 38.3455  lr 1.250e-04\n",
      "epoch 521  loss 38.5947  lr 1.250e-04\n",
      "epoch 522  loss 39.2054  lr 1.250e-04\n",
      "epoch 523  loss 37.4385  lr 1.250e-04\n",
      "epoch 524  loss 38.9999  lr 1.250e-04\n",
      "epoch 525  loss 38.4687  lr 1.250e-04\n",
      "epoch 526  loss 39.0500  lr 1.250e-04\n",
      "epoch 527  loss 37.9924  lr 1.250e-04\n",
      "epoch 528  loss 39.3453  lr 1.250e-04\n",
      "epoch 529  loss 37.3311  lr 1.250e-04\n",
      "epoch 530  loss 39.1523  lr 1.250e-04\n",
      "epoch 531  loss 38.7546  lr 1.250e-04\n",
      "epoch 532  loss 38.7094  lr 1.250e-04\n",
      "epoch 533  loss 38.8435  lr 1.250e-04\n",
      "epoch 534  loss 38.4366  lr 1.250e-04\n",
      "epoch 535  loss 39.5697  lr 1.250e-04\n",
      "epoch 536  loss 37.3256  lr 1.250e-04\n",
      "epoch 537  loss 38.3509  lr 1.250e-04\n",
      "epoch 538  loss 38.3144  lr 1.250e-04\n",
      "epoch 539  loss 38.1652  lr 1.250e-04\n",
      "epoch 540  loss 37.7315  lr 1.250e-04\n",
      "epoch 541  loss 38.6065  lr 1.250e-04\n",
      "epoch 542  loss 38.5250  lr 1.250e-04\n",
      "epoch 543  loss 39.2118  lr 1.250e-04\n",
      "epoch 544  loss 38.1184  lr 1.250e-04\n",
      "epoch 545  loss 38.5003  lr 1.250e-04\n",
      "epoch 546  loss 37.6131  lr 1.250e-04\n",
      "epoch 547  loss 37.0581  lr 1.250e-04\n",
      "epoch 548  loss 40.0880  lr 1.250e-04\n",
      "epoch 549  loss 38.4131  lr 1.250e-04\n",
      "epoch 550  loss 38.6452  lr 1.250e-04\n",
      "epoch 551  loss 38.1677  lr 1.250e-04\n",
      "epoch 552  loss 38.5997  lr 1.250e-04\n",
      "epoch 553  loss 38.3376  lr 1.250e-04\n",
      "epoch 554  loss 38.5419  lr 1.250e-04\n",
      "epoch 555  loss 37.9802  lr 1.250e-04\n",
      "epoch 556  loss 37.6593  lr 1.250e-04\n",
      "epoch 557  loss 37.6595  lr 1.250e-04\n",
      "epoch 558  loss 38.0015  lr 1.250e-04\n",
      "epoch 559  loss 37.6143  lr 1.250e-04\n",
      "epoch 560  loss 37.5636  lr 1.250e-04\n",
      "epoch 561  loss 38.6041  lr 1.250e-04\n",
      "epoch 562  loss 38.2402  lr 1.250e-04\n",
      "epoch 563  loss 38.6837  lr 1.250e-04\n",
      "epoch 564  loss 38.4896  lr 1.250e-04\n",
      "epoch 565  loss 38.3462  lr 1.250e-04\n",
      "epoch 566  loss 38.5521  lr 1.250e-04\n",
      "epoch 567  loss 37.3627  lr 1.250e-04\n",
      "epoch 568  loss 39.0441  lr 1.250e-04\n",
      "epoch 569  loss 37.4716  lr 1.250e-04\n",
      "epoch 570  loss 38.6802  lr 1.250e-04\n",
      "epoch 571  loss 37.3992  lr 1.250e-04\n",
      "epoch 572  loss 37.1009  lr 1.250e-04\n",
      "epoch 573  loss 38.2078  lr 1.250e-04\n",
      "epoch 574  loss 38.7486  lr 1.250e-04\n",
      "epoch 575  loss 37.9194  lr 1.250e-04\n",
      "epoch 576  loss 37.9747  lr 1.250e-04\n",
      "epoch 577  loss 38.3338  lr 1.250e-04\n",
      "epoch 578  loss 39.3841  lr 1.250e-04\n",
      "epoch 579  loss 37.3994  lr 1.250e-04\n",
      "epoch 580  loss 37.9864  lr 1.250e-04\n",
      "epoch 581  loss 36.8971  lr 1.250e-04\n",
      "epoch 582  loss 37.2759  lr 1.250e-04\n",
      "epoch 583  loss 38.2346  lr 1.250e-04\n",
      "epoch 584  loss 37.4300  lr 1.250e-04\n",
      "epoch 585  loss 37.3432  lr 1.250e-04\n",
      "epoch 586  loss 37.0112  lr 1.250e-04\n",
      "epoch 587  loss 37.0740  lr 1.250e-04\n",
      "epoch 588  loss 37.5092  lr 1.250e-04\n",
      "epoch 589  loss 37.5900  lr 1.250e-04\n",
      "epoch 590  loss 37.2445  lr 1.250e-04\n",
      "epoch 591  loss 37.2737  lr 1.250e-04\n",
      "epoch 592  loss 36.9479  lr 1.250e-04\n",
      "epoch 593  loss 38.0227  lr 1.250e-04\n",
      "epoch 594  loss 36.2944  lr 1.250e-04\n",
      "epoch 595  loss 38.2118  lr 1.250e-04\n",
      "epoch 596  loss 37.8316  lr 1.250e-04\n",
      "epoch 597  loss 37.2606  lr 1.250e-04\n",
      "epoch 598  loss 37.9552  lr 1.250e-04\n",
      "epoch 599  loss 37.3438  lr 1.250e-04\n",
      "epoch 600  loss 38.4081  lr 1.250e-04\n",
      "epoch 601  loss 37.9535  lr 1.250e-04\n",
      "epoch 602  loss 38.6608  lr 1.250e-04\n",
      "epoch 603  loss 37.8193  lr 1.250e-04\n",
      "epoch 604  loss 38.0948  lr 1.250e-04\n",
      "epoch 605  loss 37.0817  lr 1.250e-04\n",
      "epoch 606  loss 38.1530  lr 1.250e-04\n",
      "epoch 607  loss 37.0267  lr 1.250e-04\n",
      "epoch 608  loss 37.1150  lr 1.250e-04\n",
      "epoch 609  loss 37.4779  lr 1.250e-04\n",
      "epoch 610  loss 37.2487  lr 1.250e-04\n",
      "epoch 611  loss 38.0494  lr 1.250e-04\n",
      "epoch 612  loss 38.5718  lr 6.250e-05\n",
      "epoch 613  loss 37.5056  lr 6.250e-05\n",
      "epoch 614  loss 36.2694  lr 6.250e-05\n",
      "epoch 615  loss 36.4626  lr 6.250e-05\n",
      "epoch 616  loss 36.1745  lr 6.250e-05\n",
      "epoch 617  loss 35.8222  lr 6.250e-05\n",
      "epoch 618  loss 36.4012  lr 6.250e-05\n",
      "epoch 619  loss 36.9605  lr 6.250e-05\n",
      "epoch 620  loss 37.3942  lr 6.250e-05\n",
      "epoch 621  loss 36.7824  lr 6.250e-05\n",
      "epoch 622  loss 35.5820  lr 6.250e-05\n",
      "epoch 623  loss 35.9622  lr 6.250e-05\n",
      "epoch 624  loss 37.1232  lr 6.250e-05\n",
      "epoch 625  loss 35.6895  lr 6.250e-05\n",
      "epoch 626  loss 36.4876  lr 6.250e-05\n",
      "epoch 627  loss 35.6106  lr 6.250e-05\n",
      "epoch 628  loss 37.1902  lr 6.250e-05\n",
      "epoch 629  loss 36.4176  lr 6.250e-05\n",
      "epoch 630  loss 35.3232  lr 6.250e-05\n",
      "epoch 631  loss 36.7603  lr 6.250e-05\n",
      "epoch 632  loss 35.7760  lr 6.250e-05\n",
      "epoch 633  loss 36.8882  lr 6.250e-05\n",
      "epoch 634  loss 36.7522  lr 6.250e-05\n",
      "epoch 635  loss 36.2847  lr 6.250e-05\n",
      "epoch 636  loss 36.8289  lr 6.250e-05\n",
      "epoch 637  loss 36.9624  lr 6.250e-05\n",
      "epoch 638  loss 35.6846  lr 6.250e-05\n",
      "epoch 639  loss 36.8534  lr 6.250e-05\n",
      "epoch 640  loss 36.4842  lr 6.250e-05\n",
      "epoch 641  loss 35.9981  lr 6.250e-05\n",
      "epoch 642  loss 37.6442  lr 6.250e-05\n",
      "epoch 643  loss 37.0549  lr 6.250e-05\n",
      "epoch 644  loss 35.5520  lr 6.250e-05\n",
      "epoch 645  loss 36.4204  lr 6.250e-05\n",
      "epoch 646  loss 36.0308  lr 6.250e-05\n",
      "epoch 647  loss 37.1903  lr 6.250e-05\n",
      "epoch 648  loss 35.6824  lr 6.250e-05\n",
      "epoch 649  loss 38.3129  lr 6.250e-05\n",
      "epoch 650  loss 35.8531  lr 6.250e-05\n",
      "epoch 651  loss 35.9467  lr 6.250e-05\n",
      "epoch 652  loss 36.8836  lr 6.250e-05\n",
      "epoch 653  loss 35.8999  lr 6.250e-05\n",
      "epoch 654  loss 36.6103  lr 6.250e-05\n",
      "epoch 655  loss 36.6314  lr 6.250e-05\n",
      "epoch 656  loss 34.9308  lr 6.250e-05\n",
      "epoch 657  loss 35.1860  lr 6.250e-05\n",
      "epoch 658  loss 35.6605  lr 6.250e-05\n",
      "epoch 659  loss 36.1377  lr 6.250e-05\n",
      "epoch 660  loss 35.1858  lr 6.250e-05\n",
      "epoch 661  loss 36.0917  lr 6.250e-05\n",
      "epoch 662  loss 37.0504  lr 6.250e-05\n",
      "epoch 663  loss 38.0246  lr 6.250e-05\n",
      "epoch 664  loss 37.1847  lr 6.250e-05\n",
      "epoch 665  loss 36.0049  lr 6.250e-05\n",
      "epoch 666  loss 37.4501  lr 6.250e-05\n",
      "epoch 667  loss 35.1181  lr 6.250e-05\n",
      "epoch 668  loss 36.7717  lr 6.250e-05\n",
      "epoch 669  loss 35.1277  lr 6.250e-05\n",
      "epoch 670  loss 36.1957  lr 6.250e-05\n",
      "epoch 671  loss 35.7324  lr 6.250e-05\n",
      "epoch 672  loss 37.2404  lr 6.250e-05\n",
      "epoch 673  loss 35.2312  lr 6.250e-05\n",
      "epoch 674  loss 36.2947  lr 6.250e-05\n",
      "epoch 675  loss 36.5763  lr 6.250e-05\n",
      "epoch 676  loss 34.8573  lr 6.250e-05\n",
      "epoch 677  loss 35.6771  lr 6.250e-05\n",
      "epoch 678  loss 36.7822  lr 6.250e-05\n",
      "epoch 679  loss 37.1892  lr 6.250e-05\n",
      "epoch 680  loss 36.7689  lr 6.250e-05\n",
      "epoch 681  loss 35.3097  lr 6.250e-05\n",
      "epoch 682  loss 36.9478  lr 6.250e-05\n",
      "epoch 683  loss 34.8622  lr 6.250e-05\n",
      "epoch 684  loss 38.4107  lr 6.250e-05\n",
      "epoch 685  loss 35.8685  lr 6.250e-05\n",
      "epoch 686  loss 35.4616  lr 6.250e-05\n",
      "epoch 687  loss 35.9659  lr 6.250e-05\n",
      "epoch 688  loss 36.4316  lr 6.250e-05\n",
      "epoch 689  loss 35.4203  lr 6.250e-05\n",
      "epoch 690  loss 36.7439  lr 6.250e-05\n",
      "epoch 691  loss 34.8389  lr 6.250e-05\n",
      "epoch 692  loss 36.3186  lr 6.250e-05\n",
      "epoch 693  loss 34.6715  lr 6.250e-05\n",
      "epoch 694  loss 35.6710  lr 6.250e-05\n",
      "epoch 695  loss 36.7700  lr 6.250e-05\n",
      "epoch 696  loss 35.6580  lr 6.250e-05\n",
      "epoch 697  loss 36.8653  lr 6.250e-05\n",
      "epoch 698  loss 36.4691  lr 6.250e-05\n",
      "epoch 699  loss 36.2489  lr 6.250e-05\n",
      "epoch 700  loss 36.0348  lr 6.250e-05\n",
      "epoch 701  loss 36.0786  lr 6.250e-05\n",
      "epoch 702  loss 34.8662  lr 6.250e-05\n",
      "epoch 703  loss 35.9468  lr 6.250e-05\n",
      "epoch 704  loss 36.5154  lr 6.250e-05\n",
      "epoch 705  loss 35.9943  lr 6.250e-05\n",
      "epoch 706  loss 37.0402  lr 6.250e-05\n",
      "epoch 707  loss 35.8908  lr 6.250e-05\n",
      "epoch 708  loss 36.3607  lr 6.250e-05\n",
      "epoch 709  loss 35.7674  lr 6.250e-05\n",
      "epoch 710  loss 36.0684  lr 6.250e-05\n",
      "epoch 711  loss 36.3512  lr 6.250e-05\n",
      "epoch 712  loss 35.9602  lr 6.250e-05\n",
      "epoch 713  loss 36.9020  lr 6.250e-05\n",
      "epoch 714  loss 35.4905  lr 6.250e-05\n",
      "epoch 715  loss 36.7582  lr 6.250e-05\n",
      "epoch 716  loss 35.6175  lr 6.250e-05\n",
      "epoch 717  loss 35.0649  lr 6.250e-05\n",
      "epoch 718  loss 35.2827  lr 6.250e-05\n",
      "epoch 719  loss 36.6256  lr 6.250e-05\n",
      "epoch 720  loss 36.8652  lr 6.250e-05\n",
      "epoch 721  loss 35.0204  lr 6.250e-05\n",
      "epoch 722  loss 36.2214  lr 6.250e-05\n",
      "epoch 723  loss 33.9653  lr 6.250e-05\n",
      "epoch 724  loss 35.9556  lr 6.250e-05\n",
      "epoch 725  loss 36.0805  lr 6.250e-05\n",
      "epoch 726  loss 37.2052  lr 6.250e-05\n",
      "epoch 727  loss 35.1788  lr 6.250e-05\n",
      "epoch 728  loss 34.4715  lr 6.250e-05\n",
      "epoch 729  loss 35.1633  lr 6.250e-05\n",
      "epoch 730  loss 35.9488  lr 6.250e-05\n",
      "epoch 731  loss 35.6412  lr 6.250e-05\n",
      "epoch 732  loss 34.0848  lr 6.250e-05\n",
      "epoch 733  loss 35.1396  lr 6.250e-05\n",
      "epoch 734  loss 36.2636  lr 6.250e-05\n",
      "epoch 735  loss 34.1406  lr 6.250e-05\n",
      "epoch 736  loss 34.6747  lr 6.250e-05\n",
      "epoch 737  loss 35.6085  lr 6.250e-05\n",
      "epoch 738  loss 36.4699  lr 6.250e-05\n",
      "epoch 739  loss 35.1956  lr 6.250e-05\n",
      "epoch 740  loss 34.3966  lr 6.250e-05\n",
      "epoch 741  loss 36.8417  lr 6.250e-05\n",
      "epoch 742  loss 35.0299  lr 6.250e-05\n",
      "epoch 743  loss 35.0001  lr 6.250e-05\n",
      "epoch 744  loss 36.4366  lr 6.250e-05\n",
      "epoch 745  loss 36.2602  lr 6.250e-05\n",
      "epoch 746  loss 35.5045  lr 6.250e-05\n",
      "epoch 747  loss 35.4463  lr 6.250e-05\n",
      "epoch 748  loss 34.4318  lr 6.250e-05\n",
      "epoch 749  loss 36.4989  lr 6.250e-05\n",
      "epoch 750  loss 35.3343  lr 6.250e-05\n",
      "epoch 751  loss 35.8992  lr 6.250e-05\n",
      "epoch 752  loss 35.2976  lr 6.250e-05\n",
      "epoch 753  loss 35.0592  lr 6.250e-05\n",
      "epoch 754  loss 35.8989  lr 6.250e-05\n",
      "epoch 755  loss 35.4951  lr 6.250e-05\n",
      "epoch 756  loss 36.8120  lr 6.250e-05\n",
      "epoch 757  loss 36.8977  lr 6.250e-05\n",
      "epoch 758  loss 34.9325  lr 6.250e-05\n",
      "epoch 759  loss 37.5622  lr 6.250e-05\n",
      "epoch 760  loss 36.3427  lr 6.250e-05\n",
      "epoch 761  loss 36.1508  lr 6.250e-05\n",
      "epoch 762  loss 34.6753  lr 6.250e-05\n",
      "epoch 763  loss 35.3544  lr 6.250e-05\n",
      "epoch 764  loss 36.4181  lr 6.250e-05\n",
      "epoch 765  loss 34.7281  lr 6.250e-05\n",
      "epoch 766  loss 35.8384  lr 6.250e-05\n",
      "epoch 767  loss 34.6601  lr 6.250e-05\n",
      "epoch 768  loss 35.2062  lr 6.250e-05\n",
      "epoch 769  loss 35.0241  lr 6.250e-05\n",
      "epoch 770  loss 35.3277  lr 6.250e-05\n",
      "epoch 771  loss 36.5374  lr 6.250e-05\n",
      "epoch 772  loss 37.1830  lr 6.250e-05\n",
      "epoch 773  loss 35.2269  lr 6.250e-05\n",
      "epoch 774  loss 35.9321  lr 6.250e-05\n",
      "epoch 775  loss 36.1676  lr 6.250e-05\n",
      "epoch 776  loss 35.2780  lr 6.250e-05\n",
      "epoch 777  loss 34.8412  lr 6.250e-05\n",
      "epoch 778  loss 36.6494  lr 6.250e-05\n",
      "epoch 779  loss 34.7204  lr 6.250e-05\n",
      "epoch 780  loss 34.5604  lr 6.250e-05\n",
      "epoch 781  loss 35.0560  lr 6.250e-05\n",
      "epoch 782  loss 36.0187  lr 6.250e-05\n",
      "epoch 783  loss 36.6865  lr 6.250e-05\n",
      "epoch 784  loss 34.8788  lr 6.250e-05\n",
      "epoch 785  loss 35.2400  lr 6.250e-05\n",
      "epoch 786  loss 35.6434  lr 6.250e-05\n",
      "epoch 787  loss 36.3449  lr 6.250e-05\n",
      "epoch 788  loss 35.3292  lr 6.250e-05\n",
      "epoch 789  loss 35.7527  lr 6.250e-05\n",
      "epoch 790  loss 35.2171  lr 6.250e-05\n",
      "epoch 791  loss 35.2762  lr 6.250e-05\n",
      "epoch 792  loss 36.0293  lr 6.250e-05\n",
      "epoch 793  loss 35.8141  lr 6.250e-05\n",
      "epoch 794  loss 36.4211  lr 6.250e-05\n",
      "epoch 795  loss 35.8572  lr 6.250e-05\n",
      "epoch 796  loss 36.1319  lr 6.250e-05\n",
      "epoch 797  loss 34.5896  lr 6.250e-05\n",
      "epoch 798  loss 34.6112  lr 6.250e-05\n",
      "epoch 799  loss 35.6992  lr 6.250e-05\n",
      "epoch 800  loss 35.3733  lr 6.250e-05\n",
      "epoch 801  loss 36.2599  lr 6.250e-05\n",
      "epoch 802  loss 35.7557  lr 6.250e-05\n",
      "epoch 803  loss 34.5607  lr 6.250e-05\n",
      "epoch 804  loss 35.4588  lr 6.250e-05\n",
      "epoch 805  loss 35.1481  lr 6.250e-05\n",
      "epoch 806  loss 35.6761  lr 6.250e-05\n",
      "epoch 807  loss 35.8889  lr 6.250e-05\n",
      "epoch 808  loss 35.9141  lr 6.250e-05\n",
      "epoch 809  loss 35.3043  lr 6.250e-05\n",
      "epoch 810  loss 35.7467  lr 6.250e-05\n",
      "epoch 811  loss 35.7770  lr 6.250e-05\n",
      "epoch 812  loss 34.8381  lr 6.250e-05\n",
      "epoch 813  loss 35.4736  lr 6.250e-05\n",
      "epoch 814  loss 34.6928  lr 6.250e-05\n",
      "epoch 815  loss 35.9455  lr 6.250e-05\n",
      "epoch 816  loss 35.8606  lr 6.250e-05\n",
      "epoch 817  loss 35.2358  lr 6.250e-05\n",
      "epoch 818  loss 34.6812  lr 6.250e-05\n",
      "epoch 819  loss 35.4304  lr 6.250e-05\n",
      "epoch 820  loss 35.9231  lr 6.250e-05\n",
      "epoch 821  loss 35.3699  lr 6.250e-05\n",
      "epoch 822  loss 35.8308  lr 6.250e-05\n",
      "epoch 823  loss 35.7156  lr 6.250e-05\n",
      "epoch 824  loss 34.3856  lr 6.250e-05\n",
      "epoch 825  loss 35.3132  lr 3.125e-05\n",
      "epoch 826  loss 34.5482  lr 3.125e-05\n",
      "epoch 827  loss 34.7915  lr 3.125e-05\n",
      "epoch 828  loss 35.3419  lr 3.125e-05\n",
      "epoch 829  loss 35.9063  lr 3.125e-05\n",
      "epoch 830  loss 35.4226  lr 3.125e-05\n",
      "epoch 831  loss 35.7913  lr 3.125e-05\n",
      "epoch 832  loss 34.6381  lr 3.125e-05\n",
      "epoch 833  loss 35.0668  lr 3.125e-05\n",
      "epoch 834  loss 35.0686  lr 3.125e-05\n",
      "epoch 835  loss 35.0258  lr 3.125e-05\n",
      "epoch 836  loss 33.7671  lr 3.125e-05\n",
      "epoch 837  loss 34.3982  lr 3.125e-05\n",
      "epoch 838  loss 33.9665  lr 3.125e-05\n",
      "epoch 839  loss 35.2013  lr 3.125e-05\n",
      "epoch 840  loss 35.2271  lr 3.125e-05\n",
      "epoch 841  loss 34.1030  lr 3.125e-05\n",
      "epoch 842  loss 34.6320  lr 3.125e-05\n",
      "epoch 843  loss 35.7536  lr 3.125e-05\n",
      "epoch 844  loss 33.4432  lr 3.125e-05\n",
      "epoch 845  loss 34.9058  lr 3.125e-05\n",
      "epoch 846  loss 34.1067  lr 3.125e-05\n",
      "epoch 847  loss 33.4404  lr 3.125e-05\n",
      "epoch 848  loss 34.6336  lr 3.125e-05\n",
      "epoch 849  loss 33.5035  lr 3.125e-05\n",
      "epoch 850  loss 33.8544  lr 3.125e-05\n",
      "epoch 851  loss 35.0770  lr 3.125e-05\n",
      "epoch 852  loss 35.2175  lr 3.125e-05\n",
      "epoch 853  loss 35.1430  lr 3.125e-05\n",
      "epoch 854  loss 35.4336  lr 3.125e-05\n",
      "epoch 855  loss 35.3258  lr 3.125e-05\n",
      "epoch 856  loss 35.5853  lr 3.125e-05\n",
      "epoch 857  loss 33.8503  lr 3.125e-05\n",
      "epoch 858  loss 34.9003  lr 3.125e-05\n",
      "epoch 859  loss 34.5220  lr 3.125e-05\n",
      "epoch 860  loss 34.4362  lr 3.125e-05\n",
      "epoch 861  loss 35.1294  lr 3.125e-05\n",
      "epoch 862  loss 35.3596  lr 3.125e-05\n",
      "epoch 863  loss 33.5486  lr 3.125e-05\n",
      "epoch 864  loss 34.9719  lr 3.125e-05\n",
      "epoch 865  loss 34.7616  lr 3.125e-05\n",
      "epoch 866  loss 34.7060  lr 3.125e-05\n",
      "epoch 867  loss 34.2175  lr 3.125e-05\n",
      "epoch 868  loss 33.9481  lr 3.125e-05\n",
      "epoch 869  loss 35.2333  lr 3.125e-05\n",
      "epoch 870  loss 33.0418  lr 3.125e-05\n",
      "epoch 871  loss 34.6500  lr 3.125e-05\n",
      "epoch 872  loss 35.0392  lr 3.125e-05\n",
      "epoch 873  loss 34.1967  lr 3.125e-05\n",
      "epoch 874  loss 33.7769  lr 3.125e-05\n",
      "epoch 875  loss 33.3521  lr 3.125e-05\n",
      "epoch 876  loss 35.0071  lr 3.125e-05\n",
      "epoch 877  loss 34.9367  lr 3.125e-05\n",
      "epoch 878  loss 33.7049  lr 3.125e-05\n",
      "epoch 879  loss 34.1917  lr 3.125e-05\n",
      "epoch 880  loss 34.7127  lr 3.125e-05\n",
      "epoch 881  loss 34.1844  lr 3.125e-05\n",
      "epoch 882  loss 34.5269  lr 3.125e-05\n",
      "epoch 883  loss 34.2540  lr 3.125e-05\n",
      "epoch 884  loss 35.3350  lr 3.125e-05\n",
      "epoch 885  loss 36.0247  lr 3.125e-05\n",
      "epoch 886  loss 35.1937  lr 3.125e-05\n",
      "epoch 887  loss 34.3851  lr 3.125e-05\n",
      "epoch 888  loss 34.7444  lr 3.125e-05\n",
      "epoch 889  loss 34.7357  lr 3.125e-05\n",
      "epoch 890  loss 35.5471  lr 3.125e-05\n",
      "epoch 891  loss 35.4038  lr 3.125e-05\n",
      "epoch 892  loss 34.2722  lr 3.125e-05\n",
      "epoch 893  loss 34.3219  lr 3.125e-05\n",
      "epoch 894  loss 35.2208  lr 3.125e-05\n",
      "epoch 895  loss 35.0692  lr 3.125e-05\n",
      "epoch 896  loss 34.8059  lr 3.125e-05\n",
      "epoch 897  loss 33.4690  lr 3.125e-05\n",
      "epoch 898  loss 35.1797  lr 3.125e-05\n",
      "epoch 899  loss 35.3502  lr 3.125e-05\n",
      "epoch 900  loss 33.8365  lr 3.125e-05\n",
      "epoch 901  loss 35.3760  lr 3.125e-05\n",
      "epoch 902  loss 33.8320  lr 3.125e-05\n",
      "epoch 903  loss 35.1636  lr 3.125e-05\n",
      "epoch 904  loss 33.3663  lr 3.125e-05\n",
      "epoch 905  loss 35.4834  lr 3.125e-05\n",
      "epoch 906  loss 34.5937  lr 3.125e-05\n",
      "epoch 907  loss 33.8032  lr 3.125e-05\n",
      "epoch 908  loss 35.3120  lr 3.125e-05\n",
      "epoch 909  loss 34.8066  lr 3.125e-05\n",
      "epoch 910  loss 36.0225  lr 3.125e-05\n",
      "epoch 911  loss 34.0871  lr 3.125e-05\n",
      "epoch 912  loss 35.0156  lr 3.125e-05\n",
      "epoch 913  loss 33.2130  lr 3.125e-05\n",
      "epoch 914  loss 35.4382  lr 3.125e-05\n",
      "epoch 915  loss 34.1126  lr 3.125e-05\n",
      "epoch 916  loss 33.8261  lr 3.125e-05\n",
      "epoch 917  loss 33.2876  lr 3.125e-05\n",
      "epoch 918  loss 33.4761  lr 3.125e-05\n",
      "epoch 919  loss 33.7130  lr 3.125e-05\n",
      "epoch 920  loss 36.3272  lr 3.125e-05\n",
      "epoch 921  loss 35.5204  lr 3.125e-05\n",
      "epoch 922  loss 35.0563  lr 3.125e-05\n",
      "epoch 923  loss 35.3531  lr 3.125e-05\n",
      "epoch 924  loss 33.1848  lr 3.125e-05\n",
      "epoch 925  loss 34.0683  lr 3.125e-05\n",
      "epoch 926  loss 35.4091  lr 1.563e-05\n",
      "epoch 927  loss 34.4892  lr 1.563e-05\n",
      "epoch 928  loss 32.5655  lr 1.563e-05\n",
      "epoch 929  loss 35.5937  lr 1.563e-05\n",
      "epoch 930  loss 33.7968  lr 1.563e-05\n",
      "epoch 931  loss 34.0393  lr 1.563e-05\n",
      "epoch 932  loss 34.3145  lr 1.563e-05\n",
      "epoch 933  loss 34.2815  lr 1.563e-05\n",
      "epoch 934  loss 33.6838  lr 1.563e-05\n",
      "epoch 935  loss 34.6257  lr 1.563e-05\n",
      "epoch 936  loss 34.3704  lr 1.563e-05\n",
      "epoch 937  loss 34.3967  lr 1.563e-05\n",
      "epoch 938  loss 33.5603  lr 1.563e-05\n",
      "epoch 939  loss 33.9007  lr 1.563e-05\n",
      "epoch 940  loss 34.2408  lr 1.563e-05\n",
      "epoch 941  loss 33.3482  lr 1.563e-05\n",
      "epoch 942  loss 34.2005  lr 1.563e-05\n",
      "epoch 943  loss 33.8795  lr 1.563e-05\n",
      "epoch 944  loss 33.4453  lr 1.563e-05\n",
      "epoch 945  loss 33.6998  lr 1.563e-05\n",
      "epoch 946  loss 34.8880  lr 1.563e-05\n",
      "epoch 947  loss 34.4890  lr 1.563e-05\n",
      "epoch 948  loss 34.1425  lr 1.563e-05\n",
      "epoch 949  loss 33.6891  lr 1.563e-05\n",
      "epoch 950  loss 33.9672  lr 1.563e-05\n",
      "epoch 951  loss 34.4510  lr 1.563e-05\n",
      "epoch 952  loss 34.0553  lr 1.563e-05\n",
      "epoch 953  loss 34.9020  lr 1.563e-05\n",
      "epoch 954  loss 34.5229  lr 1.563e-05\n",
      "epoch 955  loss 34.0996  lr 1.563e-05\n",
      "epoch 956  loss 34.3066  lr 1.563e-05\n",
      "epoch 957  loss 34.4217  lr 1.563e-05\n",
      "epoch 958  loss 34.3074  lr 1.563e-05\n",
      "epoch 959  loss 34.0731  lr 1.563e-05\n",
      "epoch 960  loss 33.7112  lr 1.563e-05\n",
      "epoch 961  loss 33.9597  lr 1.563e-05\n",
      "epoch 962  loss 34.6984  lr 1.563e-05\n",
      "epoch 963  loss 34.4848  lr 1.563e-05\n",
      "epoch 964  loss 34.6666  lr 1.563e-05\n",
      "epoch 965  loss 34.5764  lr 1.563e-05\n",
      "epoch 966  loss 33.4325  lr 1.563e-05\n",
      "epoch 967  loss 34.0131  lr 1.563e-05\n",
      "epoch 968  loss 33.8144  lr 1.563e-05\n",
      "epoch 969  loss 34.2878  lr 1.563e-05\n",
      "epoch 970  loss 34.6532  lr 1.563e-05\n",
      "epoch 971  loss 34.5526  lr 1.563e-05\n",
      "epoch 972  loss 34.0264  lr 1.563e-05\n",
      "epoch 973  loss 33.4352  lr 1.563e-05\n",
      "epoch 974  loss 34.8239  lr 1.563e-05\n",
      "epoch 975  loss 34.1035  lr 1.563e-05\n",
      "epoch 976  loss 33.2076  lr 1.563e-05\n",
      "epoch 977  loss 34.4364  lr 1.563e-05\n",
      "epoch 978  loss 34.3694  lr 1.563e-05\n",
      "epoch 979  loss 33.9561  lr 1.563e-05\n",
      "epoch 980  loss 34.0748  lr 1.563e-05\n",
      "epoch 981  loss 34.6608  lr 1.563e-05\n",
      "epoch 982  loss 34.6645  lr 1.563e-05\n",
      "epoch 983  loss 33.0003  lr 1.563e-05\n",
      "epoch 984  loss 34.5309  lr 1.563e-05\n",
      "epoch 985  loss 33.3714  lr 1.563e-05\n",
      "epoch 986  loss 34.6801  lr 1.563e-05\n",
      "epoch 987  loss 34.2632  lr 1.563e-05\n",
      "epoch 988  loss 33.5526  lr 1.563e-05\n",
      "epoch 989  loss 34.0426  lr 1.563e-05\n",
      "epoch 990  loss 34.4871  lr 1.563e-05\n",
      "epoch 991  loss 34.8741  lr 1.563e-05\n",
      "epoch 992  loss 33.5082  lr 1.563e-05\n",
      "epoch 993  loss 33.3697  lr 1.563e-05\n",
      "epoch 994  loss 33.5842  lr 1.563e-05\n",
      "epoch 995  loss 34.3495  lr 1.563e-05\n",
      "epoch 996  loss 34.5876  lr 1.563e-05\n",
      "epoch 997  loss 34.1478  lr 1.563e-05\n",
      "epoch 998  loss 33.0228  lr 1.563e-05\n",
      "epoch 999  loss 35.0640  lr 1.563e-05\n",
      "epoch 1000  loss 33.7597  lr 1.563e-05\n",
      "epoch 1001  loss 33.0754  lr 1.563e-05\n",
      "epoch 1002  loss 34.0012  lr 1.563e-05\n",
      "epoch 1003  loss 34.7626  lr 1.563e-05\n",
      "epoch 1004  loss 33.1616  lr 1.563e-05\n",
      "epoch 1005  loss 34.0964  lr 1.563e-05\n",
      "epoch 1006  loss 33.1941  lr 1.563e-05\n",
      "epoch 1007  loss 33.1740  lr 1.563e-05\n",
      "epoch 1008  loss 34.3139  lr 1.563e-05\n",
      "epoch 1009  loss 33.9270  lr 1.563e-05\n",
      "epoch 1010  loss 34.1673  lr 1.563e-05\n",
      "epoch 1011  loss 33.3184  lr 1.563e-05\n",
      "epoch 1012  loss 34.7664  lr 1.563e-05\n",
      "epoch 1013  loss 34.8915  lr 1.563e-05\n",
      "epoch 1014  loss 34.1726  lr 1.563e-05\n",
      "epoch 1015  loss 34.1331  lr 1.563e-05\n",
      "epoch 1016  loss 34.2297  lr 1.563e-05\n",
      "epoch 1017  loss 32.5301  lr 1.563e-05\n",
      "epoch 1018  loss 33.7683  lr 1.563e-05\n",
      "epoch 1019  loss 35.2297  lr 1.563e-05\n",
      "epoch 1020  loss 35.1801  lr 1.563e-05\n",
      "epoch 1021  loss 35.3220  lr 1.563e-05\n",
      "epoch 1022  loss 35.0906  lr 1.563e-05\n",
      "epoch 1023  loss 33.9310  lr 1.563e-05\n",
      "epoch 1024  loss 33.6532  lr 1.563e-05\n",
      "epoch 1025  loss 33.7189  lr 1.563e-05\n",
      "epoch 1026  loss 34.5901  lr 1.563e-05\n",
      "epoch 1027  loss 33.9124  lr 7.813e-06\n",
      "epoch 1028  loss 33.2366  lr 7.813e-06\n",
      "epoch 1029  loss 33.2428  lr 7.813e-06\n",
      "epoch 1030  loss 33.2099  lr 7.813e-06\n",
      "epoch 1031  loss 34.2256  lr 7.813e-06\n",
      "epoch 1032  loss 33.7030  lr 7.813e-06\n",
      "epoch 1033  loss 33.1452  lr 7.813e-06\n",
      "epoch 1034  loss 34.0760  lr 7.813e-06\n",
      "epoch 1035  loss 33.5925  lr 7.813e-06\n",
      "epoch 1036  loss 34.4759  lr 7.813e-06\n",
      "epoch 1037  loss 33.6669  lr 7.813e-06\n",
      "epoch 1038  loss 34.5777  lr 7.813e-06\n",
      "epoch 1039  loss 33.6090  lr 7.813e-06\n",
      "epoch 1040  loss 33.5292  lr 7.813e-06\n",
      "epoch 1041  loss 32.8377  lr 7.813e-06\n",
      "epoch 1042  loss 33.9498  lr 7.813e-06\n",
      "epoch 1043  loss 33.9850  lr 7.813e-06\n",
      "epoch 1044  loss 32.2246  lr 7.813e-06\n",
      "epoch 1045  loss 34.1281  lr 7.813e-06\n",
      "epoch 1046  loss 33.4953  lr 7.813e-06\n",
      "epoch 1047  loss 33.4709  lr 7.813e-06\n",
      "epoch 1048  loss 33.1236  lr 7.813e-06\n",
      "epoch 1049  loss 33.3419  lr 7.813e-06\n",
      "epoch 1050  loss 34.8142  lr 7.813e-06\n",
      "epoch 1051  loss 34.4418  lr 7.813e-06\n",
      "epoch 1052  loss 33.5420  lr 7.813e-06\n",
      "epoch 1053  loss 34.1314  lr 7.813e-06\n",
      "epoch 1054  loss 33.2893  lr 7.813e-06\n",
      "epoch 1055  loss 33.1740  lr 7.813e-06\n",
      "epoch 1056  loss 32.9812  lr 7.813e-06\n",
      "epoch 1057  loss 34.0702  lr 7.813e-06\n",
      "epoch 1058  loss 34.3090  lr 7.813e-06\n",
      "epoch 1059  loss 34.2228  lr 7.813e-06\n",
      "epoch 1060  loss 34.2321  lr 7.813e-06\n",
      "epoch 1061  loss 34.9038  lr 7.813e-06\n",
      "epoch 1062  loss 34.7321  lr 7.813e-06\n",
      "epoch 1063  loss 34.0494  lr 7.813e-06\n",
      "epoch 1064  loss 34.1292  lr 7.813e-06\n",
      "epoch 1065  loss 33.1933  lr 7.813e-06\n",
      "epoch 1066  loss 34.5493  lr 7.813e-06\n",
      "epoch 1067  loss 33.3726  lr 7.813e-06\n",
      "epoch 1068  loss 33.6613  lr 7.813e-06\n",
      "epoch 1069  loss 33.3650  lr 7.813e-06\n",
      "epoch 1070  loss 32.8619  lr 7.813e-06\n",
      "epoch 1071  loss 34.3964  lr 7.813e-06\n",
      "epoch 1072  loss 34.6939  lr 7.813e-06\n",
      "epoch 1073  loss 33.4397  lr 7.813e-06\n",
      "epoch 1074  loss 33.3677  lr 7.813e-06\n",
      "epoch 1075  loss 34.1953  lr 7.813e-06\n",
      "epoch 1076  loss 33.0127  lr 7.813e-06\n",
      "epoch 1077  loss 33.3975  lr 7.813e-06\n",
      "epoch 1078  loss 33.9246  lr 7.813e-06\n",
      "epoch 1079  loss 33.9682  lr 7.813e-06\n",
      "epoch 1080  loss 34.3797  lr 7.813e-06\n",
      "epoch 1081  loss 35.2103  lr 7.813e-06\n",
      "epoch 1082  loss 33.7768  lr 7.813e-06\n",
      "epoch 1083  loss 34.4459  lr 7.813e-06\n",
      "epoch 1084  loss 35.4009  lr 7.813e-06\n",
      "epoch 1085  loss 33.8418  lr 7.813e-06\n",
      "epoch 1086  loss 34.0095  lr 7.813e-06\n",
      "epoch 1087  loss 35.2259  lr 7.813e-06\n",
      "epoch 1088  loss 33.9954  lr 7.813e-06\n",
      "epoch 1089  loss 33.7665  lr 7.813e-06\n",
      "epoch 1090  loss 32.6688  lr 7.813e-06\n",
      "epoch 1091  loss 33.5524  lr 7.813e-06\n",
      "epoch 1092  loss 34.6324  lr 7.813e-06\n",
      "epoch 1093  loss 33.9414  lr 7.813e-06\n",
      "epoch 1094  loss 33.8233  lr 7.813e-06\n",
      "epoch 1095  loss 33.5772  lr 7.813e-06\n",
      "epoch 1096  loss 34.2762  lr 7.813e-06\n",
      "epoch 1097  loss 33.3112  lr 7.813e-06\n",
      "epoch 1098  loss 32.5822  lr 7.813e-06\n",
      "epoch 1099  loss 34.5706  lr 7.813e-06\n",
      "epoch 1100  loss 34.1910  lr 7.813e-06\n",
      "epoch 1101  loss 33.7882  lr 7.813e-06\n",
      "epoch 1102  loss 33.4327  lr 7.813e-06\n",
      "epoch 1103  loss 33.7520  lr 7.813e-06\n",
      "epoch 1104  loss 34.9332  lr 7.813e-06\n",
      "epoch 1105  loss 32.5464  lr 7.813e-06\n",
      "epoch 1106  loss 33.7770  lr 7.813e-06\n",
      "epoch 1107  loss 33.4178  lr 7.813e-06\n",
      "epoch 1108  loss 33.3304  lr 7.813e-06\n",
      "epoch 1109  loss 33.7909  lr 7.813e-06\n",
      "epoch 1110  loss 35.0126  lr 7.813e-06\n",
      "epoch 1111  loss 33.8823  lr 7.813e-06\n",
      "epoch 1112  loss 32.7169  lr 7.813e-06\n",
      "epoch 1113  loss 33.9517  lr 7.813e-06\n",
      "epoch 1114  loss 33.4508  lr 7.813e-06\n",
      "epoch 1115  loss 33.9332  lr 7.813e-06\n",
      "epoch 1116  loss 33.1314  lr 7.813e-06\n",
      "epoch 1117  loss 34.2024  lr 7.813e-06\n",
      "epoch 1118  loss 33.6569  lr 7.813e-06\n",
      "epoch 1119  loss 33.0481  lr 7.813e-06\n",
      "epoch 1120  loss 34.0965  lr 7.813e-06\n",
      "epoch 1121  loss 32.9483  lr 7.813e-06\n",
      "epoch 1122  loss 33.2248  lr 7.813e-06\n",
      "epoch 1123  loss 34.7697  lr 7.813e-06\n",
      "epoch 1124  loss 33.8113  lr 7.813e-06\n",
      "epoch 1125  loss 34.2008  lr 7.813e-06\n",
      "epoch 1126  loss 33.5542  lr 7.813e-06\n",
      "epoch 1127  loss 33.8938  lr 7.813e-06\n",
      "epoch 1128  loss 33.8428  lr 3.906e-06\n",
      "epoch 1129  loss 34.3968  lr 3.906e-06\n",
      "epoch 1130  loss 33.4089  lr 3.906e-06\n",
      "epoch 1131  loss 33.8381  lr 3.906e-06\n",
      "epoch 1132  loss 33.8466  lr 3.906e-06\n",
      "epoch 1133  loss 32.2005  lr 3.906e-06\n",
      "epoch 1134  loss 33.4125  lr 3.906e-06\n",
      "epoch 1135  loss 34.1893  lr 3.906e-06\n",
      "epoch 1136  loss 33.7587  lr 3.906e-06\n",
      "epoch 1137  loss 34.5066  lr 3.906e-06\n",
      "epoch 1138  loss 33.8792  lr 3.906e-06\n",
      "epoch 1139  loss 34.2020  lr 3.906e-06\n",
      "epoch 1140  loss 33.1625  lr 3.906e-06\n",
      "epoch 1141  loss 33.5964  lr 3.906e-06\n",
      "epoch 1142  loss 34.0561  lr 3.906e-06\n",
      "epoch 1143  loss 33.1825  lr 3.906e-06\n",
      "epoch 1144  loss 33.7426  lr 3.906e-06\n",
      "epoch 1145  loss 32.8928  lr 3.906e-06\n",
      "epoch 1146  loss 33.2059  lr 3.906e-06\n",
      "epoch 1147  loss 33.3942  lr 3.906e-06\n",
      "epoch 1148  loss 34.1073  lr 3.906e-06\n",
      "epoch 1149  loss 33.9582  lr 3.906e-06\n",
      "epoch 1150  loss 34.0516  lr 3.906e-06\n",
      "epoch 1151  loss 33.8898  lr 3.906e-06\n",
      "epoch 1152  loss 32.6463  lr 3.906e-06\n",
      "epoch 1153  loss 34.0768  lr 3.906e-06\n",
      "epoch 1154  loss 33.3468  lr 3.906e-06\n",
      "epoch 1155  loss 33.7789  lr 3.906e-06\n",
      "epoch 1156  loss 32.4857  lr 3.906e-06\n",
      "epoch 1157  loss 34.2481  lr 3.906e-06\n",
      "epoch 1158  loss 34.2221  lr 3.906e-06\n",
      "epoch 1159  loss 32.4371  lr 3.906e-06\n",
      "epoch 1160  loss 33.9387  lr 3.906e-06\n",
      "epoch 1161  loss 33.8310  lr 3.906e-06\n",
      "epoch 1162  loss 32.4620  lr 3.906e-06\n",
      "epoch 1163  loss 34.8349  lr 3.906e-06\n",
      "epoch 1164  loss 34.5007  lr 3.906e-06\n",
      "epoch 1165  loss 33.0135  lr 3.906e-06\n",
      "epoch 1166  loss 34.1991  lr 3.906e-06\n",
      "epoch 1167  loss 33.3170  lr 3.906e-06\n",
      "epoch 1168  loss 33.6942  lr 3.906e-06\n",
      "epoch 1169  loss 33.4621  lr 3.906e-06\n",
      "epoch 1170  loss 32.9464  lr 3.906e-06\n",
      "epoch 1171  loss 33.6270  lr 3.906e-06\n",
      "epoch 1172  loss 34.1441  lr 3.906e-06\n",
      "epoch 1173  loss 34.2827  lr 3.906e-06\n",
      "epoch 1174  loss 33.8681  lr 3.906e-06\n",
      "epoch 1175  loss 33.3659  lr 3.906e-06\n",
      "epoch 1176  loss 34.5767  lr 3.906e-06\n",
      "epoch 1177  loss 34.1981  lr 3.906e-06\n",
      "epoch 1178  loss 34.1826  lr 3.906e-06\n",
      "epoch 1179  loss 34.0148  lr 3.906e-06\n",
      "epoch 1180  loss 33.9929  lr 3.906e-06\n",
      "epoch 1181  loss 33.6837  lr 3.906e-06\n",
      "epoch 1182  loss 33.0550  lr 3.906e-06\n",
      "epoch 1183  loss 34.3319  lr 3.906e-06\n",
      "epoch 1184  loss 34.0970  lr 3.906e-06\n",
      "epoch 1185  loss 33.7148  lr 3.906e-06\n",
      "epoch 1186  loss 33.4663  lr 3.906e-06\n",
      "epoch 1187  loss 32.9179  lr 3.906e-06\n",
      "epoch 1188  loss 32.6789  lr 3.906e-06\n",
      "epoch 1189  loss 34.3005  lr 3.906e-06\n",
      "epoch 1190  loss 34.0043  lr 3.906e-06\n",
      "epoch 1191  loss 33.4718  lr 3.906e-06\n",
      "epoch 1192  loss 35.1424  lr 3.906e-06\n",
      "epoch 1193  loss 33.1845  lr 3.906e-06\n",
      "epoch 1194  loss 33.9818  lr 3.906e-06\n",
      "epoch 1195  loss 33.8870  lr 3.906e-06\n",
      "epoch 1196  loss 34.2266  lr 3.906e-06\n",
      "epoch 1197  loss 33.9902  lr 3.906e-06\n",
      "epoch 1198  loss 33.7885  lr 3.906e-06\n",
      "epoch 1199  loss 32.8641  lr 3.906e-06\n",
      "epoch 1200  loss 33.4271  lr 3.906e-06\n",
      "epoch 1201  loss 33.1742  lr 3.906e-06\n",
      "epoch 1202  loss 33.8880  lr 3.906e-06\n",
      "epoch 1203  loss 34.0850  lr 3.906e-06\n",
      "epoch 1204  loss 33.9841  lr 3.906e-06\n",
      "epoch 1205  loss 33.3330  lr 3.906e-06\n",
      "epoch 1206  loss 34.3218  lr 3.906e-06\n",
      "epoch 1207  loss 34.6248  lr 3.906e-06\n",
      "epoch 1208  loss 33.5140  lr 3.906e-06\n",
      "epoch 1209  loss 32.6585  lr 3.906e-06\n",
      "epoch 1210  loss 34.3106  lr 3.906e-06\n",
      "epoch 1211  loss 33.6160  lr 3.906e-06\n",
      "epoch 1212  loss 33.7767  lr 3.906e-06\n",
      "epoch 1213  loss 33.5460  lr 3.906e-06\n",
      "epoch 1214  loss 32.9953  lr 3.906e-06\n",
      "epoch 1215  loss 33.6198  lr 3.906e-06\n",
      "epoch 1216  loss 33.3158  lr 3.906e-06\n",
      "epoch 1217  loss 32.3092  lr 3.906e-06\n",
      "epoch 1218  loss 34.5819  lr 3.906e-06\n",
      "epoch 1219  loss 34.4375  lr 3.906e-06\n",
      "epoch 1220  loss 33.4480  lr 3.906e-06\n",
      "epoch 1221  loss 33.2062  lr 3.906e-06\n",
      "epoch 1222  loss 32.7795  lr 3.906e-06\n",
      "epoch 1223  loss 34.3565  lr 3.906e-06\n",
      "epoch 1224  loss 33.8244  lr 3.906e-06\n",
      "epoch 1225  loss 32.4780  lr 3.906e-06\n",
      "epoch 1226  loss 33.8873  lr 3.906e-06\n",
      "epoch 1227  loss 32.4097  lr 3.906e-06\n",
      "epoch 1228  loss 34.3603  lr 3.906e-06\n",
      "epoch 1229  loss 33.9762  lr 1.953e-06\n",
      "epoch 1230  loss 34.6775  lr 1.953e-06\n",
      "epoch 1231  loss 33.0947  lr 1.953e-06\n",
      "epoch 1232  loss 34.1777  lr 1.953e-06\n",
      "epoch 1233  loss 33.1251  lr 1.953e-06\n",
      "epoch 1234  loss 33.2527  lr 1.953e-06\n",
      "epoch 1235  loss 32.8227  lr 1.953e-06\n",
      "epoch 1236  loss 33.2745  lr 1.953e-06\n",
      "epoch 1237  loss 33.2560  lr 1.953e-06\n",
      "epoch 1238  loss 33.0863  lr 1.953e-06\n",
      "epoch 1239  loss 33.9358  lr 1.953e-06\n",
      "epoch 1240  loss 33.6230  lr 1.953e-06\n",
      "epoch 1241  loss 33.7694  lr 1.953e-06\n",
      "epoch 1242  loss 32.8261  lr 1.953e-06\n",
      "epoch 1243  loss 33.8853  lr 1.953e-06\n",
      "epoch 1244  loss 33.7683  lr 1.953e-06\n",
      "epoch 1245  loss 33.8755  lr 1.953e-06\n",
      "epoch 1246  loss 34.2256  lr 1.953e-06\n",
      "epoch 1247  loss 32.5952  lr 1.953e-06\n",
      "epoch 1248  loss 32.8987  lr 1.953e-06\n",
      "epoch 1249  loss 34.8110  lr 1.953e-06\n",
      "epoch 1250  loss 34.2156  lr 1.953e-06\n",
      "epoch 1251  loss 33.7821  lr 1.953e-06\n",
      "epoch 1252  loss 33.2515  lr 1.953e-06\n",
      "epoch 1253  loss 34.6333  lr 1.953e-06\n",
      "epoch 1254  loss 31.6419  lr 1.953e-06\n",
      "epoch 1255  loss 34.3527  lr 1.953e-06\n",
      "epoch 1256  loss 33.0687  lr 1.953e-06\n",
      "epoch 1257  loss 33.1661  lr 1.953e-06\n",
      "epoch 1258  loss 33.6914  lr 1.953e-06\n",
      "epoch 1259  loss 32.2417  lr 1.953e-06\n",
      "epoch 1260  loss 33.7255  lr 1.953e-06\n",
      "epoch 1261  loss 32.4717  lr 1.953e-06\n",
      "epoch 1262  loss 33.9820  lr 1.953e-06\n",
      "epoch 1263  loss 33.9514  lr 1.953e-06\n",
      "epoch 1264  loss 34.3740  lr 1.953e-06\n",
      "epoch 1265  loss 34.1361  lr 1.953e-06\n",
      "epoch 1266  loss 34.0634  lr 1.953e-06\n",
      "epoch 1267  loss 33.0042  lr 1.953e-06\n",
      "epoch 1268  loss 32.6298  lr 1.953e-06\n",
      "epoch 1269  loss 32.8792  lr 1.953e-06\n",
      "epoch 1270  loss 33.9792  lr 1.953e-06\n",
      "epoch 1271  loss 33.2176  lr 1.953e-06\n",
      "epoch 1272  loss 33.2682  lr 1.953e-06\n",
      "epoch 1273  loss 32.5467  lr 1.953e-06\n",
      "epoch 1274  loss 34.5352  lr 1.953e-06\n",
      "epoch 1275  loss 34.6851  lr 1.953e-06\n",
      "epoch 1276  loss 34.5774  lr 1.953e-06\n",
      "epoch 1277  loss 32.2447  lr 1.953e-06\n",
      "epoch 1278  loss 33.4072  lr 1.953e-06\n",
      "epoch 1279  loss 33.9070  lr 1.953e-06\n",
      "epoch 1280  loss 35.1514  lr 1.953e-06\n",
      "epoch 1281  loss 33.3536  lr 1.953e-06\n",
      "epoch 1282  loss 33.8971  lr 1.953e-06\n",
      "epoch 1283  loss 34.5405  lr 1.953e-06\n",
      "epoch 1284  loss 34.7230  lr 1.953e-06\n",
      "epoch 1285  loss 34.2361  lr 1.953e-06\n",
      "epoch 1286  loss 33.7420  lr 1.953e-06\n",
      "epoch 1287  loss 33.1123  lr 1.953e-06\n",
      "epoch 1288  loss 32.6412  lr 1.953e-06\n",
      "epoch 1289  loss 35.0276  lr 1.953e-06\n",
      "epoch 1290  loss 32.3607  lr 1.953e-06\n",
      "epoch 1291  loss 32.3871  lr 1.953e-06\n",
      "epoch 1292  loss 33.8783  lr 1.953e-06\n",
      "epoch 1293  loss 32.9589  lr 1.953e-06\n",
      "epoch 1294  loss 34.3012  lr 1.953e-06\n",
      "epoch 1295  loss 32.8788  lr 1.953e-06\n",
      "epoch 1296  loss 34.7906  lr 1.953e-06\n",
      "epoch 1297  loss 33.7643  lr 1.953e-06\n",
      "epoch 1298  loss 34.1481  lr 1.953e-06\n",
      "epoch 1299  loss 34.0277  lr 1.953e-06\n",
      "epoch 1300  loss 33.2804  lr 1.953e-06\n",
      "epoch 1301  loss 33.9828  lr 1.953e-06\n",
      "epoch 1302  loss 34.3510  lr 1.953e-06\n",
      "epoch 1303  loss 33.5974  lr 1.953e-06\n",
      "epoch 1304  loss 34.1740  lr 1.953e-06\n",
      "epoch 1305  loss 33.0892  lr 1.953e-06\n",
      "epoch 1306  loss 34.0685  lr 1.953e-06\n",
      "epoch 1307  loss 35.1789  lr 1.953e-06\n",
      "epoch 1308  loss 34.3315  lr 1.953e-06\n",
      "epoch 1309  loss 34.1059  lr 1.953e-06\n",
      "epoch 1310  loss 33.1415  lr 1.953e-06\n",
      "epoch 1311  loss 33.7980  lr 1.953e-06\n",
      "epoch 1312  loss 32.2644  lr 1.953e-06\n",
      "epoch 1313  loss 32.4821  lr 1.953e-06\n",
      "epoch 1314  loss 33.7748  lr 1.953e-06\n",
      "epoch 1315  loss 34.6797  lr 1.953e-06\n",
      "epoch 1316  loss 33.2088  lr 1.953e-06\n",
      "epoch 1317  loss 33.9571  lr 1.953e-06\n",
      "epoch 1318  loss 33.8304  lr 1.953e-06\n",
      "epoch 1319  loss 34.9012  lr 1.953e-06\n",
      "epoch 1320  loss 32.7465  lr 1.953e-06\n",
      "epoch 1321  loss 33.3047  lr 1.953e-06\n",
      "epoch 1322  loss 32.8341  lr 1.953e-06\n",
      "epoch 1323  loss 33.6604  lr 1.953e-06\n",
      "epoch 1324  loss 33.8868  lr 1.953e-06\n",
      "epoch 1325  loss 34.9402  lr 1.953e-06\n",
      "epoch 1326  loss 33.7742  lr 1.953e-06\n",
      "epoch 1327  loss 33.5806  lr 1.953e-06\n",
      "epoch 1328  loss 33.4333  lr 1.953e-06\n",
      "epoch 1329  loss 34.1011  lr 1.953e-06\n",
      "epoch 1330  loss 33.8965  lr 1.000e-06\n",
      "epoch 1331  loss 33.3290  lr 1.000e-06\n",
      "epoch 1332  loss 32.7975  lr 1.000e-06\n",
      "epoch 1333  loss 33.4089  lr 1.000e-06\n",
      "epoch 1334  loss 34.0443  lr 1.000e-06\n",
      "epoch 1335  loss 34.1206  lr 1.000e-06\n",
      "epoch 1336  loss 33.8307  lr 1.000e-06\n",
      "epoch 1337  loss 32.2924  lr 1.000e-06\n",
      "epoch 1338  loss 34.3932  lr 1.000e-06\n",
      "epoch 1339  loss 32.5295  lr 1.000e-06\n",
      "epoch 1340  loss 33.4371  lr 1.000e-06\n",
      "epoch 1341  loss 33.1493  lr 1.000e-06\n",
      "epoch 1342  loss 33.3550  lr 1.000e-06\n",
      "epoch 1343  loss 33.0910  lr 1.000e-06\n",
      "epoch 1344  loss 33.7628  lr 1.000e-06\n",
      "epoch 1345  loss 33.9285  lr 1.000e-06\n",
      "epoch 1346  loss 35.2484  lr 1.000e-06\n",
      "epoch 1347  loss 34.0177  lr 1.000e-06\n",
      "epoch 1348  loss 33.0243  lr 1.000e-06\n",
      "epoch 1349  loss 33.8500  lr 1.000e-06\n",
      "epoch 1350  loss 33.3185  lr 1.000e-06\n",
      "epoch 1351  loss 33.0607  lr 1.000e-06\n",
      "epoch 1352  loss 33.3347  lr 1.000e-06\n",
      "epoch 1353  loss 32.8255  lr 1.000e-06\n",
      "epoch 1354  loss 33.1971  lr 1.000e-06\n",
      "epoch 1355  loss 34.6783  lr 1.000e-06\n",
      "epoch 1356  loss 34.0700  lr 1.000e-06\n",
      "epoch 1357  loss 33.0108  lr 1.000e-06\n",
      "epoch 1358  loss 35.0309  lr 1.000e-06\n",
      "epoch 1359  loss 32.9296  lr 1.000e-06\n",
      "epoch 1360  loss 33.8435  lr 1.000e-06\n",
      "epoch 1361  loss 32.2109  lr 1.000e-06\n",
      "epoch 1362  loss 33.2307  lr 1.000e-06\n",
      "epoch 1363  loss 33.0284  lr 1.000e-06\n",
      "epoch 1364  loss 34.2382  lr 1.000e-06\n",
      "epoch 1365  loss 33.8544  lr 1.000e-06\n",
      "epoch 1366  loss 33.6302  lr 1.000e-06\n",
      "epoch 1367  loss 33.6591  lr 1.000e-06\n",
      "epoch 1368  loss 33.9592  lr 1.000e-06\n",
      "epoch 1369  loss 33.8975  lr 1.000e-06\n",
      "epoch 1370  loss 33.1297  lr 1.000e-06\n",
      "epoch 1371  loss 33.0181  lr 1.000e-06\n",
      "epoch 1372  loss 32.5672  lr 1.000e-06\n",
      "epoch 1373  loss 33.8397  lr 1.000e-06\n",
      "epoch 1374  loss 33.6895  lr 1.000e-06\n",
      "epoch 1375  loss 34.3733  lr 1.000e-06\n",
      "epoch 1376  loss 33.7116  lr 1.000e-06\n",
      "epoch 1377  loss 33.6607  lr 1.000e-06\n",
      "epoch 1378  loss 32.5445  lr 1.000e-06\n",
      "epoch 1379  loss 33.1647  lr 1.000e-06\n",
      "epoch 1380  loss 33.2376  lr 1.000e-06\n",
      "epoch 1381  loss 32.2078  lr 1.000e-06\n",
      "epoch 1382  loss 33.2155  lr 1.000e-06\n",
      "epoch 1383  loss 33.8683  lr 1.000e-06\n",
      "epoch 1384  loss 33.9585  lr 1.000e-06\n",
      "epoch 1385  loss 33.5465  lr 1.000e-06\n",
      "epoch 1386  loss 33.8247  lr 1.000e-06\n",
      "epoch 1387  loss 33.4650  lr 1.000e-06\n",
      "epoch 1388  loss 33.7988  lr 1.000e-06\n",
      "epoch 1389  loss 32.9655  lr 1.000e-06\n",
      "epoch 1390  loss 32.6076  lr 1.000e-06\n",
      "epoch 1391  loss 33.2958  lr 1.000e-06\n",
      "epoch 1392  loss 32.6374  lr 1.000e-06\n",
      "epoch 1393  loss 33.0728  lr 1.000e-06\n",
      "epoch 1394  loss 34.1637  lr 1.000e-06\n",
      "epoch 1395  loss 33.7277  lr 1.000e-06\n",
      "epoch 1396  loss 33.5927  lr 1.000e-06\n",
      "epoch 1397  loss 33.8578  lr 1.000e-06\n",
      "epoch 1398  loss 32.9590  lr 1.000e-06\n",
      "epoch 1399  loss 32.8433  lr 1.000e-06\n",
      "epoch 1400  loss 34.1231  lr 1.000e-06\n",
      "epoch 1401  loss 34.3127  lr 1.000e-06\n",
      "epoch 1402  loss 34.4007  lr 1.000e-06\n",
      "epoch 1403  loss 34.0409  lr 1.000e-06\n",
      "epoch 1404  loss 33.5430  lr 1.000e-06\n",
      "epoch 1405  loss 33.6848  lr 1.000e-06\n",
      "epoch 1406  loss 33.3184  lr 1.000e-06\n",
      "epoch 1407  loss 32.4640  lr 1.000e-06\n",
      "epoch 1408  loss 33.2411  lr 1.000e-06\n",
      "epoch 1409  loss 33.2522  lr 1.000e-06\n",
      "epoch 1410  loss 33.7090  lr 1.000e-06\n",
      "epoch 1411  loss 34.0523  lr 1.000e-06\n",
      "epoch 1412  loss 34.4121  lr 1.000e-06\n",
      "epoch 1413  loss 34.5293  lr 1.000e-06\n",
      "epoch 1414  loss 33.9965  lr 1.000e-06\n",
      "epoch 1415  loss 33.4631  lr 1.000e-06\n",
      "epoch 1416  loss 33.6404  lr 1.000e-06\n",
      "epoch 1417  loss 33.9108  lr 1.000e-06\n",
      "epoch 1418  loss 34.5903  lr 1.000e-06\n",
      "epoch 1419  loss 34.0248  lr 1.000e-06\n",
      "epoch 1420  loss 33.5071  lr 1.000e-06\n",
      "epoch 1421  loss 33.4212  lr 1.000e-06\n",
      "epoch 1422  loss 32.6648  lr 1.000e-06\n",
      "epoch 1423  loss 33.2618  lr 1.000e-06\n",
      "epoch 1424  loss 33.5378  lr 1.000e-06\n",
      "epoch 1425  loss 33.6037  lr 1.000e-06\n",
      "epoch 1426  loss 34.1423  lr 1.000e-06\n",
      "epoch 1427  loss 33.8868  lr 1.000e-06\n",
      "epoch 1428  loss 33.7185  lr 1.000e-06\n",
      "epoch 1429  loss 33.2118  lr 1.000e-06\n",
      "epoch 1430  loss 34.4955  lr 1.000e-06\n",
      "epoch 1431  loss 34.0684  lr 1.000e-06\n",
      "epoch 1432  loss 34.0682  lr 1.000e-06\n",
      "epoch 1433  loss 34.2494  lr 1.000e-06\n",
      "epoch 1434  loss 33.9654  lr 1.000e-06\n",
      "epoch 1435  loss 33.2517  lr 1.000e-06\n",
      "epoch 1436  loss 33.0027  lr 1.000e-06\n",
      "epoch 1437  loss 33.6840  lr 1.000e-06\n",
      "epoch 1438  loss 33.7352  lr 1.000e-06\n",
      "epoch 1439  loss 33.6903  lr 1.000e-06\n",
      "epoch 1440  loss 33.3414  lr 1.000e-06\n",
      "epoch 1441  loss 33.6789  lr 1.000e-06\n",
      "epoch 1442  loss 32.7440  lr 1.000e-06\n",
      "epoch 1443  loss 34.1935  lr 1.000e-06\n",
      "epoch 1444  loss 33.2680  lr 1.000e-06\n",
      "epoch 1445  loss 33.4431  lr 1.000e-06\n",
      "epoch 1446  loss 35.1623  lr 1.000e-06\n",
      "epoch 1447  loss 32.3978  lr 1.000e-06\n",
      "epoch 1448  loss 32.9003  lr 1.000e-06\n",
      "epoch 1449  loss 34.7681  lr 1.000e-06\n",
      "epoch 1450  loss 33.6678  lr 1.000e-06\n",
      "epoch 1451  loss 33.6770  lr 1.000e-06\n",
      "epoch 1452  loss 33.4786  lr 1.000e-06\n",
      "epoch 1453  loss 34.4787  lr 1.000e-06\n",
      "epoch 1454  loss 33.7246  lr 1.000e-06\n",
      "epoch 1455  loss 33.5097  lr 1.000e-06\n",
      "epoch 1456  loss 32.8067  lr 1.000e-06\n",
      "epoch 1457  loss 35.2239  lr 1.000e-06\n",
      "epoch 1458  loss 34.5837  lr 1.000e-06\n",
      "epoch 1459  loss 33.6139  lr 1.000e-06\n",
      "epoch 1460  loss 34.3026  lr 1.000e-06\n",
      "epoch 1461  loss 32.3049  lr 1.000e-06\n",
      "epoch 1462  loss 32.8605  lr 1.000e-06\n",
      "epoch 1463  loss 33.3781  lr 1.000e-06\n",
      "epoch 1464  loss 32.8460  lr 1.000e-06\n",
      "epoch 1465  loss 32.7447  lr 1.000e-06\n",
      "epoch 1466  loss 32.4337  lr 1.000e-06\n",
      "epoch 1467  loss 34.4457  lr 1.000e-06\n",
      "epoch 1468  loss 33.2955  lr 1.000e-06\n",
      "epoch 1469  loss 33.6870  lr 1.000e-06\n",
      "epoch 1470  loss 33.9117  lr 1.000e-06\n",
      "epoch 1471  loss 32.8672  lr 1.000e-06\n",
      "epoch 1472  loss 34.4787  lr 1.000e-06\n",
      "epoch 1473  loss 34.1910  lr 1.000e-06\n",
      "epoch 1474  loss 32.8843  lr 1.000e-06\n",
      "epoch 1475  loss 32.4912  lr 1.000e-06\n",
      "epoch 1476  loss 34.3151  lr 1.000e-06\n",
      "epoch 1477  loss 34.0652  lr 1.000e-06\n",
      "epoch 1478  loss 32.8878  lr 1.000e-06\n",
      "epoch 1479  loss 33.9205  lr 1.000e-06\n",
      "epoch 1480  loss 35.0581  lr 1.000e-06\n",
      "epoch 1481  loss 33.5659  lr 1.000e-06\n",
      "epoch 1482  loss 32.8707  lr 1.000e-06\n",
      "epoch 1483  loss 33.9198  lr 1.000e-06\n",
      "epoch 1484  loss 32.8890  lr 1.000e-06\n",
      "epoch 1485  loss 32.9642  lr 1.000e-06\n",
      "epoch 1486  loss 34.2233  lr 1.000e-06\n",
      "epoch 1487  loss 33.5535  lr 1.000e-06\n",
      "epoch 1488  loss 32.7308  lr 1.000e-06\n",
      "epoch 1489  loss 34.4755  lr 1.000e-06\n",
      "epoch 1490  loss 33.4342  lr 1.000e-06\n",
      "epoch 1491  loss 33.3281  lr 1.000e-06\n",
      "epoch 1492  loss 34.4377  lr 1.000e-06\n",
      "epoch 1493  loss 32.5702  lr 1.000e-06\n",
      "epoch 1494  loss 34.1687  lr 1.000e-06\n",
      "epoch 1495  loss 33.5084  lr 1.000e-06\n",
      "epoch 1496  loss 33.9964  lr 1.000e-06\n",
      "epoch 1497  loss 32.6710  lr 1.000e-06\n",
      "epoch 1498  loss 32.9468  lr 1.000e-06\n",
      "epoch 1499  loss 33.0586  lr 1.000e-06\n",
      "epoch 1500  loss 33.4183  lr 1.000e-06\n",
      "epoch 1501  loss 32.4650  lr 1.000e-06\n",
      "epoch 1502  loss 32.6390  lr 1.000e-06\n",
      "epoch 1503  loss 33.1288  lr 1.000e-06\n",
      "epoch 1504  loss 32.6903  lr 1.000e-06\n",
      "epoch 1505  loss 33.9192  lr 1.000e-06\n",
      "epoch 1506  loss 34.0880  lr 1.000e-06\n",
      "epoch 1507  loss 33.7591  lr 1.000e-06\n",
      "epoch 1508  loss 33.0677  lr 1.000e-06\n",
      "epoch 1509  loss 33.6778  lr 1.000e-06\n",
      "epoch 1510  loss 33.1380  lr 1.000e-06\n",
      "epoch 1511  loss 33.8960  lr 1.000e-06\n",
      "epoch 1512  loss 33.6754  lr 1.000e-06\n",
      "epoch 1513  loss 34.3178  lr 1.000e-06\n",
      "epoch 1514  loss 32.9116  lr 1.000e-06\n",
      "epoch 1515  loss 32.3697  lr 1.000e-06\n",
      "epoch 1516  loss 34.2441  lr 1.000e-06\n",
      "epoch 1517  loss 33.3388  lr 1.000e-06\n",
      "epoch 1518  loss 33.3536  lr 1.000e-06\n",
      "epoch 1519  loss 32.8976  lr 1.000e-06\n",
      "epoch 1520  loss 32.6855  lr 1.000e-06\n",
      "epoch 1521  loss 33.4311  lr 1.000e-06\n",
      "epoch 1522  loss 32.7123  lr 1.000e-06\n",
      "epoch 1523  loss 34.6149  lr 1.000e-06\n",
      "epoch 1524  loss 33.4066  lr 1.000e-06\n",
      "epoch 1525  loss 33.8490  lr 1.000e-06\n",
      "epoch 1526  loss 34.2882  lr 1.000e-06\n",
      "epoch 1527  loss 33.9365  lr 1.000e-06\n",
      "epoch 1528  loss 34.4068  lr 1.000e-06\n",
      "epoch 1529  loss 34.5198  lr 1.000e-06\n",
      "epoch 1530  loss 33.9341  lr 1.000e-06\n",
      "epoch 1531  loss 33.0637  lr 1.000e-06\n",
      "epoch 1532  loss 33.9915  lr 1.000e-06\n",
      "epoch 1533  loss 33.2886  lr 1.000e-06\n",
      "epoch 1534  loss 33.8472  lr 1.000e-06\n",
      "epoch 1535  loss 33.0640  lr 1.000e-06\n",
      "epoch 1536  loss 33.9409  lr 1.000e-06\n",
      "epoch 1537  loss 32.8283  lr 1.000e-06\n",
      "epoch 1538  loss 33.6842  lr 1.000e-06\n",
      "epoch 1539  loss 33.9399  lr 1.000e-06\n",
      "epoch 1540  loss 32.8715  lr 1.000e-06\n",
      "epoch 1541  loss 33.4247  lr 1.000e-06\n",
      "epoch 1542  loss 34.1270  lr 1.000e-06\n",
      "epoch 1543  loss 33.2064  lr 1.000e-06\n",
      "epoch 1544  loss 34.5061  lr 1.000e-06\n",
      "epoch 1545  loss 34.1007  lr 1.000e-06\n",
      "epoch 1546  loss 33.2968  lr 1.000e-06\n",
      "epoch 1547  loss 33.6870  lr 1.000e-06\n",
      "epoch 1548  loss 33.1387  lr 1.000e-06\n",
      "epoch 1549  loss 33.5317  lr 1.000e-06\n",
      "epoch 1550  loss 32.7323  lr 1.000e-06\n",
      "epoch 1551  loss 33.6221  lr 1.000e-06\n",
      "epoch 1552  loss 35.2540  lr 1.000e-06\n",
      "epoch 1553  loss 33.0231  lr 1.000e-06\n",
      "epoch 1554  loss 33.3508  lr 1.000e-06\n",
      "epoch 1555  loss 34.2709  lr 1.000e-06\n",
      "epoch 1556  loss 34.3662  lr 1.000e-06\n",
      "epoch 1557  loss 33.5179  lr 1.000e-06\n",
      "epoch 1558  loss 33.7272  lr 1.000e-06\n",
      "epoch 1559  loss 33.9057  lr 1.000e-06\n",
      "epoch 1560  loss 34.8223  lr 1.000e-06\n",
      "epoch 1561  loss 33.6164  lr 1.000e-06\n",
      "epoch 1562  loss 33.0905  lr 1.000e-06\n",
      "epoch 1563  loss 32.8545  lr 1.000e-06\n",
      "epoch 1564  loss 33.3505  lr 1.000e-06\n",
      "epoch 1565  loss 34.2616  lr 1.000e-06\n",
      "epoch 1566  loss 32.2163  lr 1.000e-06\n",
      "epoch 1567  loss 32.2417  lr 1.000e-06\n",
      "epoch 1568  loss 33.4664  lr 1.000e-06\n",
      "epoch 1569  loss 33.3061  lr 1.000e-06\n",
      "epoch 1570  loss 33.6294  lr 1.000e-06\n",
      "epoch 1571  loss 33.3710  lr 1.000e-06\n",
      "epoch 1572  loss 34.9229  lr 1.000e-06\n",
      "epoch 1573  loss 34.4891  lr 1.000e-06\n",
      "epoch 1574  loss 33.4108  lr 1.000e-06\n",
      "epoch 1575  loss 34.2532  lr 1.000e-06\n",
      "epoch 1576  loss 32.3437  lr 1.000e-06\n",
      "epoch 1577  loss 33.7900  lr 1.000e-06\n",
      "epoch 1578  loss 34.0006  lr 1.000e-06\n",
      "epoch 1579  loss 32.8441  lr 1.000e-06\n",
      "epoch 1580  loss 32.7406  lr 1.000e-06\n",
      "epoch 1581  loss 32.5806  lr 1.000e-06\n",
      "epoch 1582  loss 34.5434  lr 1.000e-06\n",
      "epoch 1583  loss 33.1868  lr 1.000e-06\n",
      "epoch 1584  loss 33.1586  lr 1.000e-06\n",
      "epoch 1585  loss 32.7433  lr 1.000e-06\n",
      "epoch 1586  loss 34.0875  lr 1.000e-06\n",
      "epoch 1587  loss 33.7290  lr 1.000e-06\n",
      "epoch 1588  loss 33.3338  lr 1.000e-06\n",
      "epoch 1589  loss 33.3405  lr 1.000e-06\n",
      "epoch 1590  loss 34.4569  lr 1.000e-06\n",
      "epoch 1591  loss 34.8534  lr 1.000e-06\n",
      "epoch 1592  loss 32.8715  lr 1.000e-06\n",
      "epoch 1593  loss 33.1937  lr 1.000e-06\n",
      "epoch 1594  loss 34.7150  lr 1.000e-06\n",
      "epoch 1595  loss 33.7068  lr 1.000e-06\n",
      "epoch 1596  loss 33.7596  lr 1.000e-06\n",
      "epoch 1597  loss 33.7151  lr 1.000e-06\n",
      "epoch 1598  loss 33.0036  lr 1.000e-06\n",
      "epoch 1599  loss 33.0885  lr 1.000e-06\n",
      "epoch 1600  loss 33.7028  lr 1.000e-06\n",
      "epoch 1601  loss 33.7009  lr 1.000e-06\n",
      "epoch 1602  loss 33.5317  lr 1.000e-06\n",
      "epoch 1603  loss 33.9005  lr 1.000e-06\n",
      "epoch 1604  loss 35.4254  lr 1.000e-06\n",
      "epoch 1605  loss 34.0415  lr 1.000e-06\n",
      "epoch 1606  loss 33.8770  lr 1.000e-06\n",
      "epoch 1607  loss 33.0492  lr 1.000e-06\n",
      "epoch 1608  loss 34.0946  lr 1.000e-06\n",
      "epoch 1609  loss 33.2999  lr 1.000e-06\n",
      "epoch 1610  loss 33.6082  lr 1.000e-06\n",
      "epoch 1611  loss 34.4239  lr 1.000e-06\n",
      "epoch 1612  loss 32.3704  lr 1.000e-06\n",
      "epoch 1613  loss 32.9368  lr 1.000e-06\n",
      "epoch 1614  loss 33.1563  lr 1.000e-06\n",
      "epoch 1615  loss 34.2984  lr 1.000e-06\n",
      "epoch 1616  loss 33.5916  lr 1.000e-06\n",
      "epoch 1617  loss 33.9407  lr 1.000e-06\n",
      "epoch 1618  loss 32.5656  lr 1.000e-06\n",
      "epoch 1619  loss 32.8558  lr 1.000e-06\n",
      "epoch 1620  loss 33.1931  lr 1.000e-06\n",
      "epoch 1621  loss 33.4792  lr 1.000e-06\n",
      "epoch 1622  loss 33.8051  lr 1.000e-06\n",
      "epoch 1623  loss 33.9601  lr 1.000e-06\n",
      "epoch 1624  loss 33.1691  lr 1.000e-06\n",
      "epoch 1625  loss 34.1396  lr 1.000e-06\n",
      "epoch 1626  loss 33.8032  lr 1.000e-06\n",
      "epoch 1627  loss 33.4711  lr 1.000e-06\n",
      "epoch 1628  loss 33.0995  lr 1.000e-06\n",
      "epoch 1629  loss 33.5720  lr 1.000e-06\n",
      "epoch 1630  loss 33.8707  lr 1.000e-06\n",
      "epoch 1631  loss 34.2577  lr 1.000e-06\n",
      "epoch 1632  loss 32.4960  lr 1.000e-06\n",
      "epoch 1633  loss 33.6527  lr 1.000e-06\n",
      "epoch 1634  loss 32.6860  lr 1.000e-06\n",
      "epoch 1635  loss 34.1175  lr 1.000e-06\n",
      "epoch 1636  loss 32.5890  lr 1.000e-06\n",
      "epoch 1637  loss 33.7010  lr 1.000e-06\n",
      "epoch 1638  loss 32.7489  lr 1.000e-06\n",
      "epoch 1639  loss 34.3269  lr 1.000e-06\n",
      "epoch 1640  loss 33.4789  lr 1.000e-06\n",
      "epoch 1641  loss 34.9532  lr 1.000e-06\n",
      "epoch 1642  loss 33.4658  lr 1.000e-06\n",
      "epoch 1643  loss 33.0440  lr 1.000e-06\n",
      "epoch 1644  loss 33.3743  lr 1.000e-06\n",
      "epoch 1645  loss 34.4253  lr 1.000e-06\n",
      "epoch 1646  loss 34.2000  lr 1.000e-06\n",
      "epoch 1647  loss 34.2085  lr 1.000e-06\n",
      "epoch 1648  loss 33.1178  lr 1.000e-06\n",
      "epoch 1649  loss 33.1936  lr 1.000e-06\n",
      "epoch 1650  loss 34.3096  lr 1.000e-06\n",
      "epoch 1651  loss 32.7098  lr 1.000e-06\n",
      "epoch 1652  loss 34.0324  lr 1.000e-06\n",
      "epoch 1653  loss 33.8886  lr 1.000e-06\n",
      "epoch 1654  loss 33.2329  lr 1.000e-06\n",
      "epoch 1655  loss 33.2966  lr 1.000e-06\n",
      "epoch 1656  loss 33.7652  lr 1.000e-06\n",
      "epoch 1657  loss 33.7754  lr 1.000e-06\n",
      "epoch 1658  loss 34.2262  lr 1.000e-06\n",
      "epoch 1659  loss 34.3591  lr 1.000e-06\n",
      "epoch 1660  loss 32.6857  lr 1.000e-06\n",
      "epoch 1661  loss 33.9085  lr 1.000e-06\n",
      "epoch 1662  loss 32.7374  lr 1.000e-06\n",
      "epoch 1663  loss 31.8770  lr 1.000e-06\n",
      "epoch 1664  loss 33.9196  lr 1.000e-06\n",
      "epoch 1665  loss 33.8435  lr 1.000e-06\n",
      "epoch 1666  loss 33.7208  lr 1.000e-06\n",
      "epoch 1667  loss 33.9122  lr 1.000e-06\n",
      "epoch 1668  loss 34.1571  lr 1.000e-06\n",
      "epoch 1669  loss 33.4254  lr 1.000e-06\n",
      "epoch 1670  loss 34.1022  lr 1.000e-06\n",
      "epoch 1671  loss 33.5573  lr 1.000e-06\n",
      "epoch 1672  loss 33.9892  lr 1.000e-06\n",
      "epoch 1673  loss 33.4298  lr 1.000e-06\n",
      "epoch 1674  loss 33.3976  lr 1.000e-06\n",
      "epoch 1675  loss 32.9668  lr 1.000e-06\n",
      "epoch 1676  loss 33.5493  lr 1.000e-06\n",
      "epoch 1677  loss 32.8583  lr 1.000e-06\n",
      "epoch 1678  loss 33.4087  lr 1.000e-06\n",
      "epoch 1679  loss 32.8503  lr 1.000e-06\n",
      "epoch 1680  loss 34.1953  lr 1.000e-06\n",
      "epoch 1681  loss 33.5641  lr 1.000e-06\n",
      "epoch 1682  loss 33.7849  lr 1.000e-06\n",
      "epoch 1683  loss 35.0992  lr 1.000e-06\n",
      "epoch 1684  loss 33.0218  lr 1.000e-06\n",
      "epoch 1685  loss 33.8140  lr 1.000e-06\n",
      "epoch 1686  loss 34.0001  lr 1.000e-06\n",
      "epoch 1687  loss 33.0680  lr 1.000e-06\n",
      "epoch 1688  loss 32.5211  lr 1.000e-06\n",
      "epoch 1689  loss 33.5815  lr 1.000e-06\n",
      "epoch 1690  loss 33.3671  lr 1.000e-06\n",
      "epoch 1691  loss 34.8449  lr 1.000e-06\n",
      "epoch 1692  loss 32.6115  lr 1.000e-06\n",
      "epoch 1693  loss 32.7403  lr 1.000e-06\n",
      "epoch 1694  loss 33.1035  lr 1.000e-06\n",
      "epoch 1695  loss 33.4548  lr 1.000e-06\n",
      "epoch 1696  loss 34.1334  lr 1.000e-06\n",
      "epoch 1697  loss 33.2144  lr 1.000e-06\n",
      "epoch 1698  loss 32.9029  lr 1.000e-06\n",
      "epoch 1699  loss 33.8610  lr 1.000e-06\n",
      "epoch 1700  loss 32.9936  lr 1.000e-06\n",
      "epoch 1701  loss 32.4541  lr 1.000e-06\n",
      "epoch 1702  loss 34.0271  lr 1.000e-06\n",
      "epoch 1703  loss 32.8896  lr 1.000e-06\n",
      "epoch 1704  loss 33.0642  lr 1.000e-06\n",
      "epoch 1705  loss 32.7748  lr 1.000e-06\n",
      "epoch 1706  loss 33.3409  lr 1.000e-06\n",
      "epoch 1707  loss 34.2960  lr 1.000e-06\n",
      "epoch 1708  loss 33.5361  lr 1.000e-06\n",
      "epoch 1709  loss 33.2203  lr 1.000e-06\n",
      "epoch 1710  loss 33.7353  lr 1.000e-06\n",
      "epoch 1711  loss 33.6810  lr 1.000e-06\n",
      "epoch 1712  loss 32.9908  lr 1.000e-06\n",
      "epoch 1713  loss 33.9949  lr 1.000e-06\n",
      "epoch 1714  loss 33.9387  lr 1.000e-06\n",
      "epoch 1715  loss 33.5404  lr 1.000e-06\n",
      "epoch 1716  loss 33.1081  lr 1.000e-06\n",
      "epoch 1717  loss 33.1956  lr 1.000e-06\n",
      "epoch 1718  loss 33.8512  lr 1.000e-06\n",
      "epoch 1719  loss 34.0182  lr 1.000e-06\n",
      "epoch 1720  loss 34.3889  lr 1.000e-06\n",
      "epoch 1721  loss 34.3268  lr 1.000e-06\n",
      "epoch 1722  loss 33.9724  lr 1.000e-06\n",
      "epoch 1723  loss 34.0452  lr 1.000e-06\n",
      "epoch 1724  loss 34.1799  lr 1.000e-06\n",
      "epoch 1725  loss 34.5674  lr 1.000e-06\n",
      "epoch 1726  loss 33.4344  lr 1.000e-06\n",
      "epoch 1727  loss 33.8380  lr 1.000e-06\n",
      "epoch 1728  loss 32.4593  lr 1.000e-06\n",
      "epoch 1729  loss 33.3611  lr 1.000e-06\n",
      "epoch 1730  loss 34.8724  lr 1.000e-06\n",
      "epoch 1731  loss 32.4430  lr 1.000e-06\n",
      "epoch 1732  loss 34.2352  lr 1.000e-06\n",
      "epoch 1733  loss 33.0491  lr 1.000e-06\n",
      "epoch 1734  loss 34.1443  lr 1.000e-06\n",
      "epoch 1735  loss 35.2448  lr 1.000e-06\n",
      "epoch 1736  loss 33.8517  lr 1.000e-06\n",
      "epoch 1737  loss 32.8368  lr 1.000e-06\n",
      "epoch 1738  loss 33.1426  lr 1.000e-06\n",
      "epoch 1739  loss 33.5942  lr 1.000e-06\n",
      "epoch 1740  loss 32.7352  lr 1.000e-06\n",
      "epoch 1741  loss 33.0663  lr 1.000e-06\n",
      "epoch 1742  loss 33.6284  lr 1.000e-06\n",
      "epoch 1743  loss 33.1897  lr 1.000e-06\n",
      "epoch 1744  loss 32.8979  lr 1.000e-06\n",
      "epoch 1745  loss 33.4762  lr 1.000e-06\n",
      "epoch 1746  loss 34.0678  lr 1.000e-06\n",
      "epoch 1747  loss 33.1759  lr 1.000e-06\n",
      "epoch 1748  loss 33.7012  lr 1.000e-06\n",
      "epoch 1749  loss 33.2131  lr 1.000e-06\n",
      "epoch 1750  loss 33.6134  lr 1.000e-06\n",
      "epoch 1751  loss 32.9472  lr 1.000e-06\n",
      "epoch 1752  loss 33.1807  lr 1.000e-06\n",
      "epoch 1753  loss 34.3746  lr 1.000e-06\n",
      "epoch 1754  loss 33.6162  lr 1.000e-06\n",
      "epoch 1755  loss 32.3993  lr 1.000e-06\n",
      "epoch 1756  loss 32.7647  lr 1.000e-06\n",
      "epoch 1757  loss 34.4609  lr 1.000e-06\n",
      "epoch 1758  loss 34.9644  lr 1.000e-06\n",
      "epoch 1759  loss 33.5114  lr 1.000e-06\n",
      "epoch 1760  loss 33.7792  lr 1.000e-06\n",
      "epoch 1761  loss 33.0545  lr 1.000e-06\n",
      "epoch 1762  loss 34.1837  lr 1.000e-06\n",
      "epoch 1763  loss 33.6263  lr 1.000e-06\n",
      "epoch 1764  loss 34.1209  lr 1.000e-06\n",
      "epoch 1765  loss 32.9877  lr 1.000e-06\n",
      "epoch 1766  loss 34.4353  lr 1.000e-06\n",
      "epoch 1767  loss 34.8658  lr 1.000e-06\n",
      "epoch 1768  loss 32.8445  lr 1.000e-06\n",
      "epoch 1769  loss 33.0402  lr 1.000e-06\n",
      "epoch 1770  loss 33.5642  lr 1.000e-06\n",
      "epoch 1771  loss 33.9136  lr 1.000e-06\n",
      "epoch 1772  loss 33.1858  lr 1.000e-06\n",
      "epoch 1773  loss 33.3085  lr 1.000e-06\n",
      "epoch 1774  loss 35.1925  lr 1.000e-06\n",
      "epoch 1775  loss 33.8725  lr 1.000e-06\n",
      "epoch 1776  loss 32.9054  lr 1.000e-06\n",
      "epoch 1777  loss 34.8782  lr 1.000e-06\n",
      "epoch 1778  loss 33.6695  lr 1.000e-06\n",
      "epoch 1779  loss 33.5044  lr 1.000e-06\n",
      "epoch 1780  loss 32.9775  lr 1.000e-06\n",
      "epoch 1781  loss 33.0173  lr 1.000e-06\n",
      "epoch 1782  loss 33.3978  lr 1.000e-06\n",
      "epoch 1783  loss 32.9507  lr 1.000e-06\n",
      "epoch 1784  loss 34.3631  lr 1.000e-06\n",
      "epoch 1785  loss 32.8101  lr 1.000e-06\n",
      "epoch 1786  loss 33.4180  lr 1.000e-06\n",
      "epoch 1787  loss 32.9990  lr 1.000e-06\n",
      "epoch 1788  loss 33.9022  lr 1.000e-06\n",
      "epoch 1789  loss 33.6781  lr 1.000e-06\n",
      "epoch 1790  loss 34.1027  lr 1.000e-06\n",
      "epoch 1791  loss 33.6129  lr 1.000e-06\n",
      "epoch 1792  loss 33.1389  lr 1.000e-06\n",
      "epoch 1793  loss 34.0966  lr 1.000e-06\n",
      "epoch 1794  loss 32.8186  lr 1.000e-06\n",
      "epoch 1795  loss 32.8923  lr 1.000e-06\n",
      "epoch 1796  loss 34.1379  lr 1.000e-06\n",
      "epoch 1797  loss 33.0100  lr 1.000e-06\n",
      "epoch 1798  loss 33.4102  lr 1.000e-06\n",
      "epoch 1799  loss 33.5112  lr 1.000e-06\n",
      "epoch 1800  loss 33.5658  lr 1.000e-06\n",
      "epoch 1801  loss 33.5948  lr 1.000e-06\n",
      "epoch 1802  loss 33.9443  lr 1.000e-06\n",
      "epoch 1803  loss 34.2964  lr 1.000e-06\n",
      "epoch 1804  loss 32.2338  lr 1.000e-06\n",
      "epoch 1805  loss 33.2129  lr 1.000e-06\n",
      "epoch 1806  loss 33.7735  lr 1.000e-06\n",
      "epoch 1807  loss 33.2621  lr 1.000e-06\n",
      "epoch 1808  loss 33.2505  lr 1.000e-06\n",
      "epoch 1809  loss 32.9395  lr 1.000e-06\n",
      "epoch 1810  loss 34.4590  lr 1.000e-06\n",
      "epoch 1811  loss 33.2886  lr 1.000e-06\n",
      "epoch 1812  loss 32.8922  lr 1.000e-06\n",
      "epoch 1813  loss 34.7278  lr 1.000e-06\n",
      "epoch 1814  loss 32.8228  lr 1.000e-06\n",
      "epoch 1815  loss 33.6213  lr 1.000e-06\n",
      "epoch 1816  loss 33.0422  lr 1.000e-06\n",
      "epoch 1817  loss 34.5036  lr 1.000e-06\n",
      "epoch 1818  loss 32.4073  lr 1.000e-06\n",
      "epoch 1819  loss 33.8754  lr 1.000e-06\n",
      "epoch 1820  loss 33.9199  lr 1.000e-06\n",
      "epoch 1821  loss 33.5900  lr 1.000e-06\n",
      "epoch 1822  loss 33.1528  lr 1.000e-06\n",
      "epoch 1823  loss 34.5961  lr 1.000e-06\n",
      "epoch 1824  loss 33.5234  lr 1.000e-06\n",
      "epoch 1825  loss 32.6351  lr 1.000e-06\n",
      "epoch 1826  loss 33.2010  lr 1.000e-06\n",
      "epoch 1827  loss 32.9126  lr 1.000e-06\n",
      "epoch 1828  loss 33.1606  lr 1.000e-06\n",
      "epoch 1829  loss 34.4645  lr 1.000e-06\n",
      "epoch 1830  loss 33.1498  lr 1.000e-06\n",
      "epoch 1831  loss 32.3486  lr 1.000e-06\n",
      "epoch 1832  loss 32.8124  lr 1.000e-06\n",
      "epoch 1833  loss 32.6700  lr 1.000e-06\n",
      "epoch 1834  loss 34.1286  lr 1.000e-06\n",
      "epoch 1835  loss 33.8845  lr 1.000e-06\n",
      "epoch 1836  loss 33.9330  lr 1.000e-06\n",
      "epoch 1837  loss 33.4303  lr 1.000e-06\n",
      "epoch 1838  loss 33.5279  lr 1.000e-06\n",
      "epoch 1839  loss 34.0205  lr 1.000e-06\n",
      "epoch 1840  loss 33.5548  lr 1.000e-06\n",
      "epoch 1841  loss 33.4920  lr 1.000e-06\n",
      "epoch 1842  loss 33.6010  lr 1.000e-06\n",
      "epoch 1843  loss 32.8098  lr 1.000e-06\n",
      "epoch 1844  loss 33.7508  lr 1.000e-06\n",
      "epoch 1845  loss 34.4703  lr 1.000e-06\n",
      "epoch 1846  loss 31.5180  lr 1.000e-06\n",
      "epoch 1847  loss 33.6246  lr 1.000e-06\n",
      "epoch 1848  loss 33.2747  lr 1.000e-06\n",
      "epoch 1849  loss 33.6122  lr 1.000e-06\n",
      "epoch 1850  loss 33.7634  lr 1.000e-06\n",
      "epoch 1851  loss 31.9750  lr 1.000e-06\n",
      "epoch 1852  loss 33.6639  lr 1.000e-06\n",
      "epoch 1853  loss 33.3863  lr 1.000e-06\n",
      "epoch 1854  loss 33.1556  lr 1.000e-06\n",
      "epoch 1855  loss 34.3851  lr 1.000e-06\n",
      "epoch 1856  loss 32.8380  lr 1.000e-06\n",
      "epoch 1857  loss 34.5034  lr 1.000e-06\n",
      "epoch 1858  loss 32.1070  lr 1.000e-06\n",
      "epoch 1859  loss 33.5371  lr 1.000e-06\n",
      "epoch 1860  loss 32.8970  lr 1.000e-06\n",
      "epoch 1861  loss 33.2549  lr 1.000e-06\n",
      "epoch 1862  loss 34.0952  lr 1.000e-06\n",
      "epoch 1863  loss 33.5315  lr 1.000e-06\n",
      "epoch 1864  loss 33.4801  lr 1.000e-06\n",
      "epoch 1865  loss 32.9265  lr 1.000e-06\n",
      "epoch 1866  loss 33.9510  lr 1.000e-06\n",
      "epoch 1867  loss 33.2750  lr 1.000e-06\n",
      "epoch 1868  loss 33.4065  lr 1.000e-06\n",
      "epoch 1869  loss 33.6465  lr 1.000e-06\n",
      "epoch 1870  loss 32.7329  lr 1.000e-06\n",
      "epoch 1871  loss 33.6419  lr 1.000e-06\n",
      "epoch 1872  loss 34.2393  lr 1.000e-06\n",
      "epoch 1873  loss 33.5384  lr 1.000e-06\n",
      "epoch 1874  loss 33.6766  lr 1.000e-06\n",
      "epoch 1875  loss 34.2489  lr 1.000e-06\n",
      "epoch 1876  loss 34.3228  lr 1.000e-06\n",
      "epoch 1877  loss 33.1858  lr 1.000e-06\n",
      "epoch 1878  loss 34.3340  lr 1.000e-06\n",
      "epoch 1879  loss 34.0931  lr 1.000e-06\n",
      "epoch 1880  loss 34.5898  lr 1.000e-06\n",
      "epoch 1881  loss 33.1404  lr 1.000e-06\n",
      "epoch 1882  loss 32.8436  lr 1.000e-06\n",
      "epoch 1883  loss 33.8477  lr 1.000e-06\n",
      "epoch 1884  loss 33.7436  lr 1.000e-06\n",
      "epoch 1885  loss 34.3639  lr 1.000e-06\n",
      "epoch 1886  loss 32.8136  lr 1.000e-06\n",
      "epoch 1887  loss 34.0158  lr 1.000e-06\n",
      "epoch 1888  loss 33.3816  lr 1.000e-06\n",
      "epoch 1889  loss 34.3160  lr 1.000e-06\n",
      "epoch 1890  loss 33.4030  lr 1.000e-06\n",
      "epoch 1891  loss 33.8065  lr 1.000e-06\n",
      "epoch 1892  loss 33.1518  lr 1.000e-06\n",
      "epoch 1893  loss 35.0740  lr 1.000e-06\n",
      "epoch 1894  loss 33.5030  lr 1.000e-06\n",
      "epoch 1895  loss 32.3134  lr 1.000e-06\n",
      "epoch 1896  loss 33.7237  lr 1.000e-06\n",
      "epoch 1897  loss 33.1609  lr 1.000e-06\n",
      "epoch 1898  loss 33.4745  lr 1.000e-06\n",
      "epoch 1899  loss 33.6487  lr 1.000e-06\n",
      "epoch 1900  loss 32.9382  lr 1.000e-06\n",
      "epoch 1901  loss 34.5790  lr 1.000e-06\n",
      "epoch 1902  loss 33.6597  lr 1.000e-06\n",
      "epoch 1903  loss 34.2936  lr 1.000e-06\n",
      "epoch 1904  loss 34.0859  lr 1.000e-06\n",
      "epoch 1905  loss 32.1046  lr 1.000e-06\n",
      "epoch 1906  loss 33.4093  lr 1.000e-06\n",
      "epoch 1907  loss 33.3866  lr 1.000e-06\n",
      "epoch 1908  loss 33.7942  lr 1.000e-06\n",
      "epoch 1909  loss 34.1940  lr 1.000e-06\n",
      "epoch 1910  loss 32.9702  lr 1.000e-06\n",
      "epoch 1911  loss 33.3724  lr 1.000e-06\n",
      "epoch 1912  loss 34.4227  lr 1.000e-06\n",
      "epoch 1913  loss 33.7149  lr 1.000e-06\n",
      "epoch 1914  loss 33.4920  lr 1.000e-06\n",
      "epoch 1915  loss 34.6291  lr 1.000e-06\n",
      "epoch 1916  loss 32.6327  lr 1.000e-06\n",
      "epoch 1917  loss 33.1968  lr 1.000e-06\n",
      "epoch 1918  loss 32.4082  lr 1.000e-06\n",
      "epoch 1919  loss 32.4476  lr 1.000e-06\n",
      "epoch 1920  loss 34.3621  lr 1.000e-06\n",
      "epoch 1921  loss 32.9853  lr 1.000e-06\n",
      "epoch 1922  loss 34.0032  lr 1.000e-06\n",
      "epoch 1923  loss 33.9796  lr 1.000e-06\n",
      "epoch 1924  loss 33.1926  lr 1.000e-06\n",
      "epoch 1925  loss 32.3805  lr 1.000e-06\n",
      "epoch 1926  loss 32.9040  lr 1.000e-06\n",
      "epoch 1927  loss 33.8174  lr 1.000e-06\n",
      "epoch 1928  loss 32.8785  lr 1.000e-06\n",
      "epoch 1929  loss 32.6606  lr 1.000e-06\n",
      "epoch 1930  loss 33.1815  lr 1.000e-06\n",
      "epoch 1931  loss 34.6572  lr 1.000e-06\n",
      "epoch 1932  loss 34.4286  lr 1.000e-06\n",
      "epoch 1933  loss 32.6193  lr 1.000e-06\n",
      "epoch 1934  loss 32.3400  lr 1.000e-06\n",
      "epoch 1935  loss 33.6178  lr 1.000e-06\n",
      "epoch 1936  loss 34.0213  lr 1.000e-06\n",
      "epoch 1937  loss 32.3155  lr 1.000e-06\n",
      "epoch 1938  loss 34.0872  lr 1.000e-06\n",
      "epoch 1939  loss 33.3099  lr 1.000e-06\n",
      "epoch 1940  loss 32.8154  lr 1.000e-06\n",
      "epoch 1941  loss 33.7112  lr 1.000e-06\n",
      "epoch 1942  loss 33.0407  lr 1.000e-06\n",
      "epoch 1943  loss 32.9378  lr 1.000e-06\n",
      "epoch 1944  loss 33.2036  lr 1.000e-06\n",
      "epoch 1945  loss 33.4518  lr 1.000e-06\n",
      "epoch 1946  loss 33.7062  lr 1.000e-06\n",
      "epoch 1947  loss 32.8845  lr 1.000e-06\n",
      "epoch 1948  loss 35.0408  lr 1.000e-06\n",
      "epoch 1949  loss 34.6317  lr 1.000e-06\n",
      "epoch 1950  loss 33.9269  lr 1.000e-06\n",
      "epoch 1951  loss 33.4351  lr 1.000e-06\n",
      "epoch 1952  loss 32.9518  lr 1.000e-06\n",
      "epoch 1953  loss 33.4839  lr 1.000e-06\n",
      "epoch 1954  loss 33.3013  lr 1.000e-06\n",
      "epoch 1955  loss 33.4486  lr 1.000e-06\n",
      "epoch 1956  loss 33.7962  lr 1.000e-06\n",
      "epoch 1957  loss 32.9867  lr 1.000e-06\n",
      "epoch 1958  loss 33.2637  lr 1.000e-06\n",
      "epoch 1959  loss 33.0024  lr 1.000e-06\n",
      "epoch 1960  loss 33.6983  lr 1.000e-06\n",
      "epoch 1961  loss 32.9462  lr 1.000e-06\n",
      "epoch 1962  loss 34.2565  lr 1.000e-06\n",
      "epoch 1963  loss 33.1138  lr 1.000e-06\n",
      "epoch 1964  loss 33.3040  lr 1.000e-06\n",
      "epoch 1965  loss 34.8270  lr 1.000e-06\n",
      "epoch 1966  loss 33.7434  lr 1.000e-06\n",
      "epoch 1967  loss 32.6242  lr 1.000e-06\n",
      "epoch 1968  loss 33.7958  lr 1.000e-06\n",
      "epoch 1969  loss 33.9860  lr 1.000e-06\n",
      "epoch 1970  loss 33.8216  lr 1.000e-06\n",
      "epoch 1971  loss 33.2215  lr 1.000e-06\n",
      "epoch 1972  loss 33.7551  lr 1.000e-06\n",
      "epoch 1973  loss 33.8057  lr 1.000e-06\n",
      "epoch 1974  loss 33.2500  lr 1.000e-06\n",
      "epoch 1975  loss 33.1139  lr 1.000e-06\n",
      "epoch 1976  loss 31.6507  lr 1.000e-06\n",
      "epoch 1977  loss 32.5045  lr 1.000e-06\n",
      "epoch 1978  loss 33.1321  lr 1.000e-06\n",
      "epoch 1979  loss 33.6290  lr 1.000e-06\n",
      "epoch 1980  loss 34.0269  lr 1.000e-06\n",
      "epoch 1981  loss 32.9120  lr 1.000e-06\n",
      "epoch 1982  loss 32.8427  lr 1.000e-06\n",
      "epoch 1983  loss 33.8790  lr 1.000e-06\n",
      "epoch 1984  loss 32.6511  lr 1.000e-06\n",
      "epoch 1985  loss 34.2869  lr 1.000e-06\n",
      "epoch 1986  loss 32.9240  lr 1.000e-06\n",
      "epoch 1987  loss 33.2767  lr 1.000e-06\n",
      "epoch 1988  loss 33.0374  lr 1.000e-06\n",
      "epoch 1989  loss 34.9739  lr 1.000e-06\n",
      "epoch 1990  loss 33.5253  lr 1.000e-06\n",
      "epoch 1991  loss 33.8454  lr 1.000e-06\n",
      "epoch 1992  loss 33.2736  lr 1.000e-06\n",
      "epoch 1993  loss 33.4562  lr 1.000e-06\n",
      "epoch 1994  loss 31.8414  lr 1.000e-06\n",
      "epoch 1995  loss 34.1732  lr 1.000e-06\n",
      "epoch 1996  loss 32.4524  lr 1.000e-06\n",
      "epoch 1997  loss 33.4322  lr 1.000e-06\n",
      "epoch 1998  loss 33.9971  lr 1.000e-06\n",
      "epoch 1999  loss 33.8930  lr 1.000e-06\n",
      "epoch 2000  loss 32.7325  lr 1.000e-06\n"
     ]
    }
   ],
   "source": [
    "import torch, torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "model = OddShapeDetector().to(device)\n",
    "criterion = nn.MSELoss()                 # regresja współrzędnych\n",
    "optim     = Adam(model.parameters(), lr=0.001) #bylo 0.001\n",
    "\n",
    "scheduler = ReduceLROnPlateau(\n",
    "    optim,\n",
    "    mode='min',         # obniżamy stratę\n",
    "    factor=0.5,         # nowy LR = lr * factor\n",
    "    patience=100,         # ile epok czekać na poprawę\n",
    "    min_lr=1e-6         # najniższy dopuszczalny LR\n",
    ")\n",
    "\n",
    "epochs = 2000\n",
    "for ep in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    running = 0.0\n",
    "    for img, coord in loader:\n",
    "        img, coord = img.to(device), coord.to(device)\n",
    "\n",
    "        pred = model(img)          # [B,2]\n",
    "        loss = criterion(pred, coord)\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        running += loss.item() * img.size(0)\n",
    "\n",
    "    scheduler.step(loss)\n",
    "    lr_now = scheduler.get_last_lr()[0]\n",
    "\n",
    "    print(f\"epoch {ep:2d}  loss {running/len(loader.dataset):.4f}  lr {lr_now:.3e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
