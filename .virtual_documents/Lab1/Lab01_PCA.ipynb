






































import numpy as np

# Define a sample matrix Z. It is 3 x 2
X = np.array([[3, 1], [1, 3], [4, 2]])
print("3x2 matrix X:")
print(X)

# Perform Singular Value Decomposition
U, Sigma, VT = np.linalg.svd(X)
print("\nLeft singular vectors (3x3 U matrix):")
print(U)
print("\nSingular values (Sigma diagonal):")
print(Sigma)

# Convert Sigma into a full diagonal matrix
Sigma_full = np.zeros((X.shape[0], X.shape[1]))
np.fill_diagonal(Sigma_full, Sigma)
print("\nCorresponding Sigma in a 3x2 matrix form:")
print(Sigma_full)
print("\nRight singular vectors (2x2 VT matrix):")
print(VT)

print("\nChecks:")
print("\n1. Is U orthogonal?")
print(U.T @ U)
print(U @ U.T)
print("\n2. Is V orthogonal?")
print(VT.T @ VT)
print(VT @ VT.T)




# Reconstruct X
X_reconstructed = U @ Sigma_full @ VT
print("\n3. Reconstructed X?")
print(X_reconstructed)








import sklearn.datasets
import pandas as pd

# Load the wine dataset
data = sklearn.datasets.load_wine()
X = data.data
y = data.target
target_names = data.target_names

# Convert to DataFrame to easily inspect column names, types, and rows
df = pd.DataFrame(X, columns=data.feature_names)

# Print column names
print("Column Names:")
print(df.columns)

# Print column types
print("\nColumn Types:")
print(df.dtypes)

# Display the first 5 rows
print("\nFirst 5 Rows:")
print(df.head())


# Count occurrences of each class in y
print("\nCounts of each class in y:")
unique, counts = np.unique(y, return_counts=True)
class_counts = dict(zip(unique, counts))
print(class_counts)
print("Overall length:", len(y))


# First, center X
X = X - np.mean(X, axis=0)

# And standardize it (it is always a good practice, so each dimension is treated as equally important)
X = X / np.std(X, axis=0)

# Perform Singular Value Decomposition
U, Sigma, VT = np.linalg.svd(X)

# Convert Sigma into a full diagonal matrix
Sigma_full = np.zeros((X.shape[0], X.shape[1]))
np.fill_diagonal(Sigma_full, Sigma)

variance_estimators = (Sigma ** 2) / (X.shape[0] - 1)

XV = U @ Sigma_full

print("\nVT, having the principal components in rows\n")
print(VT)

print("\nX projected on V (first 5 rows)\n")
print(XV[:5,:])
print("\nVariance estimators\n")
print(variance_estimators)
print("\nExplained percentage variance\n")
print(variance_estimators / sum(variance_estimators))
print("\nCumulative explained percentage variance\n")
print(np.cumsum(variance_estimators / sum(variance_estimators)))



import matplotlib.pyplot as plt

# Plot the PCA results with color coding by wine class
plt.figure(figsize=(10, 8))
for i, target_name in enumerate(target_names):
    plt.scatter(XV[y == i, 0], XV[y == i, 1], label=target_name, alpha=0.7)

plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.title("PCA of Wine Dataset")
plt.legend(title="Wine Class")
plt.grid(True)
plt.show()





from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# Load the wine dataset
data = sklearn.datasets.load_wine()
X = data.data
y = data.target
target_names = data.target_names


# Center and standardize the data before applying PCA
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Perform PCA and reduce the data to 2 components for visualization
pca = PCA(n_components = 2)
XV = pca.fit_transform(X)

print("\nthe principal components in rows\n")
print(pca.components_)

print("\nX projected on V (first 5 rows)\n")
print(XV[:5,:])
print("\nExplained percentage variance\n")
print(pca.explained_variance_ratio_)
print("\nCumulative explained percentage variance\n")
print(np.cumsum(pca.explained_variance_ratio_))

# Plot the PCA results with color coding by wine class
plt.figure(figsize=(10, 8))
for i, target_name in enumerate(target_names):
    plt.scatter(XV[y==i, 0], XV[y==i, 1], label=target_name, alpha=0.7)

plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.title("PCA of Wine Dataset")
plt.legend(title="Wine Class")
plt.grid(True)
plt.show()












import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA


# Load the Olivetti faces dataset
data = sklearn.datasets.fetch_olivetti_faces()
faces = data.images  # Original 64x64 images

X = data.data        # Flattened images (each image is 64x64 pixels, flattened to 4096 features)
print("Overall number of faces:", len(X))

# Display 50 example faces from the dataset in 10 rows, 5 faces per row
fig, ax = plt.subplots(10, 5, figsize=(10, 20))
for i in range(50):
    row = i // 5  # Determine the row index
    col = i % 5   # Determine the column index
    ax[row, col].imshow(faces[i], cmap='gray')
    ax[row, col].axis('off')
plt.suptitle("50 examples of faces from the Olivetti faces dataset")
plt.show()










# Standardize the data (important for PCA)
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Perform PCA to reduce the dimensions
n_components = 400  # Number of principal components to retain
pca = PCA(n_components=n_components)
X_pca = pca.fit_transform(X)

# Show the cumulative variance explained by the components
plt.figure(figsize=(8, 5))
plt.plot(np.cumsum(pca.explained_variance_ratio_[:20]), marker='o')
plt.xlabel("Number of Principal Components")
plt.ylabel("Cumulative Explained Variance")
plt.title("Variance Explained by PCA Components")
plt.grid(True)
plt.show()

# Visualize the principal components (eigenfaces)
eigenfaces = pca.components_.reshape((n_components, 64, 64))

fig, ax = plt.subplots(2, 5, figsize=(10, 5))
for i in range(10):
    ax[i // 5, i % 5].imshow(eigenfaces[i], cmap='gray')
    ax[i // 5, i % 5].set_title(f"PC {i+1}")
    ax[i // 5, i % 5].axis('off')
plt.suptitle("Top 10 Principal Components (Eigenfaces)")
plt.show()







interesting_faces = [0, 10, 27, 38, 40]

# Reconstruct example faces using only a subset of components
n_components_to_show = [1, 2, 3, 4, 5, 6, 7, 20, 100, 400]  # Number of components for reconstruction
fig, ax = plt.subplots(len(n_components_to_show) + 1, 5, figsize=(10, 20))



# Original faces on top row
for i in range(5):
    ax[0, i].imshow(faces[interesting_faces[i]], cmap='gray')
    ax[0, i].set_title("Original")
    ax[0, i].axis('off')

# Reconstructed faces using increasing numbers of components
for j, n_comp in enumerate(n_components_to_show, 1):

    # Copy the first n_comp components from X_pca, setting the rest to zero
    X_reduced = np.zeros_like(X_pca)
    X_reduced[:, :n_comp] = X_pca[:, :n_comp]
    X_reconstructed = pca.inverse_transform(X_reduced)


    for i in range(5):
        ax[j, i].imshow(X_reconstructed[interesting_faces[i]].reshape(64, 64), cmap='gray')
        ax[j, i].set_title(f"{n_comp} PCs")
        ax[j, i].axis('off')

plt.suptitle("Face Reconstruction with Increasing Number of Principal Components")
plt.show()






from matplotlib.cm import get_cmap

# Load the MNIST dataset
mnist = sklearn.datasets.fetch_openml("mnist_784", version=1)
X = mnist.data.values / 255.0  # Normalize pixel values to [0, 1]
y = mnist.target.astype(int)
print("Dataset length:", len(y))

# Display a few example digits from the dataset
fig, ax = plt.subplots(1, 5, figsize=(10, 4))
for i in range(5):
    ax[i].imshow(X[i].reshape(28, 28), cmap='gray')
    ax[i].axis('off')
plt.suptitle("Example Digits from MNIST Dataset")
plt.show()







# Standardize the data (important for PCA)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Perform PCA to reduce the dimensions
n_components = 100  # Number of principal components to retain
pca = PCA(n_components=n_components)
X_pca = pca.fit_transform(X_scaled)

# Show the cumulative variance explained by the components
plt.figure(figsize=(8, 5))
plt.plot(np.cumsum(pca.explained_variance_ratio_), marker='o')
plt.xlabel("Number of Principal Components")
plt.ylabel("Cumulative Explained Variance")
plt.title("Variance Explained by PCA Components")
plt.grid(True)
plt.show()

# Visualize the data in the first 2 principal components

# Define a color map with distinct colors
cmap = plt.colormaps["tab10"]    # "tab10" provides 10 distinct colors

plt.figure(figsize=(10, 8))
for i in range(10):
    subset = X_pca[y == i]  # Select only the digits of the current class
    plt.scatter(subset[:1000, 0], subset[:1000, 1], label=str(i), color=cmap(i), alpha=0.7, s=20)
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.legend(title="Digits")
plt.title("MNIST Data Projected onto First Two Principal Components")
plt.show()

# Visualize the principal components
eigendigits = pca.components_.reshape((n_components, 28, 28))

fig, ax = plt.subplots(2, 5, figsize=(10, 5))
for i in range(10):
    ax[i // 5, i % 5].imshow(eigendigits[i], cmap='gray')
    ax[i // 5, i % 5].set_title(f"PC {i+1}")
    ax[i // 5, i % 5].axis('off')
plt.suptitle("Top 10 Principal Components")
plt.show()







# Reconstruct example digits using a subset of components
n_components_to_show = [1, 2, 10, 20, 40, 100]  # Number of components for reconstruction
fig, ax = plt.subplots(len(n_components_to_show) + 1, 10, figsize=(10, 8))

# Original digits on top row
for i in range(10):
    ax[0, i].imshow(X[i].reshape(28, 28), cmap='gray')
    ax[0, i].set_title("Original")
    ax[0, i].axis('off')

# Reconstructed digits using increasing numbers of components
for j, n_comp in enumerate(n_components_to_show, 1):

    # Copy the first n_comp components from X_pca, setting the rest to zero
    X_reduced = np.zeros_like(X_pca)
    X_reduced[:, :n_comp] = X_pca[:, :n_comp]
    X_reconstructed = pca.inverse_transform(X_reduced)

    for i in range(10):
        ax[j, i].imshow(X_reconstructed[i].reshape(28, 28), cmap='gray')
        ax[j, i].set_title(f"{n_comp} PCs")
        ax[j, i].axis('off')

plt.suptitle("Digit Reconstruction with Increasing Number of Principal Components")
plt.show()






print("hello wold")



