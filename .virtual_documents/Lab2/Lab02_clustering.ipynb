






























































import numpy as np
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation
from IPython.display import HTML

# ----------------------------------------------------------------------
# SECTION 1: Generate the "mouse" dataset
# ----------------------------------------------------------------------
np.random.seed(0)  # for reproducibility

# 1) 300 points from N([0, 0], [[0.16, 0],[0, 0.16]])
mean1 = np.array([0.0, 0.0])
cov1 = np.array([[0.16, 0.0],
                 [0.0,  0.16]])
data1 = np.random.multivariate_normal(mean1, cov1, 300)

# 2) 100 points from N([0.8, 0.8], [[0.04, 0],[0, 0.04]])
mean2 = np.array([0.8, 0.8])
cov2 = np.array([[0.04, 0.0],
                 [0.0,  0.04]])
data2 = np.random.multivariate_normal(mean2, cov2, 100)

# 3) 100 points from N([-0.8, 0.8], [[0.04, 0],[0, 0.04]])
mean3 = np.array([-0.8, 0.8])
cov3 = np.array([[0.04, 0.0],
                 [0.0,  0.04]])
data3 = np.random.multivariate_normal(mean3, cov3, 100)

# Combine all data
data_X = np.vstack([data1, data2, data3])
N, d = data_X.shape  # should be (500, 2)

# ----------------------------------------------------------------------
# SECTION 2: Initialize EM for 3 Gaussian mixtures
# ----------------------------------------------------------------------
K = 3
np.random.seed(42)  # again for reproducibility

# Mixture weights, means, covariances
pi = np.ones(K) / K
mu = np.random.randn(K, d)
Sigma = np.array([np.eye(d) for _ in range(K)])

# ----------------------------------------------------------------------
# SECTION 3: Define functions for E-step & M-step
# ----------------------------------------------------------------------
def gaussian_pdf(x, mean, cov):
    """
    Compute multivariate Gaussian pdf value at x, given mean and cov.
    x is (d, ) vector, mean is (d, ), cov is (d, d).
    """
    d_ = len(x)
    det_cov = np.linalg.det(cov)
    inv_cov = np.linalg.inv(cov)
    norm_const = 1.0 / np.sqrt((2*np.pi)**d_ * det_cov)
    diff = x - mean
    exponent = -0.5 * (diff @ inv_cov @ diff.T)
    return norm_const * np.exp(exponent)

def e_step(data, pi, mu, Sigma):
    """
    E-step: compute the responsibilities resp[n, k] = p(z_n=k | x_n, theta).
    """
    N, _ = data.shape
    K = len(pi)
    resp = np.zeros((N, K))

    for n in range(N):
        for k in range(K):
            resp[n, k] = pi[k] * gaussian_pdf(data[n], mu[k], Sigma[k])
        resp[n, :] /= np.sum(resp[n, :])  # normalize

    return resp

def m_step(data, resp):
    """
    M-step: given responsibilities, update pi, mu, Sigma.
    """
    N, d_ = data.shape
    K = resp.shape[1]

    Nk = np.sum(resp, axis=0)  # shape (K,)

    # Update pi
    pi_new = Nk / N

    # Update mu
    mu_new = np.zeros((K, d_))
    for k in range(K):
        mu_new[k] = np.sum(resp[:, k].reshape(-1,1)*data, axis=0) / Nk[k]

    # Update Sigma
    Sigma_new = []
    for k in range(K):
        S_k = np.zeros((d_, d_))
        for n in range(N):
            diff = data[n] - mu_new[k]
            S_k += resp[n, k] * np.outer(diff, diff)
        S_k /= Nk[k]
        Sigma_new.append(S_k)
    Sigma_new = np.array(Sigma_new)

    return pi_new, mu_new, Sigma_new

# ----------------------------------------------------------------------
# SECTION 4: Set up for animation - plot the initial state
# ----------------------------------------------------------------------
# We'll store the EM parameters in global variables so we can update them
# within our animation function easily.

# Precompute figure, axes, and placeholders for scatter plots and cluster centers
fig, axes = plt.subplots(1, 3, figsize=(15, 5))

# We'll create one scatter per cluster:
scatters = []
center_plots = []

# Initialize the responsibilities (before first iteration) just for plotting the colors
resp_init = e_step(data_X, pi, mu, Sigma)

for k in range(K):
    # Scatter for the data with color = responsibilities for cluster k
    sc = axes[k].scatter(data_X[:, 0], data_X[:, 1],
                         c=resp_init[:, k], cmap='viridis',
                         s=20, vmin=0, vmax=1)

    scatters.append(sc)

    # Plot for the cluster center: "X" in red
    cp, = axes[k].plot(mu[k,0], mu[k,1], 'rx', markersize=14, mew=3)
    center_plots.append(cp)

    axes[k].set_title(f'Cluster {k} (iteration 0)')
    axes[k].set_xlim([-2, 2])
    axes[k].set_ylim([-1, 2])

    # Add a colorbar for each subplot
    cbar = fig.colorbar(sc, ax=axes[k])
    cbar.set_label('Posterior Probability')

plt.tight_layout()
plt.show()




iterations = 60  # number of EM iterations (frames in the animation)
    #In 60 iterations the EM converges

# ----------------------------------------------------------------------
# SECTION 5: Define the update function for animation
# ----------------------------------------------------------------------
def init():
    # Nothing special needed for init
    return scatters + center_plots

def animate(frame):
    global pi, mu, Sigma

    # Perform one E-step + M-step
    resp = e_step(data_X, pi, mu, Sigma)
    pi, mu, Sigma = m_step(data_X, resp)

    # Update the scatter colors & cluster center positions
    for k in range(K):
        # update scatter color array
        scatters[k].set_array(resp[:, k])
        # update center plot
        center_plots[k].set_data([mu[k,0]], [mu[k,1]])
        axes[k].set_title(f'Cluster {k} (iteration {frame+1})')
    return scatters + center_plots

anim = FuncAnimation(fig, animate, frames=iterations,
                     init_func=init, blit=False, interval=120)

# ----------------------------------------------------------------------
# SECTION 6: Display the animation
# ----------------------------------------------------------------------
# If you're in a Jupyter/Colab environment, you can display the animation inline:
HTML(anim.to_html5_video())


# ----------------------------------------------------------------------
# SECTION 7: plot the final state
# ----------------------------------------------------------------------
resp_final = e_step(data_X, pi, mu, Sigma)

fig, axes = plt.subplots(1, 3, figsize=(15, 4))
for k in range(K):
    sc = axes[k].scatter(data_X[:,0], data_X[:,1], c=resp_final[:, k], cmap='viridis')
    axes[k].set_title(f'Cluster {k} (iteration {iterations})')
    axes[k].set_xlabel('x')
    axes[k].set_ylabel('y')
    # Plot for the cluster center: "X" in red
    cp = axes[k].plot(mu[k,0], mu[k,1], 'rx', markersize=14, mew=3)
    # Add a colorbar
    plt.colorbar(sc, ax=axes[k])

plt.tight_layout()
plt.show()


























pip install scikit-learn-extra   #it is required for K-Medoids algorithm





import time

from sklearn.mixture import GaussianMixture
from sklearn.cluster import KMeans
from sklearn_extra.cluster import KMedoids
from matplotlib.colors import Normalize
from matplotlib.lines import Line2D  # Import Line2D for custom legend

# ----------------------------------------------------------------------
# SECTION 1: Define a helper function to measure runtime
# ----------------------------------------------------------------------
def measure_runtime(model_class, X, n_runs=5, **kwargs):
    """
    Instantiates and fits `model_class(**kwargs)` on X `n_runs` times.
    Returns average runtime across those runs (in seconds).
    """
    start = time.time()
    for _ in range(n_runs):
        model = model_class(**kwargs)
        model.fit(X)
    end = time.time()
    return (end - start)/n_runs, model

# ----------------------------------------------------------------------
# SECTION 2: Measure runtime for each algorithm + final fit for plotting
# ----------------------------------------------------------------------
n_runs = 5  # number of runs to average the runtime

# EM for Gaussian Mixtures
gm_time, gm_model = measure_runtime(GaussianMixture, data_X, n_runs=n_runs, n_components=3, covariance_type='full', random_state=42)
gm_labels = gm_model.predict(data_X)

# K-means
km_time, km_model = measure_runtime(KMeans, data_X, n_runs=n_runs, n_clusters=3, random_state=42)
km_labels = km_model.labels_

# K-medoids
kmed_time, kmed_model = measure_runtime(KMedoids, data_X, n_runs=n_runs, n_clusters=3, random_state=42, method='pam')
kmed_labels = kmed_model.labels_

print(f"Average runtime over {n_runs} runs:")
print(f"  - GaussianMixture (EM) : {gm_time:.5f} s")
print(f"  - KMeans              : {km_time:.5f} s")
print(f"  - KMedoids            : {kmed_time:.5f} s")

# ----------------------------------------------------------------------
# SECTION 3: Plot each clustering's results
# ----------------------------------------------------------------------

fig, axes = plt.subplots(1, 3, figsize=(15, 5))
data_counts = [300, 100, 100]  # Number of points per source
markers = ['o', '^', 'D']  # Circle, Triangle, Diamond


# Define markers for each data source
markers = ['o', '^', 'D']  # Circle, Triangle, Diamond
data_counts = [300, 100, 100]
# Create a normalizer to scale cluster labels to colormap range
norm = Normalize(vmin=0, vmax=2)  # Normalize colors between 0 and 2

start = 0
# EM (GaussianMixture)
for original_group in range(3):
  end = start + data_counts[original_group]
  axes[0].scatter(data_X[start:end,0], data_X[start:end,1], c=gm_labels[start:end], marker=markers[original_group], cmap='Set1', s=20, norm=norm)
  start = end

axes[0].plot(gm_model.means_[:, 0], gm_model.means_[:, 1], 'rx', markersize=14, mew=3, alpha=0.5)
axes[0].set_title("EM (GaussianMixture)")
axes[0].set_xlabel("x")
axes[0].set_ylabel("y")

start = 0
# K-Means
for original_group in range(3):
  end = start + data_counts[original_group]
  axes[1].scatter(data_X[start:end,0], data_X[start:end,1], c=km_labels[start:end], marker=markers[original_group], cmap='Set1', s=20, norm=norm)
  start = end

axes[1].plot(km_model.cluster_centers_[:, 0], km_model.cluster_centers_[:, 1], 'rx', markersize=14, mew=3, alpha=0.5)
axes[1].set_title("K-Means")
axes[1].set_xlabel("x")
axes[1].set_ylabel("y")

start = 0
# K-Medoids
for original_group in range(3):
  end = start + data_counts[original_group]
  axes[2].scatter(data_X[start:end,0], data_X[start:end,1], c=kmed_labels[start:end], marker=markers[original_group], cmap='Set1', s=20, norm=norm)
  start = end

axes[2].plot(kmed_model.cluster_centers_[:, 0], kmed_model.cluster_centers_[:, 1], 'rx', markersize=14, mew=3, alpha=0.5)
axes[2].set_title("K-Medoids")
axes[2].set_xlabel("x")
axes[2].set_ylabel("y")


legend_elements = [
    # Markers for original data sources
    Line2D([0], [0], marker='o', color='w', markerfacecolor='k', markersize=8, label='Source 0 (Circle)'),
    Line2D([0], [0], marker='^', color='w', markerfacecolor='k', markersize=8, label='Source 1 (Triangle)'),
    Line2D([0], [0], marker='D', color='w', markerfacecolor='k', markersize=8, label='Source 2 (Diamond)'),

    # Colors for inferred clusters
    Line2D([0], [0], marker='s', color=plt.cm.Set1(0.0), markersize=10, label='Cluster 0 (Red)'),
    Line2D([0], [0], marker='s', color=plt.cm.Set1(0.5), markersize=10, label='Cluster 1 (Orange)'),
    Line2D([0], [0], marker='s', color=plt.cm.Set1(1.0), markersize=10, label='Cluster 2 (Grey)')
]

# Add legend to the plot
axes[0].legend(handles=legend_elements, loc='best', frameon=True, title="Legend")

plt.tight_layout()
plt.show()






norm = Normalize(vmin=0, vmax=2)  # Normalize colors between 0 and 2 - common color pallette

# 1. Generate 500 new points from the fitted GM model
X_new, new_labels = gm_model.sample(n_samples=500)

# 2. Visualize the newly generated data
fig, axes = plt.subplots(1, 2, figsize=(15, 5))

axes[0].scatter(X_new[:, 0], X_new[:, 1], c=new_labels, cmap='Set1', s=20, norm=norm)
axes[0].set_title("New data generated from the fitted GaussianMixture model")
axes[0].set_xlabel("x")
axes[0].set_ylabel("y")
legend_elements = [
    Line2D([0], [0], marker='s', color=plt.cm.Set1(0.0), markersize=10, label='Cluster 0 (Red)'),
    Line2D([0], [0], marker='s', color=plt.cm.Set1(0.5), markersize=10, label='Cluster 1 (Orange)'),
    Line2D([0], [0], marker='s', color=plt.cm.Set1(1.0), markersize=10, label='Cluster 2 (Grey)')
]

# Add legend to the plot
axes[0].legend(handles=legend_elements, loc='best', frameon=True, title="Legend")

# 3. Visualize the original data

axes[1].scatter(data_X[:, 0], data_X[:, 1], c=[2]*300+[0]*100+[1]*100, cmap='Set1', s=20, norm=norm)  #I shuffle the colors to match the inferred clusters
axes[1].set_title("Original data")
axes[1].set_xlabel("x")
axes[1].set_ylabel("y")
legend_elements = [
    Line2D([0], [0], marker='s', color=plt.cm.Set1(1.0), markersize=10, label='Source 0 (Grey)'),
    Line2D([0], [0], marker='s', color=plt.cm.Set1(0.0), markersize=10, label='Source 1 (Red)'),
    Line2D([0], [0], marker='s', color=plt.cm.Set1(0.5), markersize=10, label='Source 2 (Orange)')
]

# Add legend to the plot
axes[1].legend(handles=legend_elements, loc='best', frameon=True, title="Legend")


plt.show()





# Generate 500 new points from the fitted K-means model
X_new, new_labels = km_model.sample(n_samples=500)






